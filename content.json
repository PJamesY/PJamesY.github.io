{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend ë¥¼ ê³µë¶€í•˜ë©° ë²ˆì—­í•˜ì—¬ì„œ ì˜¬ë¦½ë‹ˆë‹¤ ê° ì˜í™”ì— ëŒ€í•´ì„œ í‰ì ì´ 5ì ê¹Œì§€ ì¤„ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : ì´ ìœ ì €ì˜ ëª…(ìˆ˜) $n_m$ : ì´ ì˜í™”ì˜ ê°¯ìˆ˜ $r(i,j)$ : $j$ë¼ëŠ” ìœ ì €ê°€ ì˜í™” $i$ì— í‰ì ì„ ì¤¬ìœ¼ë©´ 1 $r(i,j)$ : $r(i,j) = 1$ì´ë¼ëŠ” ì¡°ê±´ í•˜ì— user $j$ê°€ movie $i$ì—ê²Œ ì¤€ í‰ì  ì ìˆ˜ ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì§€ê³  missing valueë¥¼ predictí•´ì•¼ í•œë‹¤ 1. Content_based algorithm content_basedëŠ” contentê°€ ì–´ë–¤ íŠ¹ì •í•œ ì ì¬ featureê°€ ìˆì„ê±°ë¼ê³  ìƒê°í•˜ê³  ê·¸ feature vectorë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤.ì˜ˆë¥¼ ë“¤ì–´ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ê±°ë¼ë©´ ì˜í™”ì—ëŠ” ê° ì¥ë¥´ê°€ ì¡´ì¬í•œë‹¤. ì˜í™”ì˜ ë¡œë§¨ìŠ¤ ì¥ë¥´ ì •ë„, ì•¡ì…˜ ì •ë„ë¥¼ ê°€ì¤‘ì¹˜ ê°œë…ìœ¼ë¡œ ì£¼ì–´ì„œ ê° ì˜í™”ì˜ latent feature vectorë¥¼ ì§€ì •í•´ì¤€ë‹¤ ì—¬ê¸°ì—ì„œëŠ” romanceë¥¼ $x_1$, action $x_2$ì˜ feature vectorë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤ extra(constant) featureë„ ë„£ì–´ì¤€ë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 ê°ê°ì˜ ì˜í™”ëŠ” $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ ê°ê°ì˜ ì˜í™” $x^i$ì€ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ì—ë¥¼ ë“¤ì–´ ì˜í™” Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ ì˜ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ content_based ë°©ì‹ì—ì„œëŠ” ê°ê°ì˜ content feature vectorì— ë”°ë¥¸ user parameter vectorë¥¼ learningì‹œì¼œì•¼í•œë‹¤ê°ê°ì˜ userë§ˆë‹¤ ê°ê°ì˜ í‰ì ì€ linear regressionë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ ë§Œì•½ Aliceì˜ parameter vector($\\theta^1$)ì´ $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ ì´ë¼ê³  í•œë‹¤ë©´ Aliceê°€ Cute love ì˜í™”ì— í‰ì ì„ ì¤„ ì ìˆ˜ëŠ” $(\\theta^{(1)})^T x^{(3)}$ inner productë¥¼ í•´ì£¼ë©´ 4.95 í‰ì ì„ ì¤„ê±°ë¼ëŠ” ì˜ˆì¸¡ì´ ë‚˜ì˜¨ë‹¤ $\\theta$ë¥¼ learning í•´ë³´ì Notation n : featureì˜ dimension(constantë¥¼ ì œì™¸í•œê²ƒ) ì—¬ê¸°ì„œëŠ” 2(romance, action)ì´ë‹¤ $m^j$ : $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì˜ ê°¯ìˆ˜ $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì— í•œí•´ì„œ $j$ userì˜ ì˜í™”ì— ì¤€ ì˜ˆì¸¡ í‰ì ê³¼ ì‹¤ì œ í‰ì ì˜ ì°¨ë¥¼ ìµœì†Œí™” í•´ì•¼í•œë‹¤ min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ìµœì í™” í•˜ëŠ”ë° $m^{(j)}$ëŠ” í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì—†ì• ì£¼ì–´ë„ ëœë‹¤ min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ì´ê±°ë¥¼ ëª¨ë“  userì—ê²Œ ì ìš©ì‹œì¼œì•¼ í•œë‹¤. ìš°ë¦¬ì˜ objective functionì´ ëœë‹¤ J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•´ì¤€ë‹¤ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ kê°€ 0ì¸ê²ƒê³¼ Kê°€ 0ì´ ì•„ë‹Œê²ƒì˜ ëœ»ì€ featureì˜ constant termì„ í•˜ëŠëƒ ì•ˆí•˜ëŠëƒ ì´ë‹¤ ìš°ë¦¬ì˜ Obejctive functionì˜ ìµœì¢…ì‹ì€ {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_basedëŠ” ê° contentì— ëŒ€í•œ featureë¥¼ ì•Œì•„ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë‹¤ìŒì— ì•Œì•„ë³¼ ë°©ë²•ì€ ì´ê±°ë¥¼ ë³´ì™„í•´ì¤€ Colloaborative filteringë°©ë²•ì´ë‹¤ 2. Collaborative filtering content_based ë°©ì‹ì—ì„œëŠ” content featureë¥¼ ì•Œê³  ìˆëŠ” ìƒíƒœì˜€ë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ ë˜í•œ Featureì˜ ê°¯ìˆ˜ë¥¼ ì¡°ê¸ˆë” ë§ì´ ì•Œê¸°ë¥¼ ì›í•œë‹¤ userì˜ ì˜í™” ì·¨í–¥ ë²¡í„° $\\theta$ì™€ ê° ì˜í™”ì˜ ì¥ë¥´ feature ë²¡í„° $x$ë¥¼ ì„œë¡œ êµì°¨ì ìœ¼ë¡œ learning Notation $n_m$ : ì˜í™”ì˜ ê°¯ìˆ˜ $n_u$ : userì˜ ìˆ˜ $\\theta$ë¥¼ ëœë¤ì ìœ¼ë¡œ Initializeí•œë‹¤ $\\theta$ë¥¼ ê°€ì§€ê³  $x$ë¥¼ updateì‹œí‚¨ë‹¤ \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 updateëœ $x$ë¥¼ ê°€ì§€ê³  $\\theta$ë¥¼ Updateì‹œí‚¨ë‹¤ \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_basedì™€ ë‹¤ë¥¸ì ì€ costant Termì„ ë„£ì–´ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤ cost function Jë¥¼ ìµœëŒ€í•œ ì¤„ì—¬ì£¼ëŠ” ë²¡í„°ë¥¼ ì°¾ëŠ”ë‹¤ 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5ê°œì˜ ì˜í™”ì™€ 4ëª…ì˜ user matrixì´ë‹¤ predicted ratingì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤ X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì  $e_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì ê³¼ ì˜ˆì¸¡ í‰ì ì˜ ì°¨ì´ $p_{i,k}$ : $i$ ìœ ì €ì˜ latent feature vector $q_{k,j}$ : $j$ ì˜í™”ì˜ latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : ìœ ì €ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ìœ ì € ëª…ìˆ˜ (X) Latent ê°¯ìˆ˜}\\right)$ $Q$ : ì˜í™”ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ Latent ê°¯ìˆ˜ (X) ì˜í™” ê°¯ìˆ˜ }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{â€˜}$, $q_{k,j}^{â€˜}$ ê°ê° ë²¡í„°ì´ë‹¤","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) ìµœëŒ€ê°€ëŠ¥ë„ Notation $D$ : Data (ê´€ì¸¡í•œ(Observed) ë°ì´í„°) $\\theta$ : parameter (í™•ë¥ )&lt;/font&gt; $H$ : ì•ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ $T$ : ë’·ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ ë¥¼ ê°€ì¥ ë†’ì—¬ì£¼ëŠ” $\\theta$ ë¥¼ êµ¬í•˜ëŠ”ê²ƒ MLE ê³„ì‚° 1. Maximum Liklihood ì‹ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ ì„ ì”Œìš´ë‹¤ ê³±ì…ˆì€ ê³„ì‚°ì´ ë³µì¡í•˜ë¯€ë¡œ $ln$ë¥¼ ì”Œì›Œì¤€ë‹¤ â†’ $log$ $function$ : ê³±ì„ í•©ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤ ë¡œê·¸ëŠ” ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ $\\underset{\\theta}{\\operatorname{argmax}}$ ì˜ ê°’ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤(derivative) êµ¬í•˜ê³ ì í•˜ëŠ” $\\theta$ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì´ $0$ì´ ë˜ë„ë¡ ì‹ì„ ì„¸ìš´ë‹¤ ë¯¸ë¶„í•œ ê°’ì´ $0$ ë˜ê²Œ í•˜ëŠ” $\\theta$ê°’ì„ êµ¬í•œë‹¤ \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE ê´€ì ì—ì„œ $\\hat\\theta$ ìš°ë¦¬ê°€ ìƒì‹ì ìœ¼ë¡œ ìƒê°í•˜ê³  ìˆëŠ” í™•ë¥ ì´ MLE(Maximum Liklihood Estimation)ìœ¼ë¡œ êµ¬í•œ ê²ƒì´ë‹¤ \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(ì‚¬ì „ ì§€ì‹)ì„ ê³ ë ¤í•œë‹¤ MLE(Maximum Liklihood Estimation)ê³¼ ë‹¤ë¥´ê²Œ ì¼ì–´ë‚œ ì‚¬ê±´ë§Œì„ ê³ ë ¤í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë‹¤ MLEëŠ” $P(D\\mid\\theta)$ë¥¼ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAPëŠ” $P(\\theta\\mid D)$ ì¦‰ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ë•Œ $\\theta$ì˜ í™•ë¥  ì‚¬í›„í™•ë¥ (Posterior)ì„ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAP ê³„ì‚° 1. ë² ì´ì¦ˆ ì •ë¦¬(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}ğ‘·ğ’ğ’”ğ’•ğ’†ğ’“ğ’Šğ’ğ’“=\\frac{ğ‘³ğ’Šğ’Œğ’†ğ’ğ’Šğ’‰ğ’ğ’ğ’… \\cdot ğ‘·ğ’“ğ’Šğ’ğ’“ ğ‘²ğ’ğ’ğ’˜ğ’ğ’†ğ’…ğ’ˆğ’†}{ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•}2. ê´€ê³„ì‹ ì •ë¦¬ P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•$ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•ŠëŠ”ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì´ë¯¸ ì¼ì–´ë‚œ ì‚¬ê±´ì´ê³  ì •í•´ì ¸ ìˆê¸° ë•Œë¬¸ì´ë‹¤. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ ì‚¬ì „í™•ë¥  : ì‚¬ì „í™•ë¥  ë¶„í¬ê°€ ë² íƒ€ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤ P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. ì‚¬í›„í™•ë¥  P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP ê´€ì ì—ì„œ $\\hat\\theta$ ë§Œì•½ ë˜ì§„ íšŸìˆ˜ê°€ ë§ì•„ì§€ê²Œ ë˜ë©´ $\\alpha, \\beta$ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¯¸ë¹„í•´ì§€ë¯€ë¡œ ê²°êµ­ MLEë¡œ êµ¬í•œ ê²ƒê³¼ ê°™ì•„ì§€ê²Œ ëœë‹¤ \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-02-26T06:47:49.508Z","updated":"2019-02-26T06:47:49.508Z","comments":true,"path":"2019/02/26/hello-world/","link":"","permalink":"http://progresivoJS.github.io/2019/02/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}
{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"Latent Dirichlet Allocation","slug":"LDA","date":"2019-05-13T14:11:15.000Z","updated":"2019-05-13T14:11:15.123Z","comments":true,"path":"2019/05/13/LDA/","link":"","permalink":"http://progresivoJS.github.io/2019/05/13/LDA/","excerpt":"","text":"Topic Modeling Generative Process $\\theta_i \\sim Dir(\\alpha), i \\in \\{1, \\ldots, M\\}$ $\\phi_k \\sim Dir(\\beta), k \\in \\{1, \\ldots, K\\}$ $z_{i,l} \\sim Mult(\\theta_i), i \\in \\{1, \\ldots, M\\}, l \\in \\{1, \\ldots, N\\}$ $w_{i,l} \\sim Mult(\\phi_{z_{i,l}}), i \\in \\{1, \\ldots, M\\}, l \\in \\{1, \\ldots, N\\}$ 단어 $\\boldsymbol{w}$는 $\\boldsymbol{\\phi_z}$ word-topic 분포에 의해 generated 된다 $\\boldsymbol{z}$ topic은 $\\boldsymbol{\\theta}$ document-topic 분포에 의해 generated 된다 $\\boldsymbol{\\theta}$ document-topic 분포는 $\\boldsymbol{\\alpha}$ 분포에 의해 generated 된다 $\\boldsymbol{\\phi}$ word-topic 분포는 $\\boldsymbol{\\beta}$ 분포에 의해 generated 된다 만약 우리가 $Z$ 분포를 가지고 있다면 $\\theta$와 $\\phi$를 likely하게 구할수 있다 $\\theta$ : Topic distribution in a document $\\phi$ : Word distribution in a topic $Z$를 구하는 것이 $\\theta$와 $\\phi$를 추론하는 중요한 키가 된다 Gibbs Sampling on Z (1) Gibbs Sampling을 활용하여 가장 likely한 $Z$를 구한다 factorization 으로 시작한다 $P(W, Z, \\theta, \\phi; \\alpha, \\beta)$ = \\prod_{i=1}^{K} P(\\phi_i;\\beta) \\prod_{j=1}^{M} P(\\theta_j;\\alpha) \\prod_{l=1}^{N} P(Z_{j,l} \\mid \\theta_j) P(W_{j,l} \\mid \\phi_{Z_{j,l}}) $\\theta$와 $\\phi$를 collapse시키려고 식을 전개한다 왜냐하면 W는 Data point이고, Z 는 Sampling target이고 $\\alpha, \\beta$는 prior 이기 때문에 살려야 한다 $P(W, Z; \\alpha, \\beta) = \\int_{\\theta} \\int_{\\phi} P(W, Z, \\theta, \\phi ; \\alpha, \\beta)d\\phi d\\theta$ $= \\int_{\\phi} \\prod_{i=1}^{K} P(\\phi_i;\\beta) \\prod_{j=1}^{M} \\prod_{l=1}^{N} P(W_{j,l} \\mid \\phi_{Z_{j,l}}) d\\phi \\times \\int_{\\theta} \\prod_{j=1}^{M} P(\\theta_j;\\alpha) \\prod_{l=1}^{N} P(Z_{j,l} \\mid \\theta_j) d\\theta$ $= (1) \\times (2)$ (1)번 식을 먼저 풀어본다 $(1) = \\int_{\\phi} \\prod_{i=1}^{K} P(\\phi_i;\\beta) \\prod_{j=1}^{M} \\prod_{l=1}^{N} P(W_{j,l} \\mid \\phi_{Z_{j,l}}) d\\phi$ $= \\prod_{i=1}^{K} \\int_{\\phi_i} P(\\phi_i;\\beta) \\prod_{j=1}^{M} \\prod_{l=1}^{N} P(W_{j,l} \\mid \\phi_{Z_{j,l}}) d\\phi_i$ $= \\prod_{i=1}^{K} \\int_{\\phi_i} \\frac{\\Gamma(\\sum_{v=1}^{V} \\beta_v)} {\\prod_{v=1}^{V} \\Gamma(\\beta_v)} \\prod_{v=1}^{V}\\phi_{i,v}^{\\beta_v -1} \\prod_{j=1}^{M} \\prod_{l=1}^{N} P(W_{j,l} \\mid \\phi_{Z_{j,l}}) d\\phi_i$ 여기서 v는 unique 단어의 갯수이다(dimension) $n_{j,r}^i$ : $j$번째 document에 있는 $i$번째 topic의 성질을 가진 $r$번째 unique한 단어이다 $n_{(.),v}^i$ : 모든 문서를 보고 $i$라는 topic의 성질을 가진 unique한 단어 $v$의 갯수 $=\\prod_{i=1}^{K} \\int_{\\phi_i} \\frac{\\Gamma(\\sum_{v=1}^{V} \\beta_v)} {\\prod_{v=1}^{V} \\Gamma(\\beta_v)} \\prod_{v=1}^{V}\\phi_{i,v}^{\\beta_v -1} \\prod_{v=1}^V \\phi_{i,v}^{n_{(.),v}^i} d\\phi_i$ $=\\prod_{i=1}^{K} \\int_{\\phi_i} \\frac{\\Gamma(\\sum_{v=1}^{V} \\beta_v)} {\\prod_{v=1}^{V} \\Gamma(\\beta_v)} \\prod_{v=1}^{V}\\phi_{i,v}^{n_{(.),v}^i + \\beta_v -1}d\\phi_i$ $=\\prod_{i=1}^{K} \\frac{\\prod_{v=1}^{V} \\Gamma \\left(n_{(.),v}^i + \\beta_v \\right) \\Gamma \\left(\\sum_{v=1}^V \\beta_v\\right)} {\\prod_{v=1}^V \\Gamma \\left( \\beta_v\\right) \\Gamma \\left( \\sum_{v=1}^V n_{(.),v}^i + \\beta_v \\right)} \\int_{\\phi_i} \\frac{\\Gamma \\left( \\sum_{v=1}^V n_{(.),v}^i + \\beta_v \\right)} {\\prod_{v=1}^{V} \\Gamma \\left(n_{(.),v}^i + \\beta_v \\right)} \\prod_{v=1}^{V}\\phi_{i,v}^{n_{(.),v}^i + \\beta_v -1}d\\phi_i$ $=\\prod_{i=1}^{K} \\frac{\\prod_{v=1}^{V} \\Gamma \\left(n_{(.),v}^i + \\beta_v \\right) \\Gamma \\left(\\sum_{v=1}^V \\beta_v\\right)} {\\prod_{v=1}^V \\Gamma \\left( \\beta_v\\right) \\Gamma \\left( \\sum_{v=1}^V n_{(.),v}^i + \\beta_v \\right)}$ (2)번 식을 먼저 풀어본다 $\\int_{\\theta} \\prod_{j=1}^{M} P\\left(\\theta_j;\\alpha\\right) \\prod_{l=1}^{N} P\\left(Z_{j,l} \\mid \\theta_j\\right) d\\theta$ $= \\prod_{j=1}^{M} \\int_{\\theta_j} P\\left(\\theta_j;\\alpha\\right) \\prod_{l=1}^{N} P\\left(Z_{j,l} \\mid \\theta_j\\right) d\\theta_j$ $= \\prod_{j=1}^{M} \\int_{\\theta_j} \\frac{\\Gamma \\left( \\sum_{k=1}^K \\alpha_k \\right)} {\\prod_{k=1}^{K} \\Gamma \\left( \\alpha_k \\right)} \\prod_{k=1}^K \\theta_{j,k}^{\\alpha_k -1} \\prod_{l=1}^N P \\left( Z_{j,l} \\mid \\theta_j\\right)d \\theta_j$ $n_{(.),v}^i$ : 모든 단어는 다 포함하고 $i$라는 topic의 성질을 가진 unique한 단어 $v$의 갯수","categories":[],"tags":[]},{"title":"Regularization and model selection","slug":"Regularization-and-modelselection","date":"2019-05-08T15:48:24.000Z","updated":"2019-05-08T15:48:24.735Z","comments":true,"path":"2019/05/09/Regularization-and-modelselection/","link":"","permalink":"http://progresivoJS.github.io/2019/05/09/Regularization-and-modelselection/","excerpt":"","text":"다음과 같은 hypothesis $h_\\theta(x)=g(\\theta_0 + \\theta_1 x + \\ldots + \\theta_k x^k)$ 가 있을때 $k$를 몇으로 해야 할지 어떻게 정할수 있을까? 즉 몇차로 식을 만드는게 bias / variance trade off 에 있어서 가장 좋은 모델일까를 어떻게 알수 있을까?조금 구체적으로 이해 하기 위해 유한개의 모델을 다음과 같이 정의하겠다. $M={M_1, \\ldots, M_d}$ 여기서 $M_i$는 $i$차수의 다항 회귀 모델이다. 1. Cross Validation training set $S$가 있다면 우리는 empirical risk minimization 알고리즘에 의해 다음과 같은 과정을 거쳐 최적의 hypothesis를 찾는다. 데이터셋에 각각의 $M_i$ 모델을 적용한후 hypothesis $h_i$를 얻는다. 가장 낮은 training error를 낸 hypotheses 를 선택한다 위의 방법은 효과적이지는 않다.분명 높은 차수의 hypothesis가 더 낮은 training error를 발생 시킬것이기 때문이다. 따라서 이 방법은 항상 높은 variance, 높은 차수의 hypothesis가 선택 될것이다. 더 효율이 높은 알고리즘은 Cross validation 방법이다. 방법은 다음과 같다. 랜덤하게 데이터셋 $S$를 70%는 $S_{train} : $ training 데이터로, 나머지 30% $S_{cv}$(hold out cross validation set)으로 나눈다. hypothesis $h_i$를 얻기 위해 $S_{train}$ 데이터만 사용한다. 그리고 가장 작은 에러($\\hat \\epsilon_{S_{cv}}$)를 낸 hypothesis를 선택하면 된다. (여기서 $\\hat \\epsilon_{S_{cv}}$ 는 $S_{cv}$ 데이터에 대한 empirical error를 나타낸 것이다.) $S_{train}$ 데이터로 학습하며 얻은 $h_i$로 학습할때 사용하지 않았던 $S_{cv}$로 테스트를 해보면 $h_i$의 true generalization error를 측정할수 있다. 그 다음 가장 작은 generalization error를 가진 hypothesis를 선택하면 된다. 보통의 경우 데이터의 $1/4, 1/3$ 정도를 cross validation set으로 사용한다. 하지만 위 방법은 30% 정도의 data를 training 할때 쓰지 못한다는 단점이 있다. 학습 할때 즉 항상 $0.7m$의 데이터만 사용하고 전체 데이터 $m$을 사용을 못한다는 것이다. 물론 충분한 엄청 많은 데이터가 있다면 상관없지만… 데이터가 충분하지 않다면 다음 K-fold Cross Validation 방법을 쓰는것이 조금더 데이터를 활용할수 있을것이다. 렌덤하게 데이터 $S$를 $m/k$개로 이루어진 데이터 $k$개로 나눠 준다. 그리고 각각의 데이터 셋을 $S_1, S_2, \\ldots, S_k$로 지정한다. 각각의 모델 $M_i$ 에 대해 다음을 실행한다. For $j = 1, 2, \\ldots, k$ S_j 데이터 셋만 제외하고 나머지 데이터 $S_1 \\cup \\cdots \\cup S_{j-1} \\cup S_{j+1} \\cup \\cdots S_k$ 를 가지고 학습을 시킨다음 $h_{ij}$ hypothesis를 얻는다. 그리고 나서 $S_j$데이터를 가지고 테스트를 한후 $\\widehat \\epsilon (S_j)$ 를 찾는다 $M_i$의 generalization error는 $\\widehat \\epsilon (S_j)$ 평균으로 구한다. generalization error가 가장 낮은 $M_i$을 구한 다음 다시 전체 데이터 셋 $S$를 가지고 재학습을 하여 얻어진 hypothesis가 최종 답이 된다. 데이터를 나눌때 $1/k$로 전 cross validation보다 훨씬 적게 test로 사용되긴 하였지만, 각각의 모델마다 k번의 계산이 필요하기 때문에 계산적으로는 비효율적이다. 이러한 교차 검증 방법은 single model / algorithm을 검증하고 싶을때 사용하면 좋은 검증 방법중에 하나가 될것이다. 2. Feature Selection model을 선택할때 중요하고 특별한 방법중의 하나가 Feature selection이다. 만약 supervised learning을 하려고 하는데 Feature의 갯수가 너무 많으면 ($n &gt;&gt; m$) 모델의 학습에 관련있는 몇개의 feature 만 선택해야 할것이다. simple한 선형 분류문제를 풀때 많은 수($n$)의 feature가 사용되면, hypothesis class의 VC dimension이 여전히 $O(n)$이기 때문에 학습 데이터가 많지 않은 한 overfitting 문제가 발생할수 있기 때문이다. 만약 n 개의 features가 있다면 feature를 사용할 모든 경우의 수는 $2^n$이 될것이다. feature selection 방법중에 forward selection는 다음과 같다. $\\mathcal{F}= \\emptyset$ Repeat { (a) For $i = 1, 2, \\ldots, n$ $if$ $i \\notin \\mathcal{F}$, $\\mathcal{F_i}=F \\cup \\{i\\}$로 놓는다. $\\mathcal{F_i}$를 평가하기 위해 cross validation 한다. generalization error를 구한다. (b) step (a)에서 자장 적합한 feature set $\\mathcal{F_i}$를 찾는} 전체 과정을 돌고 나서 가장 적합한 feature set을 선택한다. 전체 loop가 종료되는 시점은 The $\\mathcal{F}$가 전체 feature set $\\{1, \\ldots, n\\}$이 되었거나, 미리 지정해준 threshold hold $\\mid \\mathcal{F_i}\\mid$ 를 넘었을때이다. forward selection은 wrapper model feature selection 의 한 예시로 불리기도한다. forward search 와는 다른 방법인 backward search가 있다. 이 방법은 처음 feature set의 시작을 전체 set $\\mathcal{F} = \\{1, \\ldots, n\\}$으로 시작 한다. 그리고 반복적으로 하나씩 feature들을 지워가며 $\\mathcal{F}= \\emptyset$ 가 될때까지 최적의 feature set을 찾는다. 하지만 이러한 wrapper model feature selection 은 계산 복잡도가 모든 feature set $\\mathcal{F} = \\{1, \\ldots, n\\}$ 을 확인할때까지 $O(n^2)$가 된다. 그러한 이유로 비록 heuristic 하긴 하지만, 계산 복잡도가 조금 더 낮은 방법인 Filter feature selection 을 사용할수 있다. 여기서 사용하는 방법은 간단한 점수 $S(i)$를 활용한다. $S(i)$는 하나의 feature $x_i$가 라벨 $y$에 대해 얼마나 정보성이 있느냐의 점수이다. 점수가 큰 feature를 선택하게 된다. $S(i)$를 측정하는 한가지 방법은 학습 데이터에서 얻은 feature $x$ 와 라벨 $y$간의 서로의 correlation을 점수로 쓰는 것이다. 결국 라벨과 가장 강한 관계가 강한 feature가 점수가 높게 될것이다. 실제로는 특히 feature $x_i$의 값들이 discrete 할때는 $S(i)$를 상호 정보량($MI(x_i,y)$)으로 수치화 하여 사용한다 MI(x_i,y) = \\sum_{x_i \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_i, y) \\log \\frac{p(x_i, y)}{p(x_i)p(y)}위의 공식은 $x_i$와 $y$가 binary value로 이루어져 있을때이고,확률 $p(x_i, y)$, $p(x_i),p(y)$ training data의 empirical distribution에 의해 측정 된것이다 상호 정보량이 KL divergence로 표현될수 있다는 것을 이해하고 있으면 위 식을 조금더 직관적으로 이해할수 있을것이다. MI(x_i,y) = KL(p(x_i, y) \\mid \\mid p(x_i)p(y))KL divergence 는 확률 분포 $p(x_i, y)$와 $p(x_i)p(y)$가 얼마나 다른지 나타내는 것이다. 만약 $x_i$와 $y$가 서로 independent 하다면 $p(x_i, y) = p(x_i)p(y)$ 가 성립할것이고, 두 분포간의 KL divergence는 0이 될것이다. 또한 $x_i$가 명백히 $y$에 대한 정보량이 없다는 것을 의마기도 한다. 반대로, 만약 $x_i$가 $y$에 대한 정보량이 많으면 그들의 상호 정보량 $MI(x_i,y)$는 매우 클것이다. 자 그러면 스코어 S(i)를 구하고, 몇개의 feature를 사용할것인가를 정해야 한다. 가장 표준적인 방법은 가능한 k의 갯수를 cross validation을 통해서 구하는 것이다. 보통 이방법은 단어의 갯수가 많은 즉 feature의 갯수가 매우 많은 naive Bayes text classification을 할때 feature 갯수를 줄이기 위해 사용한다. 3. Bayesian statistics and regularization 이번에는 overfitting을 피하기 위해 쓰이는 방법을 알아보겠다. 우리는 parameter을 찾기 위해 최대 가능도 (maximum likelihood)를 사용한다. \\theta_{ML}=argmax_{\\theta}\\prod_{i=1}^m p(y^{(i)} \\mid x{(i)};\\theta)기존의 최대 가능도 방법은 $\\theta$를 random하게 지정하는게 아닌, 모르는 상수로 놓는다. 그러나, maximum likelihood 방법 말고 Bayesian 방법을 이용한 방법을 쓰게 되면 모르는 값 $\\theta$를 렌덤하게 놓고, 사전확률 분포 $P(\\theta)$를 지정한다.만약 학습 데이터 셋 $S=\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$ 이 주어졌을때 $\\theta$를 사후 확률 분포 $P(\\theta \\mid S)$ 통해 예측한다. \\begin{matrix} P(\\theta \\mid S) &=& \\frac{P(S \\mid \\theta) P(\\theta)}{P(S)} \\\\ &=& \\frac{\\left(\\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)},\\theta)\\right)p(\\theta)} {\\int_\\theta \\left(\\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)},\\theta)p(\\theta)\\right)d\\theta} \\end{matrix}위 식의 $p(y^{(i)}\\mid x^{(i)},\\theta)$는 learning 문제를 풀때마다 나오는 식이다. 만약 Bayesian logistic regression 문제를 풀면 $p(y^{(i)}\\mid x^{(i)},\\theta)=h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}$ 가 된다. $h_\\theta(x^{(i)})=1/(1+exp(-\\theta x^{(i)}))$ 새로운 x데이터에 대해 예측을 하려고 한다면 $\\theta$의 사후 확률분포를 활용하여 클래스에 대한 사후 확률 분포를 이용해서 구한다 $p(y \\mid x, S)=\\int_\\theta p(y \\mid x, \\theta)p(\\theta \\mid S)d\\theta$ $E[y \\mid x, S] = \\int_y yp(y\\mid x,S)dy$ x 데이터가 주어졌을때 $\\theta$에대한 사후 확률 분포$p(\\theta \\mid S)$에 관한 y의 평균으로 구할수 있다. 하지만 Bayesian 방법은 $\\theta$의 차수가 커지면 커질수록 계산 복잡도가 커지고, closed form도 아니라서 계산이 어렵다는 단점이 있다. 따라서 $\\theta$의 사후 확률 분포를 근사하여 구하게 되는데 이 방법을 MAP(maximum a posterior)라고 한다 $\\theta_{MAP}=argmax_\\theta \\prod_{i=1}^mp(y^{(i)}\\mid x^{(i)}, \\theta)p(\\theta)$ MAP 식을 보면 알겠지만 maximum likelihood와 다른 점은 단지 뒤에 $p(\\theta)$만 붙어 있다는 것이다. 뒤에 사전 확률 분포가 곱해져 있으므로 ML보다 overfitting에 대해 덜 민감한 장점이 있다","categories":[],"tags":[]},{"title":"Learning_Theory","slug":"Learning-Theory","date":"2019-05-07T01:46:02.000Z","updated":"2019-05-07T15:37:56.088Z","comments":true,"path":"2019/05/07/Learning-Theory/","link":"","permalink":"http://progresivoJS.github.io/2019/05/07/Learning-Theory/","excerpt":"","text":"Learning Theory Andrew ng lecture note 를 공부하며 정리한 자료입니다 1. Bias / Variance tradeoff 선형 회귀에서 데이터에 해당 하는 모델을 simple($y=\\theta_0 + \\theta_1 x$)하게 혹은 complex($y=\\theta_0 + \\theta_1 x + \\cdots + \\theta_4 x^4$)하게 만들수 있다. 4차 함수가 아무리 $y$(price) 예측을 잘한다고 할지라도, 새로운 데이터가 들어왔을때에는 정확하게 예측하기는 어렵다. 즉 학습 데이터로 학습한 complex 모델은 다른 데이터(집)에 대해서는 일반화 되지 않은것이라고 할수 있다. generalization error란 반드시 트레이닝 데이터로 나온 error를 측정하는게 아닌 새로운 테스트 데이터가 들어왔을때의 error를 나타낸다. 가장 왼쪽에 있는 모델과 오른쪽에 있는 모델 모두 generalization error가 높다. 하지만 두 모델의 error가 높은 이유는 다르다.먼저 왼쪽 모델을 보면 만약 $(x, y)$ 데이터가 서로 linear 관계가 아니라면 아무리 많은 데이터로 학습을 시켜도 정확하게 예측하기란 쉽지가 않다. 이런 경우 모델의 편향(bias)이 크다고 한다 또는 underfit 되었다고 한다. 오른쪽 모델인 경우 보통 학습 데이터가 적을때 $(x,y)$ 데이터 상관관계를 폭넓게 반영하지 못하는 경우이다. 이러한 경우 모델의 분산(variance)이 크다고 하고 overfit 되었다고 한다. bias, variance 서로 상충관계(trade off) 이다.simple 한 모델이라면 bias는 크고 variance는 작다. 반면에 complex 모델이라면 bias는 작지만 variance는 매욱 크게 된다. 2. Preliminaries 여기에서는 다음 3가지의 질문을 정의할것이다.첫째, bias variance tradeoff를 formal하게 할수 있을까?둘째, generalization error를 training error로 관계지을수 있을까?셋째, 학습 알고리즘이 잘 학습되고 있는지 증명할수 있는 조건들이 있는가? 위 질문에 답하기 위해 두개의 보조 정리를 애기해보자 Union bound $k$개의 모든 사건이 서로 독립적이지 않는 사건 $A_1, A_2, \\ldots, A_k$이 있다고 한다면 다음 식이 성립된다. P(A_1 \\cup A_2 \\cup \\cdots \\cup A_k) \\leq P(A_1) + P(A_2) + \\ldots + P(A_k)증명은 하지 않겠지만 직관적으로 이해해 보면 사건 간의 교집합이 존재할수 있기 때문에 각각의 사건의 확률의 합은 모든 사건 합의 확률보다는 크다는 것을 알수 있다. Hoeffding inequality 서로 동일한 베르누이 분포에서 나온 독립적인 확률 변수(iid) $Z_1, Z_2, \\ldots, Z_m$가 있다고 하자. $P(Z_i=1)=\\phi$, $P(Z_i=0)=1-\\phi$ 라고 표기하고, 확률변수의 평균은 $\\widehat \\phi = \\frac{1}{m}\\sum_{i=1}^m Z_i$ 라고 정의 하고, $\\gamma &gt; 0$ 보다 크다고 고정한다. learning theory에서는 $Chernoff bound$라고 하는 식은 다음과 같다. P(\\mid \\phi - \\widehat \\phi \\mid > \\gamma) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)식이 복잡해 볼수 있지만, 크게 어렵지 않다. 식을 직관적으로 이해해 보면, 만약 데이터의 갯수가 많으면 즉 $m$이 커지면 커질수록 확률변수의 평균($\\widehat \\phi$)과 측정값($\\phi$)의 차이가 클 확률이 줄어든다는 것이다. 이해를 좀더 쉽게 하기 위해서 label $y \\in \\{0,1\\}$ binary classification을 생각해 보자. 학습데이터 $S$는 $D$ 라는 특정 확률분포에서 $m$개 샘플되었다고 해보자. $S = \\{ (x^{(i)}, y^{(i)});i = 1,\\ldots,m \\}$다음과 같이 training error는 hypothesis $h$에 대해서 다음과 같이 쓸수 있다. \\widehat \\epsilon(h) = \\frac{1}{m} \\sum_{i=1}^m 1\\{h(x^{(i)}) \\neq y^{(i)}\\}training error가 아닌 우리가 알고 싶은 generalization error는 다음과 같이 나타낼수 있다. \\epsilon(h) = P_{(x,y) \\sim D}\\left( h(x) \\neq y\\right)분포 $D$에서 랜덤하게 나온 sample $(x^{(i)}, y^{(i)})$ 을 $h$가 잘못 분류한것을 나타낸다. 다시한번 언급하자면 training data는 분포 $D$에서 샘플링 된다는 것을 가정하는 것이다. 그럼 hypothesis $h_\\theta (x) =1\\{\\theta^T x \\geq 0\\}$의 파라미터 $\\theta$를 찾으려는 식을 알아보자. \\widehat \\theta = arg min _{\\theta} \\widehat\\epsilon (h_\\theta)식을 이해해보면, 즉 학습데이터에 대한 에러를 가장 줄이는 $\\widehat \\theta$를 찾는 것이다 조금 더 깊게 들어가 우리는 hypothesis class $\\mathcal{H}$를 정의할것이다. linear classification에서의 $\\mathcal{H}$는 다음과 같이 쓸수 있다. \\mathcal{H}=\\{h_\\theta : h_\\theta(x)=1 \\{\\theta^T x \\geq 0\\}, \\theta \\in \\mathbb{R}^{n+1}\\}$\\mathcal{X}$ input domain 들마다의 classifier 집합이다. \\widehat h = argmin_{h\\in \\mathcal{H}} \\widehat \\epsilon(h)hypothesis $h$는 모든 classifier hypothesis 집합 $\\mathcal{H}$ 속에서 가장 학습 데이터에 대한 오류가 작은 것이 $\\widehat h$가 될것이다. 3. The case of finite $\\mathcal{H}$ 유한개의 hypothesis 들의 집합 $\\mathcal{H}=\\{h_1, h_2, \\ldots, h_k\\}$ 로 정의할수 있고, $\\mathcal{X}$ input data에 대해 $\\{0,1\\}$로 mapping 해주는 함수가 $k$개의 있는 것과 같다. 여기서 우리는 두가지를 확인해야 한다.첫번째, 모든 h에 대해서 $\\widehat \\epsilon (h)$는 $\\epsilon(h)$가 될수 있다.두번째, 여기서 나온 error는 $\\widehat h$의 generalization error의 상한선(upper bound)이 된다. $h_i \\in \\mathcal{H}$를 하나 선택한다. 그리고 베르누이 확률변수($Z$)를 하나 정의한다. $Z$ 는 분포 $\\mathcal{D}$에서 샘플된 데이터$(x,y)\\sim \\mathcal{D}$를 $h_i$가 잘못 분류한 경우를 말한다. $Z=1\\{h_i(x) \\neq y\\}$ 학습 데이터는 동일한 분포 $\\mathcal{D}$에서 샘플 되었기 때문에 $Z$와 $Z_i$도 같은 분포에서 나온 확률 변수 임을 알수 있다. hypothesis의 학습데이터 오류는 확률변수 $Z$의 평균임을 알수 있다. \\widehat \\epsilon (h_i)=\\frac{1}{m}\\sum_{i=1}^m Z_j$\\widehat \\epsilon (h_i)$는 $m$개의 확률변수 $Z_j \\sim Bern\\left(\\phi = \\epsilon (h_i)\\right)$의 평균이다. 여기에서Hoeffding inequality 공식을 적용해보자. P(\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)식을 이해해 보면 hypothesis $h_i$의 학습데이터에 대한 오류가 generalization error 와 비슷해지는 확률은 데이터의 갯수($m$)이 많아지면 많아질수록 높아지게 된다. 우리는 더 나아가 $h_i$하나의 hypothesis 뿐만 아니라 모든 hypothesis($h \\in \\mathcal{H}$)에도 적용 되는지 알아보자. 문제를 풀기 위해 $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid &gt; \\gamma$를 하나의 사건 $A_i$로 표기하도록 하자. P(A_i) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)위에서 보았던 union bound 정리를 이용해 다음과 같이 정리할수 있다. \\begin{matrix} P\\left( \\exists h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma \\right) &=& P(A_1 \\cup A_2 \\cup \\cdots \\cup A_k) \\\\ &\\leq& \\sum_{i=1}^kP(A_i) \\\\ &\\leq& \\sum_{i=1}^k2 \\exp \\left( -2 \\gamma^2 m\\right) \\\\ &=& 2k \\exp \\left( -2 \\gamma^2 m\\right) \\\\ \\end{matrix}양변을 1에서 빼주면 다음과 같이 써줄수 있다. \\begin{matrix} P\\left( \\lnot \\exists h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma \\right) &=& P\\left( \\forall h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\gamma \\right) \\\\ &\\geq& 1-2k \\exp \\left( -2 \\gamma^2 m\\right) \\\\ \\end{matrix}위 식을 직관적으로 이해하면, $1-2k \\exp \\left( -2 \\gamma^2 m\\right)$ 이상의 확률로 $\\widehat \\epsilon (h_i)$는 $\\epsilon (h_i)$의 $\\gamma$ 범위 안에 있다. 이것을 uniform convergence 라고 부른다 Notation $\\lnot$ : not 이라는 의미입니다. 여기에서 관심있는 값은 $m$, $\\gamma$, 에러의 확률이다. $\\delta$ = $2k \\exp \\left( -2 \\gamma^2 m\\right) &gt; 0$이라고 fix하고, $\\gamma$도 특정한 값으로 fix 하면 데이터의 갯수 $m$는 얼마나 필요한지 확인힐수 있다. m \\geq \\frac{1}{2\\gamma ^2} \\log \\frac{2k}{\\delta}만약 데이터의 갯수($m$)가 위와 같이 충분히 있으면, 모든 $h \\in \\mathcal{H}$ 에 대해서 $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\gamma$ 일 확률은 최소 $1-\\delta$가 된다. 마찬가지로, 모든 $h \\in \\mathcal{H}$ 에 대해서 $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid &gt; \\gamma$ 일 확률은 최대 $\\delta$가 된다. 이러한 확률 bound는 우리가 얼마만큼의 데이터가 필요로 하는지 알수 있게 된다. 또한 $m$은 $\\log k$에 영향을 많이 받는다. 여기서 $k$는 hypothesis의 갯수이다. 이번에는 $m$과 $\\theta$를 고정하고 $\\gamma$를 유도하면 다음과 같다. \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}모든 $h \\in \\mathcal{H}$ 에 대해서 P\\left(\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}\\right) \\geq 1-\\deltageneralization error를 최소화하는 hypothesis $h$는 다음과 같이 정의할수 있다 $h^* = argmin_{h \\in \\mathcal{H}} \\epsilon (h)$ 즉 우리가 찾는 best model이 되는 것이다. \\epsilon(\\widehat h) \\leq \\widehat \\epsilon (\\widehat h) + \\gamma \\text { : 1번식}\\widehat \\epsilon (\\widehat h) + \\gamma \\leq \\widehat \\epsilon ( h^*) + \\gamma \\text { : 2번식}\\widehat \\epsilon ( h^*) + \\gamma \\leq \\epsilon ( h^*) + 2\\gamma \\text { : 3번식}위 식을 조금 더 이해하기 쉽게 notation을 다시한번 정리해보자. Notation $\\widehat h$ : training error를 가장 최소화 하는 hypothesis, $\\widehat h = argmin_{h \\in \\mathcal{H}} \\widehat \\epsilon (h)$ $\\widehat \\epsilon(\\widehat h)$ : hypothesis $\\widehat h$의 training error $\\epsilon(\\widehat h)$ : hypothesis $\\widehat h$의 generalization error 위 식에 대해서 순서대로 설명을 해보자면, 1번식 같은 경우는 $\\widehat h$의 generalization error는 training error 보다 $\\gamma$ 만큼 크다는 것이다. 2번식은 $\\widehat h$의 training error보다 $h^{star}$의 training error 가 더 크다는 것이다. $\\widehat h, h^{star}$의 정의를 잘 생각해보면 이해할수 있을것이다. 3번식은 $h^{star}$의 generalization error 보다 training error가 $\\gamma$ 만큼 더 크다는 뜻이다. 이제 다음과 같은 이론 공식을 유도할수 있다.$\\mid \\mathcal{H} \\mid$ = $k$, $m, \\delta$를 고정한다. 최소 $1 - \\delta$의 확률로 \\epsilon (\\widehat h) \\leq \\left(min_{h \\in \\mathcal{H}} \\epsilon (h)\\right) + 2 \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}$\\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}$ $min_{h \\in \\mathcal{H}} \\epsilon (h)=\\epsilon ( h^{*})$ 위 식에서 trade off 관계를 확인 할수 있다. 만약 hypothesis의 차수를 늘려주게 되면 generalization error ($\\epsilon ( h^{*})$) 는 줄어 들것이다. 하지만 hypothesis의 갯수(k)는 늘어가게 되어서 결국 이 것은 bias는 줄어들게 되지만 variance는 커지게 된다. 반대로 hypothesis의 차수가 줄어들면 generalization error는 커지게 되지만 그만큰 hypothesis의 갯수는 줄어들게 되어서 bias /variance 의 tradeoff 관계가 나타나게 된다.","categories":[],"tags":[]},{"title":"SVM","slug":"SVM-1","date":"2019-05-06T15:46:00.000Z","updated":"2019-05-06T15:46:00.796Z","comments":true,"path":"2019/05/07/SVM-1/","link":"","permalink":"http://progresivoJS.github.io/2019/05/07/SVM-1/","excerpt":"","text":"Support Vector Machine Andrew ng lecture note 를 공부하며 정리한 자료입니다. SVM을 이해하기 위해서는 Margin(마진)과 데이터를 분리해주는 경계선과 데이터 사이의 거리 Gap이 커야 한다는 것에 초점을 맞춰야 한다. 1. Margin 여기에서는 ‘confidence’라는 개념이 등장한다. confidence는 예측이 얼마나 맞는지에 대한 확신을 나타낸다. 그림을 보면 경계선(Seperating hyperplane) 근처에 있으면 Confidence가 낮고 즉 예측의 정확도가 낮아지고, 경계선에서 멀어질수록 Confidence가 높아지며 예측의 정확도가 높아진다. 2. Notation 여기에서 $x$ feature와 $y$ label를 사용한다. label $y$는 SVM에서 $y \\in \\{-1, 1\\}$ 로 지정해 준다. 사용할 파라미터는 $w$, $b$로 표기할것이다. classifier는 다음과 같다. h_{w,b}(x)=g(w^Tx + b)만약 $w^Tx + b \\geq 0$ 이면 $g(w^Tx + b)=1$ 이 되고, $w^Tx + b \\leq 0$ 이면 $g(w^Tx + b)=-1$ 이 된다. 3. Functional / geomeric margins 3.1 Functional margins training dataset $(x^{(i)},y^{(i)})$ 가 주어지면 functional margin $\\left(\\widehat\\gamma^{(i)} \\right)$은 다음과 같다. \\widehat\\gamma^{(i)}=y^{(i)} (w^Tx + b)식에서 보면 functional margin이 커지기 위해서는 $y^{(i)}=1$ 이면 $w^Tx + b$ 가 양의 부호로 커져야 하고, $y^{(i)}=-1$ 이면 $w^Tx + b$ 가 음의 부호로 커져야 한다. 또한 $y^{(i)} (w^Tx + b) &gt; 0$ 이면 예측이 맞았다는 뜻도 된다. 그러므로 functional margin은 confidence와 예측의 정확성을 나타낸다. 단 여기서 functional margin $\\left(\\widehat\\gamma \\right)$은 데이터 마다의 functional margins중에서 가장 작은 값이 functional margin이 된다. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}3.2 geomeric margins 여기에서 $w$는 경계선(seperating hyperplane)과 직교한다 A는 데이터중 하나인 $x^{(i)}$이고 라벨 $y=1$을 나타낸다 선 AB는 경계선과의 거리 $\\gamma ^{(i)}$로 나타낸다 점 B는 다음과 같이 나타낼수 있다 x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}$\\frac{w}{\\lVert w \\rVert}$ 는 unit vector를 나타낸다. 경계선 $w^Tx + b=0$을 나타내므로, 경계선 위의 점 B를 이용하면 $w^T \\left( x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}\\right)+b=0$인 식을 유도할수 있다 $\\gamma^{(i)}$에 대한 식으로 바꿔주면 다음과 같다 \\gamma^{(i)}=\\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert}geometric margins$\\left(\\gamma^{(i)}\\right)$는 다음과 같다 \\gamma^{(i)}=y^{(i)}\\left( \\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert} \\right)만약 $\\lVert w \\rVert =1$ 이면 결국 functional margin과 같다진다는 것을 알수있다 geometric margin도 데이터 마다의 geometric margin중에서 가장 작은 값이 geometric margin이 된다. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}4. The optimal margin classifier 마진을 최대화 하는 경계선을 찾아내는 것이 최적화한다는 것이다. \\begin{matrix} max_{\\gamma ,w,b} && \\gamma \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq \\gamma , i=1,\\ldots,m \\\\ && \\lVert w \\rVert=1 \\end{matrix}margin을 최대화 하는 최적화 문제이고, 모든 데이터 마다 최소한의 마진보다는 항상 크고, $\\lVert w \\rVert=1$은 결국 functional / geometric margin이 같다는 것을 나타낸다 위의 식은 풀기가 까다로운 식이므로 식을 다음과 같이 변형할수 있다 \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}위의 식을 직관적으로 이해해 보면, 예를 들어 만약 임의의 점을 $x$, 경계선 위의 점을 $x_p$ 라고 한다면 $x_p$에서 $w$방향으로 $\\gamma$만큼 이동한것이 $x$이다. x = x_p + r \\frac{w}{\\lVert w \\rVert}w \\cdot x + b = w \\left ( x_p + r \\frac{w}{\\lVert w \\rVert} \\right) + b = w \\cdot x_p + b + r \\frac{w \\cdot w}{\\lVert w \\rVert} $w \\cdot x_p + b = 0$ 이므로 ($x_p :$ 경계선 위의점) w \\cdot x + b = r \\lVert w \\rVertr = \\frac{w \\cdot x + b}{\\lVert w \\rVert} = \\frac{c}{\\lVert w \\rVert}$c:$ constant 결국 $\\gamma$를 최대화 한다는것은 $w$를 최소화 하는것과 같다 5. Lagrange duality 최적화 문제를 풀기 위해서는 Lagrange duality에 대해서 이해를 하고 있어야 한다. 강의 노트에서 나온 정도로만 간단히 정리해보겠다 \\begin{matrix} min_w && f(w) \\end{matrix}\\begin{matrix} s.t. && g_i(w) \\leq 0 \\\\ && h_i(w)=0 \\end{matrix}부등식 제약조건이 있는 것은 Primal optimization 문제라고 부른다 일반화된 라그랑지안식(generalized Lagrangian)은 다음과 같다 L(w,\\alpha , \\beta)=f(w)+\\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)여기에서 $\\alpha ,\\beta$는 Lagrange multipliers 이다 5.1 primal optimal problem \\theta_P(w)=max_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)\\theta_P(w)= \\begin{cases} f(x) & \\text{if }g_i(w) \\leq 0 \\text{, } h_i(w)=0 \\\\ \\infty & \\text{if } g_i(w) > 0 \\text{ or }h_i(w)\\neq0 \\end{cases}제약 조건을 모두 만족하면 $\\theta_P(w)=f(x)$가 되고 제약 조건하나라도 만족을못하면 무한대로 발산한다 결국 위식에서 알수 있는 것은 제약식을 모두 만족 시키려면 $\\theta_P(w)$를 가장 최소화 해야 된다는것을 알수 있다. p^*=min_w \\theta_P(w)=min_wmax_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)5.2 dual optimal problem dual problem은 primal problem과 반대로 $w$를 최소화하는 라그랑지안을 구하는 것이다 \\theta_D(\\alpha, \\beta) =min_w L(w,\\alpha , \\beta)d^{*}=max_{\\alpha ,\\beta : \\alpha >0} \\theta_D(\\alpha , \\beta)=max_{\\alpha ,\\beta : \\alpha >0} min_w L(w,\\alpha , \\beta)5.3 primal / dual optimal problem primal optimal 과 dual optimal 식은 min max를 자리바꾼것 말고는 다른게 없다. max min 함수가 min max보다 항상 작거나 같다. 하지만 특정한 조건하에서는 $d$ = $p$ 가 성립한다 $f$와 $g_i$가 convex, $h_i$가 affine하고 모든 $i$에 대해서 $g_i(w) &lt; 0$ 라고 가정하면 $w, \\alpha ,\\beta$는 반드시 존재하게 된다. $w$는 Primal problem의 solution이 되고, $\\alpha, \\beta$는 Dual problem의 solution이 된다 5.4 KKT conditions {\\partial\\over\\partial w_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,n{\\partial\\over\\partial \\beta_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,l\\alpha_i^* g_i(w^*)=0, \\text{ } i=1,\\ldots,kg_i(w^*)\\leq 0, \\text{ } i=1,\\ldots,k\\alpha_i^* \\geq0, \\text{ } i=1,\\ldots,k$w,\\alpha , \\beta$가 KKT 조건을 만족한다면 dual problem과 primal Problem이 같아지므로 두개 모두의 해가 된다 5. Optimal margin classifiers 다시 Margin classifiers문제로 돌아와 보자 \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx^{(i)} + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}제약식을 다음과 같이 고쳐 써줄수있다 g_i(w)=-y^{(i)} (w^Tx^{(i)} + b)+1 \\leq 0Functional margin은 1이되게 된다 다음 그림에서 margin이 가장 작은 점은 3개가 있다(두개의 negative 한개의 positive) 3개가 support vector가 된다. 또한 support vector에서는 $\\alpha$값이 절대 0이 되지 않는다. 그렇다는것은 KKT조건에 의해 $g_i(w)$가 0이 되어야 한다는것이다 Lagranian 식은 다음과 같다. L(w^*,\\alpha^* , \\beta^*)=\\frac{1}{2}\\lVert w \\rVert^2-\\sum_{i=1}^m \\alpha_i \\left[y^{(i)} (w^Tx^{(i)} + b)-1\\right]부등식 제약조건만 있으므로 $\\alpha_i$만 존재한다 dual problem을 푸기 위해서 $minimize_{w,b}L(w,\\alpha , \\beta)$ $\\alpha$는 고정한 상태에서 $w$, $b$에 대해서 미분을 해야한다 w에 대해 미분한다 \\nabla_w L(w,\\alpha , \\beta)= w -\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}=0w=\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}b에 대해 미분한다 {\\partial\\over\\partial b}L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i y^{(i)}=0미분해서 구한 w를 라그랑지안 식에 대입해주고 정리해주면 다음과 같은 식을 얻을수 있다 L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j \\left(x^{(i)}\\right)^T x^{(j)}-b\\sum_{i=1}^m\\alpha_i y^{(i)}$\\sum_{i=1}^m\\alpha_i y^{(i)}=0$이므로 최종식은 다음과 같다 L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j \\left(x^{(i)}\\right)^T x^{(j)}현재까지 w,b에 대해서 최소화 하는 문제를 풀었고 최종 dual problem을 풀기위해서는 제약식 $\\alpha &gt;0$ 을 포함한 $max_\\alpha W(\\alpha)$를 구해야 한다. dual problem 식은다음과 같다 max_{\\alpha} W(\\alpha)=\\sum_{i=1}^m\\alpha_i- \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j 다음의 식을 최대화하는 $\\alpha$를찾으면 $w=\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}$를 이용해 optimal $w$를 찾는다 $b$는 구해진 $w$를가지고 다음식에 대입하여 풀수 있다. b^{*}=-\\frac{max_{i:y^{(i)}=-1}w^{*T}x^{(i)}+min_{i:y^{(i)}=1}w^{*T}x^{(i)}}{2}추가적으로 $w=\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)}$를 이용하여 다음과 같은 식을 도출해낼수 있다. 이것은 나중에 kernel trick에서 사용하게 될것이다. w^Tx +b =\\left(\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)}\\right)x + b=\\sum_{i=1}^m \\alpha_i y^{(i)}+bclassification할때 예측하려는 input data ($x$)와 트레이닝 학습 데이터 ($x^{(i)}$)의 내적 합 으로 예측할수있다. 위 식은 kernel trick에서 사용된다.","categories":[],"tags":[]},{"title":"EM Algorithm","slug":"em-algorithm","date":"2019-05-05T14:29:55.000Z","updated":"2019-05-05T14:29:55.089Z","comments":true,"path":"2019/05/05/em-algorithm/","link":"","permalink":"http://progresivoJS.github.io/2019/05/05/em-algorithm/","excerpt":"","text":"latent variable 추론 clustering과 classification의 가장 큰 차이점은 숨어있는 변수가 있느냐 없느냐이다.clustering은 latent variable이 포함되어 있다classification은 observed variable로 분류 한다 $\\{ X, Z\\}$ : 모든 variable $X$ : 관측된(Observed) variable $Z$ : hidden(latent) Variable $\\theta$ : 분포 parameter latent variable을 marginal out 해주면 된다 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta)로그 속에 summation이 있으면 계산이 복잡해져서 결국에는 summation의 위치를 바꿔줘야 한다(Jensens’s inequality). 그리고 $q(z)$의 임의의 pdf를 넣어준다 \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z P(X,Z \\mid \\theta)\\right\\}l(\\theta) = \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\}CF) Jensen’s inequality $\\psi$가 convex function일때는 f \\left( \\frac{x+y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\leq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j} $\\psi$가 concave function일때는 f \\left( \\frac{x+y}{2}\\right) \\geq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\geq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j}로그는 Concave function이므로 jensen’s inequality에 의해 다음과 같이 로그 속의 summation을 다음과 같이 빼서 식을 정리할수 있다 l(\\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\} \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)}오른쪽항의 식을 정리해주면 \\sum_z \\left\\{q(z) \\ln P(X,Z \\mid \\theta) - q(z)\\ln q(z) \\right\\}첫번째 항은 $q(z)$의 가중평균이고 두번째 항은 $q(z)$가 확률분포라는 가정하에 엔트로피를 나타낸다. 따라서 다음과 같이 식을 notation 해줄 수 있다 Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)$Q(\\theta, q)$은 결국 $l(\\theta)$의 lower bound이다 이것을 최대화 시켜주면 $l(\\theta)$값도 같이 최대화 시켜줄수 있다는 것이다 단 항상 그런것은 아니다 왜냐하면 서로 inequality하기 때문이다 Maximizing lower bound(1) l(\\theta) \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)} = \\sum_z q(z) \\ln \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{q(z)}맨 오른쪽의 항을 다음과 같이 정리할수 있다 \\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)} + q(z)\\ln P(X \\mid \\theta) \\right\\}=\\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)}\\right\\} + \\ln P(X \\mid \\theta)다음을 최적화 시키기 위한 식으로 바꿔주기 위해 summation 속의 로그를 역수로 취해줌으로써 Kullback-Leiber divergence로 변형 시켜준다 L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}이 식을 보면 많은 뜻을 볼수 있다 기존의 우리가 maximize시키고 싶었던 $\\ln P(X \\mid \\theta)$에 KL divergence term을 빼주는 식이 되었다 즉 KL divergence term을 0에 가깝게 해주면 해줄수록 우리의 objective function에 가까워 진다KL divergence term이 0에 가까워 진다는 것은 $q(z)$ 분포 모양과 $P(Z \\mid X, \\theta)$의 분포 모양이 비슷해 진다는 것을 의미한다 Maximizing lower bound(2) Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}우리가 $L(\\theta, q)$를 구한 이유는우리가 제일 처음 구한 $Q(\\theta, q)$만 가지고 optimal value를 구하는 것은 쉽지 않다. 왜냐하면 $q(z)$를 업데이트하는 방법에 대한 정확한 지식이 없기 때문이다하지만 $L(\\theta, q)$를 가지고 $q(z)$를 업데이트 하기는 쉽다 임의의 $\\theta$에 대해 $q(z)$는 $P(Z \\mid X, \\theta)$의 분포와 비슷하게 Update해주면 된다 KL\\left(q(Z) \\lVert P(Z \\mid X, \\theta)\\right) = 0 \\to q^t(z) = P(Z \\mid X, \\theta^t)특정한 iteration동안 $\\theta^t$로 되면 $q^t(z)$로 업그레이드 된다 Q(\\theta, q^t) = E_{q^t(z)} \\ln P(X,Z \\mid \\theta^t) + H(q^t)\\theta^{t+1} = argmax_{\\theta}Q(\\theta, q^t) = argmax_{\\theta}E_{q^t(z)} \\ln P(X,Z \\mid \\theta)그러면 업그레이드 된 $q^t$를 가지고 다시 $\\theta$를 업그레이드 해준다 정리 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta) 1. EM 알고리즘은 latent variable이 포함된 maximum likelihood를 찾는것이다 2. 처음에는 $\\theta$를 랜덤으로 정해준다 3. likelihood가 converge할때 까지 iteration을 돌려준다","categories":[],"tags":[]},{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend 를 공부하며 번역하여서 올립니다 각 영화에 대해서 평점이 5점까지 줄수 있다고 가정한다 Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : 총 유저의 명(수) $n_m$ : 총 영화의 갯수 $r(i,j)$ : $j$라는 유저가 영화 $i$에 평점을 줬으면 1 $r(i,j)$ : $r(i,j) = 1$이라는 조건 하에 user $j$가 movie $i$에게 준 평점 점수 우리는 주어진 데이터를 가지고 missing value를 predict해야 한다 1. Content_based algorithm content_based는 content가 어떤 특정한 잠재 feature가 있을거라고 생각하고 그 feature vector를 적용하는 것이다.예를 들어 영화를 추천하는 거라면 영화에는 각 장르가 존재한다. 영화의 로맨스 장르 정도, 액션 정도를 가중치 개념으로 주어서 각 영화의 latent feature vector를 지정해준다 여기에서는 romance를 $x_1$, action $x_2$의 feature vector를 만들어준다 extra(constant) feature도 넣어준다 Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 각각의 영화는 $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ 각각의 영화 $x^i$은 feature vector를 가지고 있다에를 들어 영화 Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ 의 feature vector를 가지고 있다 content_based 방식에서는 각각의 content feature vector에 따른 user parameter vector를 learning시켜야한다각각의 user마다 각각의 평점은 linear regression방식으로 나타난다 만약 Alice의 parameter vector($\\theta^1$)이 $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ 이라고 한다면 Alice가 Cute love 영화에 평점을 줄 점수는 $(\\theta^{(1)})^T x^{(3)}$ inner product를 해주면 4.95 평점을 줄거라는 예측이 나온다 $\\theta$를 learning 해보자 Notation n : feature의 dimension(constant를 제외한것) 여기서는 2(romance, action)이다 $m^j$ : $j$ user가 평점을 준 영화의 갯수 $j$ user가 평점을 준 영화에 한해서 $j$ user의 영화에 준 예측 평점과 실제 평점의 차를 최소화 해야한다 min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2최적화 하는데 $m^{(j)}$는 필요 없으므로 없애주어도 된다 min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2이거를 모든 user에게 적용시켜야 한다. 우리의 objective function이 된다 J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$에 대해서 미분을 해준다 $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ k가 0인것과 K가 0이 아닌것의 뜻은 feature의 constant term을 하느냐 안하느냐 이다 우리의 Obejctive function의 최종식은 {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_based는 각 content에 대한 feature를 알아야 한다는 단점이 있다. 다음에 알아볼 방법은 이거를 보완해준 Colloaborative filtering방법이다 2. Collaborative filtering content_based 방식에서는 content feature를 알고 있는 상태였다. 하지만 현실에서는 불가능하다 또한 Feature의 갯수를 조금더 많이 알기를 원한다 user의 영화 취향 벡터 $\\theta$와 각 영화의 장르 feature 벡터 $x$를 서로 교차적으로 learning Notation $n_m$ : 영화의 갯수 $n_u$ : user의 수 $\\theta$를 랜덤적으로 Initialize한다 $\\theta$를 가지고 $x$를 update시킨다 \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 update된 $x$를 가지고 $\\theta$를 Update시킨다 \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_based와 다른점은 costant Term을 넣어주지 않는다는 것이다 cost function J를 최대한 줄여주는 벡터를 찾는다 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5개의 영화와 4명의 user matrix이다 predicted rating은 다음과 같이 나타낼수 있다 X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점 $e_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점과 예측 평점의 차이 $p_{i,k}$ : $i$ 유저의 latent feature vector $q_{k,j}$ : $j$ 영화의 latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : 유저들의 Latent Matrix / shape : $\\left( \\text{유저 명수 (X) Latent 갯수}\\right)$ $Q$ : 영화들의 Latent Matrix / shape : $\\left( \\text{ Latent 갯수 (X) 영화 갯수 }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{‘}$, $q_{k,j}^{‘}$ 각각 벡터이다","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) 최대가능도 Notation $D$ : Data (관측한(Observed) 데이터) $\\theta$ : parameter (확률)&lt;/font&gt; $H$ : 앞면이 나온 횟수 $T$ : 뒷면이 나온 횟수 Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ 를 가장 높여주는 $\\theta$ 를 구하는것 MLE 계산 1. Maximum Liklihood 식 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ 을 씌운다 곱셈은 계산이 복잡하므로 $ln$를 씌워준다 → $log$ $function$ : 곱을 합으로 바꿔준다 로그는 단조 증가 함수이므로 $\\underset{\\theta}{\\operatorname{argmax}}$ 의 값은 변하지 않는다 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ 에 대해서 미분을 한다(derivative) 구하고자 하는 $\\theta$에 대해 미분한 값이 $0$이 되도록 식을 세운다 미분한 값이 $0$ 되게 하는 $\\theta$값을 구한다 \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE 관점에서 $\\hat\\theta$ 우리가 상식적으로 생각하고 있는 확률이 MLE(Maximum Liklihood Estimation)으로 구한 것이다 \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(사전 지식)을 고려한다 MLE(Maximum Liklihood Estimation)과 다르게 일어난 사건만을 고려하는것이 아니다 MLE는 $P(D\\mid\\theta)$를 최대화 하는 $\\theta$를 구하는 것이다 MAP는 $P(\\theta\\mid D)$ 즉 데이터가 주어졌을때 $\\theta$의 확률 사후확률(Posterior)을 최대화 하는 $\\theta$를 구하는 것이다 MAP 계산 1. 베이즈 정리(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}𝑷𝒐𝒔𝒕𝒆𝒓𝒊𝒐𝒓=\\frac{𝑳𝒊𝒌𝒆𝒍𝒊𝒉𝒐𝒐𝒅 \\cdot 𝑷𝒓𝒊𝒐𝒓 𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆}{𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕}2. 관계식 정리 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕$은 크게 중요하지 않는다. 주어진 데이터는 이미 일어난 사건이고 정해져 있기 때문이다. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ 사전확률 : 사전확률 분포가 베타 분포를 따른다고 가정한다 P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. 사후확률 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP 관점에서 $\\hat\\theta$ 만약 던진 횟수가 많아지게 되면 $\\alpha, \\beta$가 미치는 영향은 미비해지므로 결국 MLE로 구한 것과 같아지게 된다 \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]}]}
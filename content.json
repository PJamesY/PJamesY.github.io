{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"SVM","slug":"SVM","date":"2019-05-06T14:35:45.000Z","updated":"2019-05-06T14:35:45.025Z","comments":true,"path":"2019/05/06/SVM/","link":"","permalink":"http://progresivoJS.github.io/2019/05/06/SVM/","excerpt":"","text":"Support Vector Machine Andrew ng lecture note ë¥¼ ê³µë¶€í•˜ë©° ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤ SVMì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” Margin(ë§ˆì§„)ê³¼ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ì£¼ëŠ” ê²½ê³„ì„ ê³¼ ë°ì´í„° ì‚¬ì´ì˜ ê±°ë¦¬ Gapì´ ì»¤ì•¼ í•œë‹¤ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì¶°ì•¼ í•œë‹¤ 1. Margin ì—¬ê¸°ì—ì„œëŠ” â€˜confidenceâ€™ë¼ëŠ” ê°œë…ì´ ë“±ì¥í•œë‹¤ confidenceëŠ” ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ë§ëŠ”ì§€ì— ëŒ€í•œ í™•ì‹ ì„ ë‚˜íƒ€ë‚¸ë‹¤ ê·¸ë¦¼ì„ ë³´ë©´ ê²½ê³„ì„ (Seperating hyperplane) ê·¼ì²˜ì— ìˆìœ¼ë©´ Confidenceê°€ ë‚®ê³  ì¦‰ ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë‚®ì•„ì§€ê³ , ê²½ê³„ì„ ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ Confidenceê°€ ë†’ì•„ì§€ë©° ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§„ë‹¤. 2. Notation ì—¬ê¸°ì—ì„œ $x$ featureì™€ $y$ labelë¥¼ ì‚¬ìš©í•œë‹¤ label $y$ëŠ” SVMì—ì„œ $y \\in \\{-1, 1\\}$ ë¡œ ì§€ì •í•´ ì¤€ë‹¤ ì‚¬ìš©í•  íŒŒë¼ë¯¸í„°ëŠ” $w$, $b$ë¡œ í‘œê¸°í• ê²ƒì´ë‹¤. classifierëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ h_{w,b}(x)=g(w^Tx + b)ë§Œì•½ $w^Tx + b \\geq 0$ ì´ë©´ $g(w^Tx + b)=1$ ì´ ë˜ê³ , $w^Tx + b \\leq 0$ ì´ë©´ $g(w^Tx + b)=-1$ ì´ ëœë‹¤ 3. Functional / geomeric margins 3.1 Functional margins training dataset $(x^{(i)},y^{(i)})$ ê°€ ì£¼ì–´ì§€ë©´ functional margin $\\left(\\widehat\\gamma^{(i)} \\right)$ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ \\widehat\\gamma^{(i)}=y^{(i)} (w^Tx + b)ì‹ì—ì„œ ë³´ë©´ functional marginì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” $y^{(i)}=1$ ì´ë©´ $w^Tx + b$ ê°€ ì–‘ì˜ ë¶€í˜¸ë¡œ ì»¤ì ¸ì•¼ í•˜ê³ , $y^{(i)}=-1$ ì´ë©´ $w^Tx + b$ ê°€ ìŒì˜ ë¶€í˜¸ë¡œ ì»¤ì ¸ì•¼ í•œë‹¤. ë˜í•œ $y^{(i)} (w^Tx + b) &gt; 0$ ì´ë©´ ì˜ˆì¸¡ì´ ë§ì•˜ë‹¤ëŠ” ëœ»ë„ ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ functional marginì€ confidenceì™€ ì˜ˆì¸¡ì˜ ì •í™•ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤ ë‹¨ ì—¬ê¸°ì„œ functional margin $\\left(\\widehat\\gamma \\right)$ì€ ë°ì´í„° ë§ˆë‹¤ì˜ functional marginì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ ê°’ì´ functional marginì´ ëœë‹¤. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}3.2 geomeric margins ì—¬ê¸°ì—ì„œ $w$ëŠ” ê²½ê³„ì„ (seperating hyperplane)ê³¼ ì§êµí•œë‹¤ AëŠ” ë°ì´í„°ì¤‘ í•˜ë‚˜ì¸ $x^{(i)}$ì´ê³  ë¼ë²¨ $y=1$ì„ ë‚˜íƒ€ë‚¸ë‹¤ ì„  ABëŠ” ê²½ê³„ì„ ê³¼ì˜ ê±°ë¦¬ $\\gamma ^{(i)}$ë¡œ ë‚˜íƒ€ë‚¸ë‹¤ ì  BëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤ x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}$\\frac{w}{\\lVert w \\rVert}$ ëŠ” unit vectorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê²½ê³„ì„  $w^Tx + b=0$ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ê²½ê³„ì„  ìœ„ì˜ ì  Bë¥¼ ì´ìš©í•˜ë©´ $w^T \\left( x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}\\right)+b=0$ì¸ ì‹ì„ ìœ ë„í• ìˆ˜ ìˆë‹¤ $\\gamma^{(i)}$ì— ëŒ€í•œ ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ \\gamma^{(i)}=\\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert}geometric margins$\\left(\\gamma^{(i)}\\right)$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ \\gamma^{(i)}=y^{(i)}\\left( \\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert} \\right)ë§Œì•½ $\\lVert w \\rVert =1$ ì´ë©´ ê²°êµ­ functional marginê³¼ ê°™ë‹¤ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œìˆ˜ìˆë‹¤ geometric marginë„ ë°ì´í„° ë§ˆë‹¤ì˜ geometric marginì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ ê°’ì´ geometric marginì´ ëœë‹¤. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}4. The optimal margin classifier ë§ˆì§„ì„ ìµœëŒ€í™” í•˜ëŠ” ê²½ê³„ì„ ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ìµœì í™”í•œë‹¤ëŠ” ê²ƒì´ë‹¤. \\begin{matrix} max_{\\gamma ,w,b} && \\gamma \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq \\gamma , i=1,\\ldots,m \\\\ && \\lVert w \\rVert=1 \\end{matrix}marginì„ ìµœëŒ€í™” í•˜ëŠ” ìµœì í™” ë¬¸ì œì´ê³ , ëª¨ë“  ë°ì´í„° ë§ˆë‹¤ ìµœì†Œí•œì˜ ë§ˆì§„ë³´ë‹¤ëŠ” í•­ìƒ í¬ê³ , $\\lVert w \\rVert=1$ì€ ê²°êµ­ functional / geometric marginì´ ê°™ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤ ìœ„ì˜ ì‹ì€ í’€ê¸°ê°€ ê¹Œë‹¤ë¡œìš´ ì‹ì´ë¯€ë¡œ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í• ìˆ˜ ìˆë‹¤ \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}ìœ„ì˜ ì‹ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ ë³´ë©´, ì˜ˆë¥¼ ë“¤ì–´ ë§Œì•½ ì„ì˜ì˜ ì ì„ $x$, ê²½ê³„ì„  ìœ„ì˜ ì ì„ $x_p$ ë¼ê³  í•œë‹¤ë©´ $x_p$ì—ì„œ $w$ë°©í–¥ìœ¼ë¡œ $\\gamma$ë§Œí¼ ì´ë™í•œê²ƒì´ $x$ì´ë‹¤. x = x_p + r \\frac{w}{\\lVert w \\rVert}w \\cdot x + b = w \\left ( x_p + r \\frac{w}{\\lVert w \\rVert} \\right) + b = w \\cdot x_p + b + r \\frac{w \\cdot w}{\\lVert w \\rVert} $w \\cdot x_p + b = 0$ ì´ë¯€ë¡œ ($x_p :$ ê²½ê³„ì„  ìœ„ì˜ì ) w \\cdot x + b = r \\lVert w \\rVertr = \\frac{w \\cdot x + b}{\\lVert w \\rVert} = \\frac{c}{\\lVert w \\rVert}$c:$ constant ê²°êµ­ $\\gamma$ë¥¼ ìµœëŒ€í™” í•œë‹¤ëŠ”ê²ƒì€ $w$ë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤ 5. Lagrange duality ìµœì í™” ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œëŠ” Lagrange dualityì— ëŒ€í•´ì„œ ì´í•´ë¥¼ í•˜ê³  ìˆì–´ì•¼ í•œë‹¤. ê°•ì˜ ë…¸íŠ¸ì—ì„œ ë‚˜ì˜¨ ì •ë„ë¡œë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ë³´ê² ë‹¤ \\begin{matrix} min_w && f(w) \\end{matrix}\\begin{matrix} s.t. && g_i(w) \\leq 0 \\\\ && h_i(w)=0 \\end{matrix}ë¶€ë“±ì‹ ì œì•½ì¡°ê±´ì´ ìˆëŠ” ê²ƒì€ Primal optimization ë¬¸ì œë¼ê³  ë¶€ë¥¸ë‹¤ ì¼ë°˜í™”ëœ ë¼ê·¸ë‘ì§€ì•ˆì‹(generalized Lagrangian)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ L(w,\\alpha , \\beta)=f(w)+\\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)ì—¬ê¸°ì—ì„œ $\\alpha ,\\beta$ëŠ” Lagrange multipliers ì´ë‹¤ 5.1 primal optimal problem \\theta_P(w)=max_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)\\theta_P(w)= \\begin{cases} f(x) & \\text{if }g_i(w) \\leq 0 \\text{, } h_i(w)=0 \\\\ \\infty & \\text{if } g_i(w) > 0 \\text{ or }h_i(w)\\neq0 \\end{cases}ì œì•½ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ë©´ $\\theta_P(w)=f(x)$ê°€ ë˜ê³  ì œì•½ ì¡°ê±´í•˜ë‚˜ë¼ë„ ë§Œì¡±ì„ëª»í•˜ë©´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•œë‹¤ ê²°êµ­ ìœ„ì‹ì—ì„œ ì•Œìˆ˜ ìˆëŠ” ê²ƒì€ ì œì•½ì‹ì„ ë§Œì¡± ì‹œí‚¨ë‹¤ëŠ” ê²ƒì€ $\\theta_P(w)$ë¥¼ ê°€ì¥ ìµœì†Œí™” í•´ì•¼ ëœë‹¤ëŠ”ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤ p^*=min_w \\theta_P(w)=min_wmax_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)5.2 dual optimal problem dual problemì€ primal problemê³¼ ë°˜ëŒ€ë¡œ $w$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¼ê·¸ë‘ì§€ì•ˆì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ \\theta_D(\\alpha, \\beta) =min_w L(w,\\alpha , \\beta)d^*=max_{\\alpha ,\\beta : \\alpha >0} \\theta_D(\\alpha , \\beta)=max_{\\alpha ,\\beta : \\alpha >0} min_w L(w,\\alpha , \\beta)5.3 primal / dual optimal problem primal optimal ê³¼ dual optimal ì‹ì€ min maxë¥¼ ìë¦¬ë°”ê¾¼ê²ƒ ë§ê³ ëŠ” ë‹¤ë¥¸ê²Œ ì—†ë‹¤. max min í•¨ìˆ˜ê°€ min maxë³´ë‹¤ í•­ìƒ ì‘ê±°ë‚˜ ê°™ë‹¤. í•˜ì§€ë§Œ íŠ¹ì •í•œ ì¡°ê±´í•˜ì—ì„œëŠ” $d^ = p^$ê°€ ì„±ë¦½í•œë‹¤ $f$ì™€ $g_i$ê°€ convex, $h_i$ê°€ affineí•˜ê³  ëª¨ë“  $i$ì— ëŒ€í•´ì„œ $g_i(w) &lt; 0$ ë¼ê³  ê°€ì •í•˜ë©´ $w, \\alpha ,\\beta$ëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•˜ê²Œ ëœë‹¤. $w^$ëŠ” Primal problemì˜ solutionì´ ë˜ê³ , $\\alpha^, \\beta ^*$ëŠ” Dual problemì˜ solutionì´ ëœë‹¤ 5.4 KKT conditions {\\partial\\over\\partial w_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,n{\\partial\\over\\partial \\beta_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,l\\alpha_i^* g_i(w^*)=0, \\text{ } i=1,\\ldots,kg_i(w^*)\\leq 0, \\text{ } i=1,\\ldots,k\\alpha_i^* \\geq0, \\text{ } i=1,\\ldots,k$w^,\\alpha^ , \\beta^*$ê°€ KKT ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ë©´ dual problemê³¼ primal Problemì´ ê°™ì•„ì§€ë¯€ë¡œ ë‘ê°œ ëª¨ë‘ì˜ í•´ê°€ ëœë‹¤ 5. Optimal margin classifiers ë‹¤ì‹œ Margin classifiersë¬¸ì œë¡œ ëŒì•„ì™€ ë³´ì \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx^{(i)} + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}ì œì•½ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³ ì³ ì¨ì¤„ìˆ˜ìˆë‹¤ g_i(w)=-y^{(i)} (w^Tx^{(i)} + b)+1 \\leq 0Functional marginì€ 1ì´ë˜ê²Œ ëœë‹¤ ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ marginì´ ê°€ì¥ ì‘ì€ ì ì€ 3ê°œê°€ ìˆë‹¤(ë‘ê°œì˜ negative í•œê°œì˜ positive) 3ê°œê°€ support vectorê°€ ëœë‹¤. ë˜í•œ support vectorì—ì„œëŠ” $\\alpha$ê°’ì´ ì ˆëŒ€ 0ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë ‡ë‹¤ëŠ”ê²ƒì€ KKTì¡°ê±´ì— ì˜í•´ $g_i(w)$ê°€ 0ì´ ë˜ì–´ì•¼ í•œë‹¤ëŠ”ê²ƒì´ë‹¤ Lagranian ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. L(w^*,\\alpha^* , \\beta^*)=\\frac{1}{2}\\lVert w \\rVert^2-\\sum_{i=1}^m \\alpha_i \\left[y^{(i)} (w^Tx^{(i)} + b)-1\\right]ë¶€ë“±ì‹ ì œì•½ì¡°ê±´ë§Œ ìˆìœ¼ë¯€ë¡œ $\\alpha_i$ë§Œ ì¡´ì¬í•œë‹¤ dual problemì„ í‘¸ê¸° ìœ„í•´ì„œ $minimize_{w,b}L(w,\\alpha , \\beta)$ $\\alpha$ëŠ” ê³ ì •í•œ ìƒíƒœì—ì„œ $w$, $b$ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•´ì•¼í•œë‹¤","categories":[],"tags":[]},{"title":"EM Algorithm","slug":"em-algorithm","date":"2019-05-05T14:29:55.000Z","updated":"2019-05-05T14:29:55.089Z","comments":true,"path":"2019/05/05/em-algorithm/","link":"","permalink":"http://progresivoJS.github.io/2019/05/05/em-algorithm/","excerpt":"","text":"latent variable ì¶”ë¡  clusteringê³¼ classificationì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ìˆ¨ì–´ìˆëŠ” ë³€ìˆ˜ê°€ ìˆëŠëƒ ì—†ëŠëƒì´ë‹¤.clusteringì€ latent variableì´ í¬í•¨ë˜ì–´ ìˆë‹¤classificationì€ observed variableë¡œ ë¶„ë¥˜ í•œë‹¤ $\\{ X, Z\\}$ : ëª¨ë“  variable $X$ : ê´€ì¸¡ëœ(Observed) variable $Z$ : hidden(latent) Variable $\\theta$ : ë¶„í¬ parameter latent variableì„ marginal out í•´ì£¼ë©´ ëœë‹¤ P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta)ë¡œê·¸ ì†ì— summationì´ ìˆìœ¼ë©´ ê³„ì‚°ì´ ë³µì¡í•´ì ¸ì„œ ê²°êµ­ì—ëŠ” summationì˜ ìœ„ì¹˜ë¥¼ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤(Jensensâ€™s inequality). ê·¸ë¦¬ê³  $q(z)$ì˜ ì„ì˜ì˜ pdfë¥¼ ë„£ì–´ì¤€ë‹¤ \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z P(X,Z \\mid \\theta)\\right\\}l(\\theta) = \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\}CF) Jensenâ€™s inequality $\\psi$ê°€ convex functionì¼ë•ŒëŠ” f \\left( \\frac{x+y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\leq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j} $\\psi$ê°€ concave functionì¼ë•ŒëŠ” f \\left( \\frac{x+y}{2}\\right) \\geq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\geq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j}ë¡œê·¸ëŠ” Concave functionì´ë¯€ë¡œ jensenâ€™s inequalityì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì´ ë¡œê·¸ ì†ì˜ summationì„ ë‹¤ìŒê³¼ ê°™ì´ ë¹¼ì„œ ì‹ì„ ì •ë¦¬í• ìˆ˜ ìˆë‹¤ l(\\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\} \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)}ì˜¤ë¥¸ìª½í•­ì˜ ì‹ì„ ì •ë¦¬í•´ì£¼ë©´ \\sum_z \\left\\{q(z) \\ln P(X,Z \\mid \\theta) - q(z)\\ln q(z) \\right\\}ì²«ë²ˆì§¸ í•­ì€ $q(z)$ì˜ ê°€ì¤‘í‰ê· ì´ê³  ë‘ë²ˆì§¸ í•­ì€ $q(z)$ê°€ í™•ë¥ ë¶„í¬ë¼ëŠ” ê°€ì •í•˜ì— ì—”íŠ¸ë¡œí”¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ notation í•´ì¤„ ìˆ˜ ìˆë‹¤ Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)$Q(\\theta, q)$ì€ ê²°êµ­ $l(\\theta)$ì˜ lower boundì´ë‹¤ ì´ê²ƒì„ ìµœëŒ€í™” ì‹œì¼œì£¼ë©´ $l(\\theta)$ê°’ë„ ê°™ì´ ìµœëŒ€í™” ì‹œì¼œì¤„ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤ ë‹¨ í•­ìƒ ê·¸ëŸ°ê²ƒì€ ì•„ë‹ˆë‹¤ ì™œëƒí•˜ë©´ ì„œë¡œ inequalityí•˜ê¸° ë•Œë¬¸ì´ë‹¤ Maximizing lower bound(1) l(\\theta) \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)} = \\sum_z q(z) \\ln \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{q(z)}ë§¨ ì˜¤ë¥¸ìª½ì˜ í•­ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í• ìˆ˜ ìˆë‹¤ \\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)} + q(z)\\ln P(X \\mid \\theta) \\right\\}=\\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)}\\right\\} + \\ln P(X \\mid \\theta)ë‹¤ìŒì„ ìµœì í™” ì‹œí‚¤ê¸° ìœ„í•œ ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° ìœ„í•´ summation ì†ì˜ ë¡œê·¸ë¥¼ ì—­ìˆ˜ë¡œ ì·¨í•´ì¤Œìœ¼ë¡œì¨ Kullback-Leiber divergenceë¡œ ë³€í˜• ì‹œì¼œì¤€ë‹¤ L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}ì´ ì‹ì„ ë³´ë©´ ë§ì€ ëœ»ì„ ë³¼ìˆ˜ ìˆë‹¤ ê¸°ì¡´ì˜ ìš°ë¦¬ê°€ maximizeì‹œí‚¤ê³  ì‹¶ì—ˆë˜ $\\ln P(X \\mid \\theta)$ì— KL divergence termì„ ë¹¼ì£¼ëŠ” ì‹ì´ ë˜ì—ˆë‹¤ ì¦‰ KL divergence termì„ 0ì— ê°€ê¹ê²Œ í•´ì£¼ë©´ í•´ì¤„ìˆ˜ë¡ ìš°ë¦¬ì˜ objective functionì— ê°€ê¹Œì›Œ ì§„ë‹¤KL divergence termì´ 0ì— ê°€ê¹Œì›Œ ì§„ë‹¤ëŠ” ê²ƒì€ $q(z)$ ë¶„í¬ ëª¨ì–‘ê³¼ $P(Z \\mid X, \\theta)$ì˜ ë¶„í¬ ëª¨ì–‘ì´ ë¹„ìŠ·í•´ ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤ Maximizing lower bound(2) Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}ìš°ë¦¬ê°€ $L(\\theta, q)$ë¥¼ êµ¬í•œ ì´ìœ ëŠ”ìš°ë¦¬ê°€ ì œì¼ ì²˜ìŒ êµ¬í•œ $Q(\\theta, q)$ë§Œ ê°€ì§€ê³  optimal valueë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. ì™œëƒí•˜ë©´ $q(z)$ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •í™•í•œ ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤í•˜ì§€ë§Œ $L(\\theta, q)$ë¥¼ ê°€ì§€ê³  $q(z)$ë¥¼ ì—…ë°ì´íŠ¸ í•˜ê¸°ëŠ” ì‰½ë‹¤ ì„ì˜ì˜ $\\theta$ì— ëŒ€í•´ $q(z)$ëŠ” $P(Z \\mid X, \\theta)$ì˜ ë¶„í¬ì™€ ë¹„ìŠ·í•˜ê²Œ Updateí•´ì£¼ë©´ ëœë‹¤ KL\\left(q(Z) \\lVert P(Z \\mid X, \\theta)\\right) = 0 \\to q^t(z) = P(Z \\mid X, \\theta^t)íŠ¹ì •í•œ iterationë™ì•ˆ $\\theta^t$ë¡œ ë˜ë©´ $q^t(z)$ë¡œ ì—…ê·¸ë ˆì´ë“œ ëœë‹¤ Q(\\theta, q^t) = E_{q^t(z)} \\ln P(X,Z \\mid \\theta^t) + H(q^t)\\theta^{t+1} = argmax_{\\theta}Q(\\theta, q^t) = argmax_{\\theta}E_{q^t(z)} \\ln P(X,Z \\mid \\theta)ê·¸ëŸ¬ë©´ ì—…ê·¸ë ˆì´ë“œ ëœ $q^t$ë¥¼ ê°€ì§€ê³  ë‹¤ì‹œ $\\theta$ë¥¼ ì—…ê·¸ë ˆì´ë“œ í•´ì¤€ë‹¤ ì •ë¦¬ P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta) 1. EM ì•Œê³ ë¦¬ì¦˜ì€ latent variableì´ í¬í•¨ëœ maximum likelihoodë¥¼ ì°¾ëŠ”ê²ƒì´ë‹¤ 2. ì²˜ìŒì—ëŠ” $\\theta$ë¥¼ ëœë¤ìœ¼ë¡œ ì •í•´ì¤€ë‹¤ 3. likelihoodê°€ convergeí• ë•Œ ê¹Œì§€ iterationì„ ëŒë ¤ì¤€ë‹¤","categories":[],"tags":[]},{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend ë¥¼ ê³µë¶€í•˜ë©° ë²ˆì—­í•˜ì—¬ì„œ ì˜¬ë¦½ë‹ˆë‹¤ ê° ì˜í™”ì— ëŒ€í•´ì„œ í‰ì ì´ 5ì ê¹Œì§€ ì¤„ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : ì´ ìœ ì €ì˜ ëª…(ìˆ˜) $n_m$ : ì´ ì˜í™”ì˜ ê°¯ìˆ˜ $r(i,j)$ : $j$ë¼ëŠ” ìœ ì €ê°€ ì˜í™” $i$ì— í‰ì ì„ ì¤¬ìœ¼ë©´ 1 $r(i,j)$ : $r(i,j) = 1$ì´ë¼ëŠ” ì¡°ê±´ í•˜ì— user $j$ê°€ movie $i$ì—ê²Œ ì¤€ í‰ì  ì ìˆ˜ ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì§€ê³  missing valueë¥¼ predictí•´ì•¼ í•œë‹¤ 1. Content_based algorithm content_basedëŠ” contentê°€ ì–´ë–¤ íŠ¹ì •í•œ ì ì¬ featureê°€ ìˆì„ê±°ë¼ê³  ìƒê°í•˜ê³  ê·¸ feature vectorë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤.ì˜ˆë¥¼ ë“¤ì–´ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ê±°ë¼ë©´ ì˜í™”ì—ëŠ” ê° ì¥ë¥´ê°€ ì¡´ì¬í•œë‹¤. ì˜í™”ì˜ ë¡œë§¨ìŠ¤ ì¥ë¥´ ì •ë„, ì•¡ì…˜ ì •ë„ë¥¼ ê°€ì¤‘ì¹˜ ê°œë…ìœ¼ë¡œ ì£¼ì–´ì„œ ê° ì˜í™”ì˜ latent feature vectorë¥¼ ì§€ì •í•´ì¤€ë‹¤ ì—¬ê¸°ì—ì„œëŠ” romanceë¥¼ $x_1$, action $x_2$ì˜ feature vectorë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤ extra(constant) featureë„ ë„£ì–´ì¤€ë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 ê°ê°ì˜ ì˜í™”ëŠ” $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ ê°ê°ì˜ ì˜í™” $x^i$ì€ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ì—ë¥¼ ë“¤ì–´ ì˜í™” Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ ì˜ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ content_based ë°©ì‹ì—ì„œëŠ” ê°ê°ì˜ content feature vectorì— ë”°ë¥¸ user parameter vectorë¥¼ learningì‹œì¼œì•¼í•œë‹¤ê°ê°ì˜ userë§ˆë‹¤ ê°ê°ì˜ í‰ì ì€ linear regressionë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ ë§Œì•½ Aliceì˜ parameter vector($\\theta^1$)ì´ $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ ì´ë¼ê³  í•œë‹¤ë©´ Aliceê°€ Cute love ì˜í™”ì— í‰ì ì„ ì¤„ ì ìˆ˜ëŠ” $(\\theta^{(1)})^T x^{(3)}$ inner productë¥¼ í•´ì£¼ë©´ 4.95 í‰ì ì„ ì¤„ê±°ë¼ëŠ” ì˜ˆì¸¡ì´ ë‚˜ì˜¨ë‹¤ $\\theta$ë¥¼ learning í•´ë³´ì Notation n : featureì˜ dimension(constantë¥¼ ì œì™¸í•œê²ƒ) ì—¬ê¸°ì„œëŠ” 2(romance, action)ì´ë‹¤ $m^j$ : $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì˜ ê°¯ìˆ˜ $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì— í•œí•´ì„œ $j$ userì˜ ì˜í™”ì— ì¤€ ì˜ˆì¸¡ í‰ì ê³¼ ì‹¤ì œ í‰ì ì˜ ì°¨ë¥¼ ìµœì†Œí™” í•´ì•¼í•œë‹¤ min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ìµœì í™” í•˜ëŠ”ë° $m^{(j)}$ëŠ” í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì—†ì• ì£¼ì–´ë„ ëœë‹¤ min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ì´ê±°ë¥¼ ëª¨ë“  userì—ê²Œ ì ìš©ì‹œì¼œì•¼ í•œë‹¤. ìš°ë¦¬ì˜ objective functionì´ ëœë‹¤ J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•´ì¤€ë‹¤ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ kê°€ 0ì¸ê²ƒê³¼ Kê°€ 0ì´ ì•„ë‹Œê²ƒì˜ ëœ»ì€ featureì˜ constant termì„ í•˜ëŠëƒ ì•ˆí•˜ëŠëƒ ì´ë‹¤ ìš°ë¦¬ì˜ Obejctive functionì˜ ìµœì¢…ì‹ì€ {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_basedëŠ” ê° contentì— ëŒ€í•œ featureë¥¼ ì•Œì•„ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë‹¤ìŒì— ì•Œì•„ë³¼ ë°©ë²•ì€ ì´ê±°ë¥¼ ë³´ì™„í•´ì¤€ Colloaborative filteringë°©ë²•ì´ë‹¤ 2. Collaborative filtering content_based ë°©ì‹ì—ì„œëŠ” content featureë¥¼ ì•Œê³  ìˆëŠ” ìƒíƒœì˜€ë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ ë˜í•œ Featureì˜ ê°¯ìˆ˜ë¥¼ ì¡°ê¸ˆë” ë§ì´ ì•Œê¸°ë¥¼ ì›í•œë‹¤ userì˜ ì˜í™” ì·¨í–¥ ë²¡í„° $\\theta$ì™€ ê° ì˜í™”ì˜ ì¥ë¥´ feature ë²¡í„° $x$ë¥¼ ì„œë¡œ êµì°¨ì ìœ¼ë¡œ learning Notation $n_m$ : ì˜í™”ì˜ ê°¯ìˆ˜ $n_u$ : userì˜ ìˆ˜ $\\theta$ë¥¼ ëœë¤ì ìœ¼ë¡œ Initializeí•œë‹¤ $\\theta$ë¥¼ ê°€ì§€ê³  $x$ë¥¼ updateì‹œí‚¨ë‹¤ \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 updateëœ $x$ë¥¼ ê°€ì§€ê³  $\\theta$ë¥¼ Updateì‹œí‚¨ë‹¤ \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_basedì™€ ë‹¤ë¥¸ì ì€ costant Termì„ ë„£ì–´ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤ cost function Jë¥¼ ìµœëŒ€í•œ ì¤„ì—¬ì£¼ëŠ” ë²¡í„°ë¥¼ ì°¾ëŠ”ë‹¤ 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5ê°œì˜ ì˜í™”ì™€ 4ëª…ì˜ user matrixì´ë‹¤ predicted ratingì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤ X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì  $e_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì ê³¼ ì˜ˆì¸¡ í‰ì ì˜ ì°¨ì´ $p_{i,k}$ : $i$ ìœ ì €ì˜ latent feature vector $q_{k,j}$ : $j$ ì˜í™”ì˜ latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : ìœ ì €ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ìœ ì € ëª…ìˆ˜ (X) Latent ê°¯ìˆ˜}\\right)$ $Q$ : ì˜í™”ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ Latent ê°¯ìˆ˜ (X) ì˜í™” ê°¯ìˆ˜ }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{â€˜}$, $q_{k,j}^{â€˜}$ ê°ê° ë²¡í„°ì´ë‹¤","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) ìµœëŒ€ê°€ëŠ¥ë„ Notation $D$ : Data (ê´€ì¸¡í•œ(Observed) ë°ì´í„°) $\\theta$ : parameter (í™•ë¥ )&lt;/font&gt; $H$ : ì•ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ $T$ : ë’·ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ ë¥¼ ê°€ì¥ ë†’ì—¬ì£¼ëŠ” $\\theta$ ë¥¼ êµ¬í•˜ëŠ”ê²ƒ MLE ê³„ì‚° 1. Maximum Liklihood ì‹ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ ì„ ì”Œìš´ë‹¤ ê³±ì…ˆì€ ê³„ì‚°ì´ ë³µì¡í•˜ë¯€ë¡œ $ln$ë¥¼ ì”Œì›Œì¤€ë‹¤ â†’ $log$ $function$ : ê³±ì„ í•©ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤ ë¡œê·¸ëŠ” ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ $\\underset{\\theta}{\\operatorname{argmax}}$ ì˜ ê°’ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤(derivative) êµ¬í•˜ê³ ì í•˜ëŠ” $\\theta$ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì´ $0$ì´ ë˜ë„ë¡ ì‹ì„ ì„¸ìš´ë‹¤ ë¯¸ë¶„í•œ ê°’ì´ $0$ ë˜ê²Œ í•˜ëŠ” $\\theta$ê°’ì„ êµ¬í•œë‹¤ \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE ê´€ì ì—ì„œ $\\hat\\theta$ ìš°ë¦¬ê°€ ìƒì‹ì ìœ¼ë¡œ ìƒê°í•˜ê³  ìˆëŠ” í™•ë¥ ì´ MLE(Maximum Liklihood Estimation)ìœ¼ë¡œ êµ¬í•œ ê²ƒì´ë‹¤ \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(ì‚¬ì „ ì§€ì‹)ì„ ê³ ë ¤í•œë‹¤ MLE(Maximum Liklihood Estimation)ê³¼ ë‹¤ë¥´ê²Œ ì¼ì–´ë‚œ ì‚¬ê±´ë§Œì„ ê³ ë ¤í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë‹¤ MLEëŠ” $P(D\\mid\\theta)$ë¥¼ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAPëŠ” $P(\\theta\\mid D)$ ì¦‰ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ë•Œ $\\theta$ì˜ í™•ë¥  ì‚¬í›„í™•ë¥ (Posterior)ì„ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAP ê³„ì‚° 1. ë² ì´ì¦ˆ ì •ë¦¬(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}ğ‘·ğ’ğ’”ğ’•ğ’†ğ’“ğ’Šğ’ğ’“=\\frac{ğ‘³ğ’Šğ’Œğ’†ğ’ğ’Šğ’‰ğ’ğ’ğ’… \\cdot ğ‘·ğ’“ğ’Šğ’ğ’“ ğ‘²ğ’ğ’ğ’˜ğ’ğ’†ğ’…ğ’ˆğ’†}{ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•}2. ê´€ê³„ì‹ ì •ë¦¬ P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•$ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•ŠëŠ”ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì´ë¯¸ ì¼ì–´ë‚œ ì‚¬ê±´ì´ê³  ì •í•´ì ¸ ìˆê¸° ë•Œë¬¸ì´ë‹¤. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ ì‚¬ì „í™•ë¥  : ì‚¬ì „í™•ë¥  ë¶„í¬ê°€ ë² íƒ€ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤ P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. ì‚¬í›„í™•ë¥  P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP ê´€ì ì—ì„œ $\\hat\\theta$ ë§Œì•½ ë˜ì§„ íšŸìˆ˜ê°€ ë§ì•„ì§€ê²Œ ë˜ë©´ $\\alpha, \\beta$ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¯¸ë¹„í•´ì§€ë¯€ë¡œ ê²°êµ­ MLEë¡œ êµ¬í•œ ê²ƒê³¼ ê°™ì•„ì§€ê²Œ ëœë‹¤ \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]}]}
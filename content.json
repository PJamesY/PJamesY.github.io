{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"SVM","slug":"SVM","date":"2019-05-06T14:35:45.000Z","updated":"2019-05-06T14:35:45.025Z","comments":true,"path":"2019/05/06/SVM/","link":"","permalink":"http://progresivoJS.github.io/2019/05/06/SVM/","excerpt":"","text":"Support Vector Machine Andrew ng lecture note 를 공부하며 정리한 자료입니다 SVM을 이해하기 위해서는 Margin(마진)과 데이터를 분리해주는 경계선과 데이터 사이의 거리 Gap이 커야 한다는 것에 초점을 맞춰야 한다 1. Margin 여기에서는 ‘confidence’라는 개념이 등장한다 confidence는 예측이 얼마나 맞는지에 대한 확신을 나타낸다 그림을 보면 경계선(Seperating hyperplane) 근처에 있으면 Confidence가 낮고 즉 예측의 정확도가 낮아지고, 경계선에서 멀어질수록 Confidence가 높아지며 예측의 정확도가 높아진다. 2. Notation 여기에서 $x$ feature와 $y$ label를 사용한다 label $y$는 SVM에서 $y \\in \\{-1, 1\\}$ 로 지정해 준다 사용할 파라미터는 $w$, $b$로 표기할것이다. classifier는 다음과 같다 h_{w,b}(x)=g(w^Tx + b)만약 $w^Tx + b \\geq 0$ 이면 $g(w^Tx + b)=1$ 이 되고, $w^Tx + b \\leq 0$ 이면 $g(w^Tx + b)=-1$ 이 된다 3. Functional / geomeric margins 3.1 Functional margins training dataset $(x^{(i)},y^{(i)})$ 가 주어지면 functional margin $\\left(\\widehat\\gamma^{(i)} \\right)$은 다음과 같다 \\widehat\\gamma^{(i)}=y^{(i)} (w^Tx + b)식에서 보면 functional margin이 커지기 위해서는 $y^{(i)}=1$ 이면 $w^Tx + b$ 가 양의 부호로 커져야 하고, $y^{(i)}=-1$ 이면 $w^Tx + b$ 가 음의 부호로 커져야 한다. 또한 $y^{(i)} (w^Tx + b) &gt; 0$ 이면 예측이 맞았다는 뜻도 된다. 그러므로 functional margin은 confidence와 예측의 정확성을 나타낸다 단 여기서 functional margin $\\left(\\widehat\\gamma \\right)$은 데이터 마다의 functional margin중에서 가장 작은 값이 functional margin이 된다. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}3.2 geomeric margins 여기에서 $w$는 경계선(seperating hyperplane)과 직교한다 A는 데이터중 하나인 $x^{(i)}$이고 라벨 $y=1$을 나타낸다 선 AB는 경계선과의 거리 $\\gamma ^{(i)}$로 나타낸다 점 B는 다음과 같이 나타낼수 있다 x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}$\\frac{w}{\\lVert w \\rVert}$ 는 unit vector를 나타낸다. 경계선 $w^Tx + b=0$을 나타내므로, 경계선 위의 점 B를 이용하면 $w^T \\left( x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}\\right)+b=0$인 식을 유도할수 있다 $\\gamma^{(i)}$에 대한 식으로 바꿔주면 다음과 같다 \\gamma^{(i)}=\\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert}geometric margins$\\left(\\gamma^{(i)}\\right)$는 다음과 같다 \\gamma^{(i)}=y^{(i)}\\left( \\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert} \\right)만약 $\\lVert w \\rVert =1$ 이면 결국 functional margin과 같다진다는 것을 알수있다 geometric margin도 데이터 마다의 geometric margin중에서 가장 작은 값이 geometric margin이 된다. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}4. The optimal margin classifier 마진을 최대화 하는 경계선을 찾아내는 것이 최적화한다는 것이다. \\begin{matrix} max_{\\gamma ,w,b} && \\gamma \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq \\gamma , i=1,\\ldots,m \\\\ && \\lVert w \\rVert=1 \\end{matrix}margin을 최대화 하는 최적화 문제이고, 모든 데이터 마다 최소한의 마진보다는 항상 크고, $\\lVert w \\rVert=1$은 결국 functional / geometric margin이 같다는 것을 나타낸다 위의 식은 풀기가 까다로운 식이므로 식을 다음과 같이 변형할수 있다 \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}위의 식을 직관적으로 이해해 보면, 예를 들어 만약 임의의 점을 $x$, 경계선 위의 점을 $x_p$ 라고 한다면 $x_p$에서 $w$방향으로 $\\gamma$만큼 이동한것이 $x$이다. x = x_p + r \\frac{w}{\\lVert w \\rVert}w \\cdot x + b = w \\left ( x_p + r \\frac{w}{\\lVert w \\rVert} \\right) + b = w \\cdot x_p + b + r \\frac{w \\cdot w}{\\lVert w \\rVert} $w \\cdot x_p + b = 0$ 이므로 ($x_p :$ 경계선 위의점) w \\cdot x + b = r \\lVert w \\rVertr = \\frac{w \\cdot x + b}{\\lVert w \\rVert} = \\frac{c}{\\lVert w \\rVert}$c:$ constant 결국 $\\gamma$를 최대화 한다는것은 $w$를 최소화 하는것과 같다 5. Lagrange duality 최적화 문제를 풀기 위해서는 Lagrange duality에 대해서 이해를 하고 있어야 한다. 강의 노트에서 나온 정도로만 간단히 정리해보겠다 \\begin{matrix} min_w && f(w) \\end{matrix}\\begin{matrix} s.t. && g_i(w) \\leq 0 \\\\ && h_i(w)=0 \\end{matrix}부등식 제약조건이 있는 것은 Primal optimization 문제라고 부른다 일반화된 라그랑지안식(generalized Lagrangian)은 다음과 같다 L(w,\\alpha , \\beta)=f(w)+\\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)여기에서 $\\alpha ,\\beta$는 Lagrange multipliers 이다 5.1 primal optimal problem \\theta_P(w)=max_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)\\theta_P(w)= \\begin{cases} f(x) & \\text{if }g_i(w) \\leq 0 \\text{, } h_i(w)=0 \\\\ \\infty & \\text{if } g_i(w) > 0 \\text{ or }h_i(w)\\neq0 \\end{cases}제약 조건을 모두 만족하면 $\\theta_P(w)=f(x)$가 되고 제약 조건하나라도 만족을못하면 무한대로 발산한다 결국 위식에서 알수 있는 것은 제약식을 만족 시킨다는 것은 $\\theta_P(w)$를 가장 최소화 해야 된다는것을 알수 있다 p^*=min_w \\theta_P(w)=min_wmax_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)5.2 dual optimal problem dual problem은 primal problem과 반대로 $w$를 최소화하는 라그랑지안을 구하는 것이다 \\theta_D(\\alpha, \\beta) =min_w L(w,\\alpha , \\beta)d^*=max_{\\alpha ,\\beta : \\alpha >0} \\theta_D(\\alpha , \\beta)=max_{\\alpha ,\\beta : \\alpha >0} min_w L(w,\\alpha , \\beta)5.3 primal / dual optimal problem primal optimal 과 dual optimal 식은 min max를 자리바꾼것 말고는 다른게 없다. max min 함수가 min max보다 항상 작거나 같다. 하지만 특정한 조건하에서는 $d^ = p^$가 성립한다 $f$와 $g_i$가 convex, $h_i$가 affine하고 모든 $i$에 대해서 $g_i(w) &lt; 0$ 라고 가정하면 $w, \\alpha ,\\beta$는 반드시 존재하게 된다. $w^$는 Primal problem의 solution이 되고, $\\alpha^, \\beta ^*$는 Dual problem의 solution이 된다 5.4 KKT conditions {\\partial\\over\\partial w_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,n{\\partial\\over\\partial \\beta_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,l\\alpha_i^* g_i(w^*)=0, \\text{ } i=1,\\ldots,kg_i(w^*)\\leq 0, \\text{ } i=1,\\ldots,k\\alpha_i^* \\geq0, \\text{ } i=1,\\ldots,k$w^,\\alpha^ , \\beta^*$가 KKT 조건을 만족한다면 dual problem과 primal Problem이 같아지므로 두개 모두의 해가 된다 5. Optimal margin classifiers 다시 Margin classifiers문제로 돌아와 보자 \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx^{(i)} + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}제약식을 다음과 같이 고쳐 써줄수있다 g_i(w)=-y^{(i)} (w^Tx^{(i)} + b)+1 \\leq 0Functional margin은 1이되게 된다 다음 그림에서 margin이 가장 작은 점은 3개가 있다(두개의 negative 한개의 positive) 3개가 support vector가 된다. 또한 support vector에서는 $\\alpha$값이 절대 0이 되지 않는다. 그렇다는것은 KKT조건에 의해 $g_i(w)$가 0이 되어야 한다는것이다 Lagranian 식은 다음과 같다. L(w^*,\\alpha^* , \\beta^*)=\\frac{1}{2}\\lVert w \\rVert^2-\\sum_{i=1}^m \\alpha_i \\left[y^{(i)} (w^Tx^{(i)} + b)-1\\right]부등식 제약조건만 있으므로 $\\alpha_i$만 존재한다 dual problem을 푸기 위해서 $minimize_{w,b}L(w,\\alpha , \\beta)$ $\\alpha$는 고정한 상태에서 $w$, $b$에 대해서 미분을 해야한다","categories":[],"tags":[]},{"title":"EM Algorithm","slug":"em-algorithm","date":"2019-05-05T14:29:55.000Z","updated":"2019-05-05T14:29:55.089Z","comments":true,"path":"2019/05/05/em-algorithm/","link":"","permalink":"http://progresivoJS.github.io/2019/05/05/em-algorithm/","excerpt":"","text":"latent variable 추론 clustering과 classification의 가장 큰 차이점은 숨어있는 변수가 있느냐 없느냐이다.clustering은 latent variable이 포함되어 있다classification은 observed variable로 분류 한다 $\\{ X, Z\\}$ : 모든 variable $X$ : 관측된(Observed) variable $Z$ : hidden(latent) Variable $\\theta$ : 분포 parameter latent variable을 marginal out 해주면 된다 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta)로그 속에 summation이 있으면 계산이 복잡해져서 결국에는 summation의 위치를 바꿔줘야 한다(Jensens’s inequality). 그리고 $q(z)$의 임의의 pdf를 넣어준다 \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z P(X,Z \\mid \\theta)\\right\\}l(\\theta) = \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\}CF) Jensen’s inequality $\\psi$가 convex function일때는 f \\left( \\frac{x+y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\leq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j} $\\psi$가 concave function일때는 f \\left( \\frac{x+y}{2}\\right) \\geq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\geq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j}로그는 Concave function이므로 jensen’s inequality에 의해 다음과 같이 로그 속의 summation을 다음과 같이 빼서 식을 정리할수 있다 l(\\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\} \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)}오른쪽항의 식을 정리해주면 \\sum_z \\left\\{q(z) \\ln P(X,Z \\mid \\theta) - q(z)\\ln q(z) \\right\\}첫번째 항은 $q(z)$의 가중평균이고 두번째 항은 $q(z)$가 확률분포라는 가정하에 엔트로피를 나타낸다. 따라서 다음과 같이 식을 notation 해줄 수 있다 Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)$Q(\\theta, q)$은 결국 $l(\\theta)$의 lower bound이다 이것을 최대화 시켜주면 $l(\\theta)$값도 같이 최대화 시켜줄수 있다는 것이다 단 항상 그런것은 아니다 왜냐하면 서로 inequality하기 때문이다 Maximizing lower bound(1) l(\\theta) \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)} = \\sum_z q(z) \\ln \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{q(z)}맨 오른쪽의 항을 다음과 같이 정리할수 있다 \\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)} + q(z)\\ln P(X \\mid \\theta) \\right\\}=\\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)}\\right\\} + \\ln P(X \\mid \\theta)다음을 최적화 시키기 위한 식으로 바꿔주기 위해 summation 속의 로그를 역수로 취해줌으로써 Kullback-Leiber divergence로 변형 시켜준다 L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}이 식을 보면 많은 뜻을 볼수 있다 기존의 우리가 maximize시키고 싶었던 $\\ln P(X \\mid \\theta)$에 KL divergence term을 빼주는 식이 되었다 즉 KL divergence term을 0에 가깝게 해주면 해줄수록 우리의 objective function에 가까워 진다KL divergence term이 0에 가까워 진다는 것은 $q(z)$ 분포 모양과 $P(Z \\mid X, \\theta)$의 분포 모양이 비슷해 진다는 것을 의미한다 Maximizing lower bound(2) Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}우리가 $L(\\theta, q)$를 구한 이유는우리가 제일 처음 구한 $Q(\\theta, q)$만 가지고 optimal value를 구하는 것은 쉽지 않다. 왜냐하면 $q(z)$를 업데이트하는 방법에 대한 정확한 지식이 없기 때문이다하지만 $L(\\theta, q)$를 가지고 $q(z)$를 업데이트 하기는 쉽다 임의의 $\\theta$에 대해 $q(z)$는 $P(Z \\mid X, \\theta)$의 분포와 비슷하게 Update해주면 된다 KL\\left(q(Z) \\lVert P(Z \\mid X, \\theta)\\right) = 0 \\to q^t(z) = P(Z \\mid X, \\theta^t)특정한 iteration동안 $\\theta^t$로 되면 $q^t(z)$로 업그레이드 된다 Q(\\theta, q^t) = E_{q^t(z)} \\ln P(X,Z \\mid \\theta^t) + H(q^t)\\theta^{t+1} = argmax_{\\theta}Q(\\theta, q^t) = argmax_{\\theta}E_{q^t(z)} \\ln P(X,Z \\mid \\theta)그러면 업그레이드 된 $q^t$를 가지고 다시 $\\theta$를 업그레이드 해준다 정리 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta) 1. EM 알고리즘은 latent variable이 포함된 maximum likelihood를 찾는것이다 2. 처음에는 $\\theta$를 랜덤으로 정해준다 3. likelihood가 converge할때 까지 iteration을 돌려준다","categories":[],"tags":[]},{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend 를 공부하며 번역하여서 올립니다 각 영화에 대해서 평점이 5점까지 줄수 있다고 가정한다 Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : 총 유저의 명(수) $n_m$ : 총 영화의 갯수 $r(i,j)$ : $j$라는 유저가 영화 $i$에 평점을 줬으면 1 $r(i,j)$ : $r(i,j) = 1$이라는 조건 하에 user $j$가 movie $i$에게 준 평점 점수 우리는 주어진 데이터를 가지고 missing value를 predict해야 한다 1. Content_based algorithm content_based는 content가 어떤 특정한 잠재 feature가 있을거라고 생각하고 그 feature vector를 적용하는 것이다.예를 들어 영화를 추천하는 거라면 영화에는 각 장르가 존재한다. 영화의 로맨스 장르 정도, 액션 정도를 가중치 개념으로 주어서 각 영화의 latent feature vector를 지정해준다 여기에서는 romance를 $x_1$, action $x_2$의 feature vector를 만들어준다 extra(constant) feature도 넣어준다 Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 각각의 영화는 $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ 각각의 영화 $x^i$은 feature vector를 가지고 있다에를 들어 영화 Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ 의 feature vector를 가지고 있다 content_based 방식에서는 각각의 content feature vector에 따른 user parameter vector를 learning시켜야한다각각의 user마다 각각의 평점은 linear regression방식으로 나타난다 만약 Alice의 parameter vector($\\theta^1$)이 $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ 이라고 한다면 Alice가 Cute love 영화에 평점을 줄 점수는 $(\\theta^{(1)})^T x^{(3)}$ inner product를 해주면 4.95 평점을 줄거라는 예측이 나온다 $\\theta$를 learning 해보자 Notation n : feature의 dimension(constant를 제외한것) 여기서는 2(romance, action)이다 $m^j$ : $j$ user가 평점을 준 영화의 갯수 $j$ user가 평점을 준 영화에 한해서 $j$ user의 영화에 준 예측 평점과 실제 평점의 차를 최소화 해야한다 min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2최적화 하는데 $m^{(j)}$는 필요 없으므로 없애주어도 된다 min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2이거를 모든 user에게 적용시켜야 한다. 우리의 objective function이 된다 J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$에 대해서 미분을 해준다 $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ k가 0인것과 K가 0이 아닌것의 뜻은 feature의 constant term을 하느냐 안하느냐 이다 우리의 Obejctive function의 최종식은 {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_based는 각 content에 대한 feature를 알아야 한다는 단점이 있다. 다음에 알아볼 방법은 이거를 보완해준 Colloaborative filtering방법이다 2. Collaborative filtering content_based 방식에서는 content feature를 알고 있는 상태였다. 하지만 현실에서는 불가능하다 또한 Feature의 갯수를 조금더 많이 알기를 원한다 user의 영화 취향 벡터 $\\theta$와 각 영화의 장르 feature 벡터 $x$를 서로 교차적으로 learning Notation $n_m$ : 영화의 갯수 $n_u$ : user의 수 $\\theta$를 랜덤적으로 Initialize한다 $\\theta$를 가지고 $x$를 update시킨다 \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 update된 $x$를 가지고 $\\theta$를 Update시킨다 \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_based와 다른점은 costant Term을 넣어주지 않는다는 것이다 cost function J를 최대한 줄여주는 벡터를 찾는다 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5개의 영화와 4명의 user matrix이다 predicted rating은 다음과 같이 나타낼수 있다 X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점 $e_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점과 예측 평점의 차이 $p_{i,k}$ : $i$ 유저의 latent feature vector $q_{k,j}$ : $j$ 영화의 latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : 유저들의 Latent Matrix / shape : $\\left( \\text{유저 명수 (X) Latent 갯수}\\right)$ $Q$ : 영화들의 Latent Matrix / shape : $\\left( \\text{ Latent 갯수 (X) 영화 갯수 }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{‘}$, $q_{k,j}^{‘}$ 각각 벡터이다","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) 최대가능도 Notation $D$ : Data (관측한(Observed) 데이터) $\\theta$ : parameter (확률)&lt;/font&gt; $H$ : 앞면이 나온 횟수 $T$ : 뒷면이 나온 횟수 Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ 를 가장 높여주는 $\\theta$ 를 구하는것 MLE 계산 1. Maximum Liklihood 식 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ 을 씌운다 곱셈은 계산이 복잡하므로 $ln$를 씌워준다 → $log$ $function$ : 곱을 합으로 바꿔준다 로그는 단조 증가 함수이므로 $\\underset{\\theta}{\\operatorname{argmax}}$ 의 값은 변하지 않는다 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ 에 대해서 미분을 한다(derivative) 구하고자 하는 $\\theta$에 대해 미분한 값이 $0$이 되도록 식을 세운다 미분한 값이 $0$ 되게 하는 $\\theta$값을 구한다 \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE 관점에서 $\\hat\\theta$ 우리가 상식적으로 생각하고 있는 확률이 MLE(Maximum Liklihood Estimation)으로 구한 것이다 \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(사전 지식)을 고려한다 MLE(Maximum Liklihood Estimation)과 다르게 일어난 사건만을 고려하는것이 아니다 MLE는 $P(D\\mid\\theta)$를 최대화 하는 $\\theta$를 구하는 것이다 MAP는 $P(\\theta\\mid D)$ 즉 데이터가 주어졌을때 $\\theta$의 확률 사후확률(Posterior)을 최대화 하는 $\\theta$를 구하는 것이다 MAP 계산 1. 베이즈 정리(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}𝑷𝒐𝒔𝒕𝒆𝒓𝒊𝒐𝒓=\\frac{𝑳𝒊𝒌𝒆𝒍𝒊𝒉𝒐𝒐𝒅 \\cdot 𝑷𝒓𝒊𝒐𝒓 𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆}{𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕}2. 관계식 정리 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕$은 크게 중요하지 않는다. 주어진 데이터는 이미 일어난 사건이고 정해져 있기 때문이다. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ 사전확률 : 사전확률 분포가 베타 분포를 따른다고 가정한다 P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. 사후확률 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP 관점에서 $\\hat\\theta$ 만약 던진 횟수가 많아지게 되면 $\\alpha, \\beta$가 미치는 영향은 미비해지므로 결국 MLE로 구한 것과 같아지게 된다 \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]}]}
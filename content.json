{"meta":{"title":"James Blog","subtitle":null,"description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) 최대가능도 Notation $D$ : Data (관측한(Observed) 데이터) $\\theta$ : parameter (확률)&lt;/font&gt; $H$ : 앞면이 나온 횟수 $T$ : 뒷면이 나온 횟수 Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ 를 가장 높여주는 $\\theta$ 를 구하는것 MLE 계산 1. Maximum Liklihood 식 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ 을 씌운다 곱셈은 계산이 복잡하므로 $ln$를 씌워준다 → $log$ $function$ : 곱을 합으로 바꿔준다 로그는 단조 증가 함수이므로 $\\underset{\\theta}{\\operatorname{argmax}}$ 의 값은 변하지 않는다 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ 에 대해서 미분을 한다(derivative) 구하고자 하는 $\\theta$에 대해 미분한 값이 $0$이 되도록 식을 세운다 미분한 값이 $0$ 되게 하는 $\\theta$값을 구한다 \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE 관점에서 $\\hat\\theta$ 우리가 상식적으로 생각하고 있는 확률이 MLE(Maximum Liklihood Estimation)으로 구한 것이다 \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(사전 지식)을 고려한다 MLE(Maximum Liklihood Estimation)과 다르게 일어난 사건만을 고려하는것이 아니다 MLE는 $P(D\\mid\\theta)$를 최대화 하는 $\\theta$를 구하는 것이다 MAP는 $P(\\theta\\mid D)$ 즉 데이터가 주어졌을때 $\\theta$의 확률 사후확률(Posterior)을 최대화 하는 $\\theta$를 구하는 것이다 MAP 계산 1. 베이즈 정리(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}𝑷𝒐𝒔𝒕𝒆𝒓𝒊𝒐𝒓=\\frac{𝑳𝒊𝒌𝒆𝒍𝒊𝒉𝒐𝒐𝒅 \\cdot 𝑷𝒓𝒊𝒐𝒓 𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆}{𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕}2. 관계식 정리 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕$은 크게 중요하지 않는다. 주어진 데이터는 이미 일어난 사건이고 정해져 있기 때문이다. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ 사전확률 : 사전확률 분포가 베타 분포를 따른다고 가정한다 P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. 사후확률 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP 관점에서 $\\hat\\theta$ 만약 던진 횟수가 많아지게 되면 $\\alpha, \\beta$가 미치는 영향은 미비해지므로 결국 MLE로 구한 것과 같아지게 된다 \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2019-02-26T06:47:49.508Z","updated":"2019-02-26T06:47:49.508Z","comments":true,"path":"2019/02/26/hello-world/","link":"","permalink":"http://progresivoJS.github.io/2019/02/26/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}]}
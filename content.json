{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"Regularization and model selection","slug":"Regularization-and-modelselection","date":"2019-05-08T15:48:24.000Z","updated":"2019-05-08T15:48:24.735Z","comments":true,"path":"2019/05/09/Regularization-and-modelselection/","link":"","permalink":"http://progresivoJS.github.io/2019/05/09/Regularization-and-modelselection/","excerpt":"","text":"ë‹¤ìŒê³¼ ê°™ì€ hypothesis $h_\\theta(x)=g(\\theta_0 + \\theta_1 x + \\ldots + \\theta_k x^k)$ ê°€ ìˆì„ë•Œ $k$ë¥¼ ëª‡ìœ¼ë¡œ í•´ì•¼ í• ì§€ ì–´ë–»ê²Œ ì •í• ìˆ˜ ìˆì„ê¹Œ? ì¦‰ ëª‡ì°¨ë¡œ ì‹ì„ ë§Œë“œëŠ”ê²Œ bias / variance trade off ì— ìˆì–´ì„œ ê°€ì¥ ì¢‹ì€ ëª¨ë¸ì¼ê¹Œë¥¼ ì–´ë–»ê²Œ ì•Œìˆ˜ ìˆì„ê¹Œ?ì¡°ê¸ˆ êµ¬ì²´ì ìœ¼ë¡œ ì´í•´ í•˜ê¸° ìœ„í•´ ìœ í•œê°œì˜ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•˜ê² ë‹¤. $M={M_1, \\ldots, M_d}$ ì—¬ê¸°ì„œ $M_i$ëŠ” $i$ì°¨ìˆ˜ì˜ ë‹¤í•­ íšŒê·€ ëª¨ë¸ì´ë‹¤. 1. Cross Validation training set $S$ê°€ ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” empirical risk minimization ì•Œê³ ë¦¬ì¦˜ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ê±°ì³ ìµœì ì˜ hypothesisë¥¼ ì°¾ëŠ”ë‹¤. ë°ì´í„°ì…‹ì— ê°ê°ì˜ $M_i$ ëª¨ë¸ì„ ì ìš©í•œí›„ hypothesis $h_i$ë¥¼ ì–»ëŠ”ë‹¤. ê°€ì¥ ë‚®ì€ training errorë¥¼ ë‚¸ hypotheses ë¥¼ ì„ íƒí•œë‹¤ ìœ„ì˜ ë°©ë²•ì€ íš¨ê³¼ì ì´ì§€ëŠ” ì•Šë‹¤.ë¶„ëª… ë†’ì€ ì°¨ìˆ˜ì˜ hypothesisê°€ ë” ë‚®ì€ training errorë¥¼ ë°œìƒ ì‹œí‚¬ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ ì´ ë°©ë²•ì€ í•­ìƒ ë†’ì€ variance, ë†’ì€ ì°¨ìˆ˜ì˜ hypothesisê°€ ì„ íƒ ë ê²ƒì´ë‹¤. ë” íš¨ìœ¨ì´ ë†’ì€ ì•Œê³ ë¦¬ì¦˜ì€ Cross validation ë°©ë²•ì´ë‹¤. ë°©ë²•ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. ëœë¤í•˜ê²Œ ë°ì´í„°ì…‹ $S$ë¥¼ 70%ëŠ” $S_{train} : $ training ë°ì´í„°ë¡œ, ë‚˜ë¨¸ì§€ 30% $S_{cv}$(hold out cross validation set)ìœ¼ë¡œ ë‚˜ëˆˆë‹¤. hypothesis $h_i$ë¥¼ ì–»ê¸° ìœ„í•´ $S_{train}$ ë°ì´í„°ë§Œ ì‚¬ìš©í•œë‹¤. ê·¸ë¦¬ê³  ê°€ì¥ ì‘ì€ ì—ëŸ¬($\\hat \\epsilon_{S_{cv}}$)ë¥¼ ë‚¸ hypothesisë¥¼ ì„ íƒí•˜ë©´ ëœë‹¤. (ì—¬ê¸°ì„œ $\\hat \\epsilon_{S_{cv}}$ ëŠ” $S_{cv}$ ë°ì´í„°ì— ëŒ€í•œ empirical errorë¥¼ ë‚˜íƒ€ë‚¸ ê²ƒì´ë‹¤.) $S_{train}$ ë°ì´í„°ë¡œ í•™ìŠµí•˜ë©° ì–»ì€ $h_i$ë¡œ í•™ìŠµí• ë•Œ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë˜ $S_{cv}$ë¡œ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë³´ë©´ $h_i$ì˜ true generalization errorë¥¼ ì¸¡ì •í• ìˆ˜ ìˆë‹¤. ê·¸ ë‹¤ìŒ ê°€ì¥ ì‘ì€ generalization errorë¥¼ ê°€ì§„ hypothesisë¥¼ ì„ íƒí•˜ë©´ ëœë‹¤. ë³´í†µì˜ ê²½ìš° ë°ì´í„°ì˜ $1/4, 1/3$ ì •ë„ë¥¼ cross validation setìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. í•˜ì§€ë§Œ ìœ„ ë°©ë²•ì€ 30% ì •ë„ì˜ dataë¥¼ training í• ë•Œ ì“°ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. í•™ìŠµ í• ë•Œ ì¦‰ í•­ìƒ $0.7m$ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ê³  ì „ì²´ ë°ì´í„° $m$ì„ ì‚¬ìš©ì„ ëª»í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ë¬¼ë¡  ì¶©ë¶„í•œ ì—„ì²­ ë§ì€ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ìƒê´€ì—†ì§€ë§Œâ€¦ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ë‹¤ìŒ K-fold Cross Validation ë°©ë²•ì„ ì“°ëŠ”ê²ƒì´ ì¡°ê¸ˆë” ë°ì´í„°ë¥¼ í™œìš©í• ìˆ˜ ìˆì„ê²ƒì´ë‹¤. ë Œë¤í•˜ê²Œ ë°ì´í„° $S$ë¥¼ $m/k$ê°œë¡œ ì´ë£¨ì–´ì§„ ë°ì´í„° $k$ê°œë¡œ ë‚˜ëˆ  ì¤€ë‹¤. ê·¸ë¦¬ê³  ê°ê°ì˜ ë°ì´í„° ì…‹ì„ $S_1, S_2, \\ldots, S_k$ë¡œ ì§€ì •í•œë‹¤. ê°ê°ì˜ ëª¨ë¸ $M_i$ ì— ëŒ€í•´ ë‹¤ìŒì„ ì‹¤í–‰í•œë‹¤. For $j = 1, 2, \\ldots, k$ S_j ë°ì´í„° ì…‹ë§Œ ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ë°ì´í„° $S_1 \\cup \\cdots \\cup S_{j-1} \\cup S_{j+1} \\cup \\cdots S_k$ ë¥¼ ê°€ì§€ê³  í•™ìŠµì„ ì‹œí‚¨ë‹¤ìŒ $h_{ij}$ hypothesisë¥¼ ì–»ëŠ”ë‹¤. ê·¸ë¦¬ê³  ë‚˜ì„œ $S_j$ë°ì´í„°ë¥¼ ê°€ì§€ê³  í…ŒìŠ¤íŠ¸ë¥¼ í•œí›„ $\\widehat \\epsilon (S_j)$ ë¥¼ ì°¾ëŠ”ë‹¤ $M_i$ì˜ generalization errorëŠ” $\\widehat \\epsilon (S_j)$ í‰ê· ìœ¼ë¡œ êµ¬í•œë‹¤. generalization errorê°€ ê°€ì¥ ë‚®ì€ $M_i$ì„ êµ¬í•œ ë‹¤ìŒ ë‹¤ì‹œ ì „ì²´ ë°ì´í„° ì…‹ $S$ë¥¼ ê°€ì§€ê³  ì¬í•™ìŠµì„ í•˜ì—¬ ì–»ì–´ì§„ hypothesisê°€ ìµœì¢… ë‹µì´ ëœë‹¤. ë°ì´í„°ë¥¼ ë‚˜ëˆŒë•Œ $1/k$ë¡œ ì „ cross validationë³´ë‹¤ í›¨ì”¬ ì ê²Œ testë¡œ ì‚¬ìš©ë˜ê¸´ í•˜ì˜€ì§€ë§Œ, ê°ê°ì˜ ëª¨ë¸ë§ˆë‹¤ kë²ˆì˜ ê³„ì‚°ì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì— ê³„ì‚°ì ìœ¼ë¡œëŠ” ë¹„íš¨ìœ¨ì ì´ë‹¤. ì´ëŸ¬í•œ êµì°¨ ê²€ì¦ ë°©ë²•ì€ single model / algorithmì„ ê²€ì¦í•˜ê³  ì‹¶ì„ë•Œ ì‚¬ìš©í•˜ë©´ ì¢‹ì€ ê²€ì¦ ë°©ë²•ì¤‘ì— í•˜ë‚˜ê°€ ë ê²ƒì´ë‹¤. 2. Feature Selection modelì„ ì„ íƒí• ë•Œ ì¤‘ìš”í•˜ê³  íŠ¹ë³„í•œ ë°©ë²•ì¤‘ì˜ í•˜ë‚˜ê°€ Feature selectionì´ë‹¤. ë§Œì•½ supervised learningì„ í•˜ë ¤ê³  í•˜ëŠ”ë° Featureì˜ ê°¯ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ($n &gt;&gt; m$) ëª¨ë¸ì˜ í•™ìŠµì— ê´€ë ¨ìˆëŠ” ëª‡ê°œì˜ feature ë§Œ ì„ íƒí•´ì•¼ í• ê²ƒì´ë‹¤. simpleí•œ ì„ í˜• ë¶„ë¥˜ë¬¸ì œë¥¼ í’€ë•Œ ë§ì€ ìˆ˜($n$)ì˜ featureê°€ ì‚¬ìš©ë˜ë©´, hypothesis classì˜ VC dimensionì´ ì—¬ì „íˆ $O(n)$ì´ê¸° ë•Œë¬¸ì— í•™ìŠµ ë°ì´í„°ê°€ ë§ì§€ ì•Šì€ í•œ overfitting ë¬¸ì œê°€ ë°œìƒí• ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. ë§Œì•½ n ê°œì˜ featuresê°€ ìˆë‹¤ë©´ featureë¥¼ ì‚¬ìš©í•  ëª¨ë“  ê²½ìš°ì˜ ìˆ˜ëŠ” $2^n$ì´ ë ê²ƒì´ë‹¤. feature selection ë°©ë²•ì¤‘ì— forward selectionëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. $\\mathcal{F}= \\emptyset$ Repeat { (a) For $i = 1, 2, \\ldots, n$ $if$ $i \\notin \\mathcal{F}$, $\\mathcal{F_i}=F \\cup \\{i\\}$ë¡œ ë†“ëŠ”ë‹¤. $\\mathcal{F_i}$ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ cross validation í•œë‹¤. generalization errorë¥¼ êµ¬í•œë‹¤. (b) step (a)ì—ì„œ ìì¥ ì í•©í•œ feature set $\\mathcal{F_i}$ë¥¼ ì°¾ëŠ”} ì „ì²´ ê³¼ì •ì„ ëŒê³  ë‚˜ì„œ ê°€ì¥ ì í•©í•œ feature setì„ ì„ íƒí•œë‹¤. ì „ì²´ loopê°€ ì¢…ë£Œë˜ëŠ” ì‹œì ì€ The $\\mathcal{F}$ê°€ ì „ì²´ feature set $\\{1, \\ldots, n\\}$ì´ ë˜ì—ˆê±°ë‚˜, ë¯¸ë¦¬ ì§€ì •í•´ì¤€ threshold hold $\\mid \\mathcal{F_i}\\mid$ ë¥¼ ë„˜ì—ˆì„ë•Œì´ë‹¤. forward selectionì€ wrapper model feature selection ì˜ í•œ ì˜ˆì‹œë¡œ ë¶ˆë¦¬ê¸°ë„í•œë‹¤. forward search ì™€ëŠ” ë‹¤ë¥¸ ë°©ë²•ì¸ backward searchê°€ ìˆë‹¤. ì´ ë°©ë²•ì€ ì²˜ìŒ feature setì˜ ì‹œì‘ì„ ì „ì²´ set $\\mathcal{F} = \\{1, \\ldots, n\\}$ìœ¼ë¡œ ì‹œì‘ í•œë‹¤. ê·¸ë¦¬ê³  ë°˜ë³µì ìœ¼ë¡œ í•˜ë‚˜ì”© featureë“¤ì„ ì§€ì›Œê°€ë©° $\\mathcal{F}= \\emptyset$ ê°€ ë ë•Œê¹Œì§€ ìµœì ì˜ feature setì„ ì°¾ëŠ”ë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ wrapper model feature selection ì€ ê³„ì‚° ë³µì¡ë„ê°€ ëª¨ë“  feature set $\\mathcal{F} = \\{1, \\ldots, n\\}$ ì„ í™•ì¸í• ë•Œê¹Œì§€ $O(n^2)$ê°€ ëœë‹¤. ê·¸ëŸ¬í•œ ì´ìœ ë¡œ ë¹„ë¡ heuristic í•˜ê¸´ í•˜ì§€ë§Œ, ê³„ì‚° ë³µì¡ë„ê°€ ì¡°ê¸ˆ ë” ë‚®ì€ ë°©ë²•ì¸ Filter feature selection ì„ ì‚¬ìš©í• ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì€ ê°„ë‹¨í•œ ì ìˆ˜ $S(i)$ë¥¼ í™œìš©í•œë‹¤. $S(i)$ëŠ” í•˜ë‚˜ì˜ feature $x_i$ê°€ ë¼ë²¨ $y$ì— ëŒ€í•´ ì–¼ë§ˆë‚˜ ì •ë³´ì„±ì´ ìˆëŠëƒì˜ ì ìˆ˜ì´ë‹¤. ì ìˆ˜ê°€ í° featureë¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤. $S(i)$ë¥¼ ì¸¡ì •í•˜ëŠ” í•œê°€ì§€ ë°©ë²•ì€ í•™ìŠµ ë°ì´í„°ì—ì„œ ì–»ì€ feature $x$ ì™€ ë¼ë²¨ $y$ê°„ì˜ ì„œë¡œì˜ correlationì„ ì ìˆ˜ë¡œ ì“°ëŠ” ê²ƒì´ë‹¤. ê²°êµ­ ë¼ë²¨ê³¼ ê°€ì¥ ê°•í•œ ê´€ê³„ê°€ ê°•í•œ featureê°€ ì ìˆ˜ê°€ ë†’ê²Œ ë ê²ƒì´ë‹¤. ì‹¤ì œë¡œëŠ” íŠ¹íˆ feature $x_i$ì˜ ê°’ë“¤ì´ discrete í• ë•ŒëŠ” $S(i)$ë¥¼ ìƒí˜¸ ì •ë³´ëŸ‰($MI(x_i,y)$)ìœ¼ë¡œ ìˆ˜ì¹˜í™” í•˜ì—¬ ì‚¬ìš©í•œë‹¤ MI(x_i,y) = \\sum_{x_i \\in \\{0,1\\}} \\sum_{y \\in \\{0,1\\}} p(x_i, y) \\log \\frac{p(x_i, y)}{p(x_i)p(y)}ìœ„ì˜ ê³µì‹ì€ $x_i$ì™€ $y$ê°€ binary valueë¡œ ì´ë£¨ì–´ì ¸ ìˆì„ë•Œì´ê³ ,í™•ë¥  $p(x_i, y)$, $p(x_i),p(y)$ training dataì˜ empirical distributionì— ì˜í•´ ì¸¡ì • ëœê²ƒì´ë‹¤ ìƒí˜¸ ì •ë³´ëŸ‰ì´ KL divergenceë¡œ í‘œí˜„ë ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì´í•´í•˜ê³  ìˆìœ¼ë©´ ìœ„ ì‹ì„ ì¡°ê¸ˆë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í• ìˆ˜ ìˆì„ê²ƒì´ë‹¤. MI(x_i,y) = KL(p(x_i, y) \\mid \\mid p(x_i)p(y))KL divergence ëŠ” í™•ë¥  ë¶„í¬ $p(x_i, y)$ì™€ $p(x_i)p(y)$ê°€ ì–¼ë§ˆë‚˜ ë‹¤ë¥¸ì§€ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë‹¤. ë§Œì•½ $x_i$ì™€ $y$ê°€ ì„œë¡œ independent í•˜ë‹¤ë©´ $p(x_i, y) = p(x_i)p(y)$ ê°€ ì„±ë¦½í• ê²ƒì´ê³ , ë‘ ë¶„í¬ê°„ì˜ KL divergenceëŠ” 0ì´ ë ê²ƒì´ë‹¤. ë˜í•œ $x_i$ê°€ ëª…ë°±íˆ $y$ì— ëŒ€í•œ ì •ë³´ëŸ‰ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì˜ë§ˆê¸°ë„ í•œë‹¤. ë°˜ëŒ€ë¡œ, ë§Œì•½ $x_i$ê°€ $y$ì— ëŒ€í•œ ì •ë³´ëŸ‰ì´ ë§ìœ¼ë©´ ê·¸ë“¤ì˜ ìƒí˜¸ ì •ë³´ëŸ‰ $MI(x_i,y)$ëŠ” ë§¤ìš° í´ê²ƒì´ë‹¤. ì ê·¸ëŸ¬ë©´ ìŠ¤ì½”ì–´ S(i)ë¥¼ êµ¬í•˜ê³ , ëª‡ê°œì˜ featureë¥¼ ì‚¬ìš©í• ê²ƒì¸ê°€ë¥¼ ì •í•´ì•¼ í•œë‹¤. ê°€ì¥ í‘œì¤€ì ì¸ ë°©ë²•ì€ ê°€ëŠ¥í•œ kì˜ ê°¯ìˆ˜ë¥¼ cross validationì„ í†µí•´ì„œ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ë³´í†µ ì´ë°©ë²•ì€ ë‹¨ì–´ì˜ ê°¯ìˆ˜ê°€ ë§ì€ ì¦‰ featureì˜ ê°¯ìˆ˜ê°€ ë§¤ìš° ë§ì€ naive Bayes text classificationì„ í• ë•Œ feature ê°¯ìˆ˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì‚¬ìš©í•œë‹¤. 3. Bayesian statistics and regularization ì´ë²ˆì—ëŠ” overfittingì„ í”¼í•˜ê¸° ìœ„í•´ ì“°ì´ëŠ” ë°©ë²•ì„ ì•Œì•„ë³´ê² ë‹¤. ìš°ë¦¬ëŠ” parameterì„ ì°¾ê¸° ìœ„í•´ ìµœëŒ€ ê°€ëŠ¥ë„ (maximum likelihood)ë¥¼ ì‚¬ìš©í•œë‹¤. \\theta_{ML}=argmax_{\\theta}\\prod_{i=1}^m p(y^{(i)} \\mid x{(i)};\\theta)ê¸°ì¡´ì˜ ìµœëŒ€ ê°€ëŠ¥ë„ ë°©ë²•ì€ $\\theta$ë¥¼ randomí•˜ê²Œ ì§€ì •í•˜ëŠ”ê²Œ ì•„ë‹Œ, ëª¨ë¥´ëŠ” ìƒìˆ˜ë¡œ ë†“ëŠ”ë‹¤. ê·¸ëŸ¬ë‚˜, maximum likelihood ë°©ë²• ë§ê³  Bayesian ë°©ë²•ì„ ì´ìš©í•œ ë°©ë²•ì„ ì“°ê²Œ ë˜ë©´ ëª¨ë¥´ëŠ” ê°’ $\\theta$ë¥¼ ë Œë¤í•˜ê²Œ ë†“ê³ , ì‚¬ì „í™•ë¥  ë¶„í¬ $P(\\theta)$ë¥¼ ì§€ì •í•œë‹¤.ë§Œì•½ í•™ìŠµ ë°ì´í„° ì…‹ $S=\\{(x^{(i)}, y^{(i)})\\}_{i=1}^m$ ì´ ì£¼ì–´ì¡Œì„ë•Œ $\\theta$ë¥¼ ì‚¬í›„ í™•ë¥  ë¶„í¬ $P(\\theta \\mid S)$ í†µí•´ ì˜ˆì¸¡í•œë‹¤. \\begin{matrix} P(\\theta \\mid S) &=& \\frac{P(S \\mid \\theta) P(\\theta)}{P(S)} \\\\ &=& \\frac{\\left(\\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)},\\theta)\\right)p(\\theta)} {\\int_\\theta \\left(\\prod_{i=1}^m p(y^{(i)}\\mid x^{(i)},\\theta)p(\\theta)\\right)d\\theta} \\end{matrix}ìœ„ ì‹ì˜ $p(y^{(i)}\\mid x^{(i)},\\theta)$ëŠ” learning ë¬¸ì œë¥¼ í’€ë•Œë§ˆë‹¤ ë‚˜ì˜¤ëŠ” ì‹ì´ë‹¤. ë§Œì•½ Bayesian logistic regression ë¬¸ì œë¥¼ í’€ë©´ $p(y^{(i)}\\mid x^{(i)},\\theta)=h_\\theta(x^{(i)})^{y^{(i)}}(1-h_\\theta(x^{(i)}))^{(1-y^{(i)})}$ ê°€ ëœë‹¤. $h_\\theta(x^{(i)})=1/(1+exp(-\\theta x^{(i)}))$ ìƒˆë¡œìš´ xë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ì„ í•˜ë ¤ê³  í•œë‹¤ë©´ $\\theta$ì˜ ì‚¬í›„ í™•ë¥ ë¶„í¬ë¥¼ í™œìš©í•˜ì—¬ í´ë˜ìŠ¤ì— ëŒ€í•œ ì‚¬í›„ í™•ë¥  ë¶„í¬ë¥¼ ì´ìš©í•´ì„œ êµ¬í•œë‹¤ $p(y \\mid x, S)=\\int_\\theta p(y \\mid x, \\theta)p(\\theta \\mid S)d\\theta$ $E[y \\mid x, S] = \\int_y yp(y\\mid x,S)dy$ x ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ë•Œ $\\theta$ì—ëŒ€í•œ ì‚¬í›„ í™•ë¥  ë¶„í¬$p(\\theta \\mid S)$ì— ê´€í•œ yì˜ í‰ê· ìœ¼ë¡œ êµ¬í• ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ Bayesian ë°©ë²•ì€ $\\theta$ì˜ ì°¨ìˆ˜ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ ê³„ì‚° ë³µì¡ë„ê°€ ì»¤ì§€ê³ , closed formë„ ì•„ë‹ˆë¼ì„œ ê³„ì‚°ì´ ì–´ë µë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë”°ë¼ì„œ $\\theta$ì˜ ì‚¬í›„ í™•ë¥  ë¶„í¬ë¥¼ ê·¼ì‚¬í•˜ì—¬ êµ¬í•˜ê²Œ ë˜ëŠ”ë° ì´ ë°©ë²•ì„ MAP(maximum a posterior)ë¼ê³  í•œë‹¤ $\\theta_{MAP}=argmax_\\theta \\prod_{i=1}^mp(y^{(i)}\\mid x^{(i)}, \\theta)p(\\theta)$ MAP ì‹ì„ ë³´ë©´ ì•Œê² ì§€ë§Œ maximum likelihoodì™€ ë‹¤ë¥¸ ì ì€ ë‹¨ì§€ ë’¤ì— $p(\\theta)$ë§Œ ë¶™ì–´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë’¤ì— ì‚¬ì „ í™•ë¥  ë¶„í¬ê°€ ê³±í•´ì ¸ ìˆìœ¼ë¯€ë¡œ MLë³´ë‹¤ overfittingì— ëŒ€í•´ ëœ ë¯¼ê°í•œ ì¥ì ì´ ìˆë‹¤","categories":[],"tags":[]},{"title":"Learning_Theory","slug":"Learning-Theory","date":"2019-05-07T01:46:02.000Z","updated":"2019-05-07T15:37:56.088Z","comments":true,"path":"2019/05/07/Learning-Theory/","link":"","permalink":"http://progresivoJS.github.io/2019/05/07/Learning-Theory/","excerpt":"","text":"Learning Theory Andrew ng lecture note ë¥¼ ê³µë¶€í•˜ë©° ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤ 1. Bias / Variance tradeoff ì„ í˜• íšŒê·€ì—ì„œ ë°ì´í„°ì— í•´ë‹¹ í•˜ëŠ” ëª¨ë¸ì„ simple($y=\\theta_0 + \\theta_1 x$)í•˜ê²Œ í˜¹ì€ complex($y=\\theta_0 + \\theta_1 x + \\cdots + \\theta_4 x^4$)í•˜ê²Œ ë§Œë“¤ìˆ˜ ìˆë‹¤. 4ì°¨ í•¨ìˆ˜ê°€ ì•„ë¬´ë¦¬ $y$(price) ì˜ˆì¸¡ì„ ì˜í•œë‹¤ê³  í• ì§€ë¼ë„, ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ë•Œì—ëŠ” ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê¸°ëŠ” ì–´ë µë‹¤. ì¦‰ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµí•œ complex ëª¨ë¸ì€ ë‹¤ë¥¸ ë°ì´í„°(ì§‘)ì— ëŒ€í•´ì„œëŠ” ì¼ë°˜í™” ë˜ì§€ ì•Šì€ê²ƒì´ë¼ê³  í• ìˆ˜ ìˆë‹¤. generalization errorë€ ë°˜ë“œì‹œ íŠ¸ë ˆì´ë‹ ë°ì´í„°ë¡œ ë‚˜ì˜¨ errorë¥¼ ì¸¡ì •í•˜ëŠ”ê²Œ ì•„ë‹Œ ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ë“¤ì–´ì™”ì„ë•Œì˜ errorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê°€ì¥ ì™¼ìª½ì— ìˆëŠ” ëª¨ë¸ê³¼ ì˜¤ë¥¸ìª½ì— ìˆëŠ” ëª¨ë¸ ëª¨ë‘ generalization errorê°€ ë†’ë‹¤. í•˜ì§€ë§Œ ë‘ ëª¨ë¸ì˜ errorê°€ ë†’ì€ ì´ìœ ëŠ” ë‹¤ë¥´ë‹¤.ë¨¼ì € ì™¼ìª½ ëª¨ë¸ì„ ë³´ë©´ ë§Œì•½ $(x, y)$ ë°ì´í„°ê°€ ì„œë¡œ linear ê´€ê³„ê°€ ì•„ë‹ˆë¼ë©´ ì•„ë¬´ë¦¬ ë§ì€ ë°ì´í„°ë¡œ í•™ìŠµì„ ì‹œì¼œë„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ê¸°ë€ ì‰½ì§€ê°€ ì•Šë‹¤. ì´ëŸ° ê²½ìš° ëª¨ë¸ì˜ í¸í–¥(bias)ì´ í¬ë‹¤ê³  í•œë‹¤ ë˜ëŠ” underfit ë˜ì—ˆë‹¤ê³  í•œë‹¤. ì˜¤ë¥¸ìª½ ëª¨ë¸ì¸ ê²½ìš° ë³´í†µ í•™ìŠµ ë°ì´í„°ê°€ ì ì„ë•Œ $(x,y)$ ë°ì´í„° ìƒê´€ê´€ê³„ë¥¼ í­ë„“ê²Œ ë°˜ì˜í•˜ì§€ ëª»í•˜ëŠ” ê²½ìš°ì´ë‹¤. ì´ëŸ¬í•œ ê²½ìš° ëª¨ë¸ì˜ ë¶„ì‚°(variance)ì´ í¬ë‹¤ê³  í•˜ê³  overfit ë˜ì—ˆë‹¤ê³  í•œë‹¤. bias, variance ì„œë¡œ ìƒì¶©ê´€ê³„(trade off) ì´ë‹¤.simple í•œ ëª¨ë¸ì´ë¼ë©´ biasëŠ” í¬ê³  varianceëŠ” ì‘ë‹¤. ë°˜ë©´ì— complex ëª¨ë¸ì´ë¼ë©´ biasëŠ” ì‘ì§€ë§Œ varianceëŠ” ë§¤ìš± í¬ê²Œ ëœë‹¤. 2. Preliminaries ì—¬ê¸°ì—ì„œëŠ” ë‹¤ìŒ 3ê°€ì§€ì˜ ì§ˆë¬¸ì„ ì •ì˜í• ê²ƒì´ë‹¤.ì²«ì§¸, bias variance tradeoffë¥¼ formalí•˜ê²Œ í• ìˆ˜ ìˆì„ê¹Œ?ë‘˜ì§¸, generalization errorë¥¼ training errorë¡œ ê´€ê³„ì§€ì„ìˆ˜ ìˆì„ê¹Œ?ì…‹ì§¸, í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì´ ì˜ í•™ìŠµë˜ê³  ìˆëŠ”ì§€ ì¦ëª…í• ìˆ˜ ìˆëŠ” ì¡°ê±´ë“¤ì´ ìˆëŠ”ê°€? ìœ„ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ë‘ê°œì˜ ë³´ì¡° ì •ë¦¬ë¥¼ ì• ê¸°í•´ë³´ì Union bound $k$ê°œì˜ ëª¨ë“  ì‚¬ê±´ì´ ì„œë¡œ ë…ë¦½ì ì´ì§€ ì•ŠëŠ” ì‚¬ê±´ $A_1, A_2, \\ldots, A_k$ì´ ìˆë‹¤ê³  í•œë‹¤ë©´ ë‹¤ìŒ ì‹ì´ ì„±ë¦½ëœë‹¤. P(A_1 \\cup A_2 \\cup \\cdots \\cup A_k) \\leq P(A_1) + P(A_2) + \\ldots + P(A_k)ì¦ëª…ì€ í•˜ì§€ ì•Šê² ì§€ë§Œ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ ë³´ë©´ ì‚¬ê±´ ê°„ì˜ êµì§‘í•©ì´ ì¡´ì¬í• ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ê°ê°ì˜ ì‚¬ê±´ì˜ í™•ë¥ ì˜ í•©ì€ ëª¨ë“  ì‚¬ê±´ í•©ì˜ í™•ë¥ ë³´ë‹¤ëŠ” í¬ë‹¤ëŠ” ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤. Hoeffding inequality ì„œë¡œ ë™ì¼í•œ ë² ë¥´ëˆ„ì´ ë¶„í¬ì—ì„œ ë‚˜ì˜¨ ë…ë¦½ì ì¸ í™•ë¥  ë³€ìˆ˜(iid) $Z_1, Z_2, \\ldots, Z_m$ê°€ ìˆë‹¤ê³  í•˜ì. $P(Z_i=1)=\\phi$, $P(Z_i=0)=1-\\phi$ ë¼ê³  í‘œê¸°í•˜ê³ , í™•ë¥ ë³€ìˆ˜ì˜ í‰ê· ì€ $\\widehat \\phi = \\frac{1}{m}\\sum_{i=1}^m Z_i$ ë¼ê³  ì •ì˜ í•˜ê³ , $\\gamma &gt; 0$ ë³´ë‹¤ í¬ë‹¤ê³  ê³ ì •í•œë‹¤. learning theoryì—ì„œëŠ” $Chernoff bound$ë¼ê³  í•˜ëŠ” ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. P(\\mid \\phi - \\widehat \\phi \\mid > \\gamma) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)ì‹ì´ ë³µì¡í•´ ë³¼ìˆ˜ ìˆì§€ë§Œ, í¬ê²Œ ì–´ë µì§€ ì•Šë‹¤. ì‹ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ ë³´ë©´, ë§Œì•½ ë°ì´í„°ì˜ ê°¯ìˆ˜ê°€ ë§ìœ¼ë©´ ì¦‰ $m$ì´ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡ í™•ë¥ ë³€ìˆ˜ì˜ í‰ê· ($\\widehat \\phi$)ê³¼ ì¸¡ì •ê°’($\\phi$)ì˜ ì°¨ì´ê°€ í´ í™•ë¥ ì´ ì¤„ì–´ë“ ë‹¤ëŠ” ê²ƒì´ë‹¤. ì´í•´ë¥¼ ì¢€ë” ì‰½ê²Œ í•˜ê¸° ìœ„í•´ì„œ label $y \\in \\{0,1\\}$ binary classificationì„ ìƒê°í•´ ë³´ì. í•™ìŠµë°ì´í„° $S$ëŠ” $D$ ë¼ëŠ” íŠ¹ì • í™•ë¥ ë¶„í¬ì—ì„œ $m$ê°œ ìƒ˜í”Œë˜ì—ˆë‹¤ê³  í•´ë³´ì. $S = \\{ (x^{(i)}, y^{(i)});i = 1,\\ldots,m \\}$ë‹¤ìŒê³¼ ê°™ì´ training errorëŠ” hypothesis $h$ì— ëŒ€í•´ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì“¸ìˆ˜ ìˆë‹¤. \\widehat \\epsilon(h) = \\frac{1}{m} \\sum_{i=1}^m 1\\{h(x^{(i)}) \\neq y^{(i)}\\}training errorê°€ ì•„ë‹Œ ìš°ë¦¬ê°€ ì•Œê³  ì‹¶ì€ generalization errorëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤. \\epsilon(h) = P_{(x,y) \\sim D}\\left( h(x) \\neq y\\right)ë¶„í¬ $D$ì—ì„œ ëœë¤í•˜ê²Œ ë‚˜ì˜¨ sample $(x^{(i)}, y^{(i)})$ ì„ $h$ê°€ ì˜ëª» ë¶„ë¥˜í•œê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¤ì‹œí•œë²ˆ ì–¸ê¸‰í•˜ìë©´ training dataëŠ” ë¶„í¬ $D$ì—ì„œ ìƒ˜í”Œë§ ëœë‹¤ëŠ” ê²ƒì„ ê°€ì •í•˜ëŠ” ê²ƒì´ë‹¤. ê·¸ëŸ¼ hypothesis $h_\\theta (x) =1\\{\\theta^T x \\geq 0\\}$ì˜ íŒŒë¼ë¯¸í„° $\\theta$ë¥¼ ì°¾ìœ¼ë ¤ëŠ” ì‹ì„ ì•Œì•„ë³´ì. \\widehat \\theta = arg min _{\\theta} \\widehat\\epsilon (h_\\theta)ì‹ì„ ì´í•´í•´ë³´ë©´, ì¦‰ í•™ìŠµë°ì´í„°ì— ëŒ€í•œ ì—ëŸ¬ë¥¼ ê°€ì¥ ì¤„ì´ëŠ” $\\widehat \\theta$ë¥¼ ì°¾ëŠ” ê²ƒì´ë‹¤ ì¡°ê¸ˆ ë” ê¹Šê²Œ ë“¤ì–´ê°€ ìš°ë¦¬ëŠ” hypothesis class $\\mathcal{H}$ë¥¼ ì •ì˜í• ê²ƒì´ë‹¤. linear classificationì—ì„œì˜ $\\mathcal{H}$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì“¸ìˆ˜ ìˆë‹¤. \\mathcal{H}=\\{h_\\theta : h_\\theta(x)=1 \\{\\theta^T x \\geq 0\\}, \\theta \\in \\mathbb{R}^{n+1}\\}$\\mathcal{X}$ input domain ë“¤ë§ˆë‹¤ì˜ classifier ì§‘í•©ì´ë‹¤. \\widehat h = argmin_{h\\in \\mathcal{H}} \\widehat \\epsilon(h)hypothesis $h$ëŠ” ëª¨ë“  classifier hypothesis ì§‘í•© $\\mathcal{H}$ ì†ì—ì„œ ê°€ì¥ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•œ ì˜¤ë¥˜ê°€ ì‘ì€ ê²ƒì´ $\\widehat h$ê°€ ë ê²ƒì´ë‹¤. 3. The case of finite $\\mathcal{H}$ ìœ í•œê°œì˜ hypothesis ë“¤ì˜ ì§‘í•© $\\mathcal{H}=\\{h_1, h_2, \\ldots, h_k\\}$ ë¡œ ì •ì˜í• ìˆ˜ ìˆê³ , $\\mathcal{X}$ input dataì— ëŒ€í•´ $\\{0,1\\}$ë¡œ mapping í•´ì£¼ëŠ” í•¨ìˆ˜ê°€ $k$ê°œì˜ ìˆëŠ” ê²ƒê³¼ ê°™ë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ë‘ê°€ì§€ë¥¼ í™•ì¸í•´ì•¼ í•œë‹¤.ì²«ë²ˆì§¸, ëª¨ë“  hì— ëŒ€í•´ì„œ $\\widehat \\epsilon (h)$ëŠ” $\\epsilon(h)$ê°€ ë ìˆ˜ ìˆë‹¤.ë‘ë²ˆì§¸, ì—¬ê¸°ì„œ ë‚˜ì˜¨ errorëŠ” $\\widehat h$ì˜ generalization errorì˜ ìƒí•œì„ (upper bound)ì´ ëœë‹¤. $h_i \\in \\mathcal{H}$ë¥¼ í•˜ë‚˜ ì„ íƒí•œë‹¤. ê·¸ë¦¬ê³  ë² ë¥´ëˆ„ì´ í™•ë¥ ë³€ìˆ˜($Z$)ë¥¼ í•˜ë‚˜ ì •ì˜í•œë‹¤. $Z$ ëŠ” ë¶„í¬ $\\mathcal{D}$ì—ì„œ ìƒ˜í”Œëœ ë°ì´í„°$(x,y)\\sim \\mathcal{D}$ë¥¼ $h_i$ê°€ ì˜ëª» ë¶„ë¥˜í•œ ê²½ìš°ë¥¼ ë§í•œë‹¤. $Z=1\\{h_i(x) \\neq y\\}$ í•™ìŠµ ë°ì´í„°ëŠ” ë™ì¼í•œ ë¶„í¬ $\\mathcal{D}$ì—ì„œ ìƒ˜í”Œ ë˜ì—ˆê¸° ë•Œë¬¸ì— $Z$ì™€ $Z_i$ë„ ê°™ì€ ë¶„í¬ì—ì„œ ë‚˜ì˜¨ í™•ë¥  ë³€ìˆ˜ ì„ì„ ì•Œìˆ˜ ìˆë‹¤. hypothesisì˜ í•™ìŠµë°ì´í„° ì˜¤ë¥˜ëŠ” í™•ë¥ ë³€ìˆ˜ $Z$ì˜ í‰ê· ì„ì„ ì•Œìˆ˜ ìˆë‹¤. \\widehat \\epsilon (h_i)=\\frac{1}{m}\\sum_{i=1}^m Z_j$\\widehat \\epsilon (h_i)$ëŠ” $m$ê°œì˜ í™•ë¥ ë³€ìˆ˜ $Z_j \\sim Bern\\left(\\phi = \\epsilon (h_i)\\right)$ì˜ í‰ê· ì´ë‹¤. ì—¬ê¸°ì—ì„œHoeffding inequality ê³µì‹ì„ ì ìš©í•´ë³´ì. P(\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)ì‹ì„ ì´í•´í•´ ë³´ë©´ hypothesis $h_i$ì˜ í•™ìŠµë°ì´í„°ì— ëŒ€í•œ ì˜¤ë¥˜ê°€ generalization error ì™€ ë¹„ìŠ·í•´ì§€ëŠ” í™•ë¥ ì€ ë°ì´í„°ì˜ ê°¯ìˆ˜($m$)ì´ ë§ì•„ì§€ë©´ ë§ì•„ì§ˆìˆ˜ë¡ ë†’ì•„ì§€ê²Œ ëœë‹¤. ìš°ë¦¬ëŠ” ë” ë‚˜ì•„ê°€ $h_i$í•˜ë‚˜ì˜ hypothesis ë¿ë§Œ ì•„ë‹ˆë¼ ëª¨ë“  hypothesis($h \\in \\mathcal{H}$)ì—ë„ ì ìš© ë˜ëŠ”ì§€ ì•Œì•„ë³´ì. ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid &gt; \\gamma$ë¥¼ í•˜ë‚˜ì˜ ì‚¬ê±´ $A_i$ë¡œ í‘œê¸°í•˜ë„ë¡ í•˜ì. P(A_i) \\leq 2 exp \\left( -2 \\gamma^2 m\\right)ìœ„ì—ì„œ ë³´ì•˜ë˜ union bound ì •ë¦¬ë¥¼ ì´ìš©í•´ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í• ìˆ˜ ìˆë‹¤. \\begin{matrix} P\\left( \\exists h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma \\right) &=& P(A_1 \\cup A_2 \\cup \\cdots \\cup A_k) \\\\ &\\leq& \\sum_{i=1}^kP(A_i) \\\\ &\\leq& \\sum_{i=1}^k2 \\exp \\left( -2 \\gamma^2 m\\right) \\\\ &=& 2k \\exp \\left( -2 \\gamma^2 m\\right) \\\\ \\end{matrix}ì–‘ë³€ì„ 1ì—ì„œ ë¹¼ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì¨ì¤„ìˆ˜ ìˆë‹¤. \\begin{matrix} P\\left( \\lnot \\exists h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid > \\gamma \\right) &=& P\\left( \\forall h \\in \\mathcal{H} .\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\gamma \\right) \\\\ &\\geq& 1-2k \\exp \\left( -2 \\gamma^2 m\\right) \\\\ \\end{matrix}ìœ„ ì‹ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ë©´, $1-2k \\exp \\left( -2 \\gamma^2 m\\right)$ ì´ìƒì˜ í™•ë¥ ë¡œ $\\widehat \\epsilon (h_i)$ëŠ” $\\epsilon (h_i)$ì˜ $\\gamma$ ë²”ìœ„ ì•ˆì— ìˆë‹¤. ì´ê²ƒì„ uniform convergence ë¼ê³  ë¶€ë¥¸ë‹¤ Notation $\\lnot$ : not ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ ê´€ì‹¬ìˆëŠ” ê°’ì€ $m$, $\\gamma$, ì—ëŸ¬ì˜ í™•ë¥ ì´ë‹¤. $\\delta$ = $2k \\exp \\left( -2 \\gamma^2 m\\right) &gt; 0$ì´ë¼ê³  fixí•˜ê³ , $\\gamma$ë„ íŠ¹ì •í•œ ê°’ìœ¼ë¡œ fix í•˜ë©´ ë°ì´í„°ì˜ ê°¯ìˆ˜ $m$ëŠ” ì–¼ë§ˆë‚˜ í•„ìš”í•œì§€ í™•ì¸íìˆ˜ ìˆë‹¤. m \\geq \\frac{1}{2\\gamma ^2} \\log \\frac{2k}{\\delta}ë§Œì•½ ë°ì´í„°ì˜ ê°¯ìˆ˜($m$)ê°€ ìœ„ì™€ ê°™ì´ ì¶©ë¶„íˆ ìˆìœ¼ë©´, ëª¨ë“  $h \\in \\mathcal{H}$ ì— ëŒ€í•´ì„œ $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\gamma$ ì¼ í™•ë¥ ì€ ìµœì†Œ $1-\\delta$ê°€ ëœë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ, ëª¨ë“  $h \\in \\mathcal{H}$ ì— ëŒ€í•´ì„œ $\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid &gt; \\gamma$ ì¼ í™•ë¥ ì€ ìµœëŒ€ $\\delta$ê°€ ëœë‹¤. ì´ëŸ¬í•œ í™•ë¥  boundëŠ” ìš°ë¦¬ê°€ ì–¼ë§ˆë§Œí¼ì˜ ë°ì´í„°ê°€ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ì•Œìˆ˜ ìˆê²Œ ëœë‹¤. ë˜í•œ $m$ì€ $\\log k$ì— ì˜í–¥ì„ ë§ì´ ë°›ëŠ”ë‹¤. ì—¬ê¸°ì„œ $k$ëŠ” hypothesisì˜ ê°¯ìˆ˜ì´ë‹¤. ì´ë²ˆì—ëŠ” $m$ê³¼ $\\theta$ë¥¼ ê³ ì •í•˜ê³  $\\gamma$ë¥¼ ìœ ë„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}ëª¨ë“  $h \\in \\mathcal{H}$ ì— ëŒ€í•´ì„œ P\\left(\\mid \\epsilon (h_i) - \\widehat \\epsilon (h_i) \\mid \\leq \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}\\right) \\geq 1-\\deltageneralization errorë¥¼ ìµœì†Œí™”í•˜ëŠ” hypothesis $h$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í• ìˆ˜ ìˆë‹¤ $h^* = argmin_{h \\in \\mathcal{H}} \\epsilon (h)$ ì¦‰ ìš°ë¦¬ê°€ ì°¾ëŠ” best modelì´ ë˜ëŠ” ê²ƒì´ë‹¤. \\epsilon(\\widehat h) \\leq \\widehat \\epsilon (\\widehat h) + \\gamma \\text { : 1ë²ˆì‹}\\widehat \\epsilon (\\widehat h) + \\gamma \\leq \\widehat \\epsilon ( h^*) + \\gamma \\text { : 2ë²ˆì‹}\\widehat \\epsilon ( h^*) + \\gamma \\leq \\epsilon ( h^*) + 2\\gamma \\text { : 3ë²ˆì‹}ìœ„ ì‹ì„ ì¡°ê¸ˆ ë” ì´í•´í•˜ê¸° ì‰½ê²Œ notationì„ ë‹¤ì‹œí•œë²ˆ ì •ë¦¬í•´ë³´ì. Notation $\\widehat h$ : training errorë¥¼ ê°€ì¥ ìµœì†Œí™” í•˜ëŠ” hypothesis, $\\widehat h = argmin_{h \\in \\mathcal{H}} \\widehat \\epsilon (h)$ $\\widehat \\epsilon(\\widehat h)$ : hypothesis $\\widehat h$ì˜ training error $\\epsilon(\\widehat h)$ : hypothesis $\\widehat h$ì˜ generalization error ìœ„ ì‹ì— ëŒ€í•´ì„œ ìˆœì„œëŒ€ë¡œ ì„¤ëª…ì„ í•´ë³´ìë©´, 1ë²ˆì‹ ê°™ì€ ê²½ìš°ëŠ” $\\widehat h$ì˜ generalization errorëŠ” training error ë³´ë‹¤ $\\gamma$ ë§Œí¼ í¬ë‹¤ëŠ” ê²ƒì´ë‹¤. 2ë²ˆì‹ì€ $\\widehat h$ì˜ training errorë³´ë‹¤ $h^{star}$ì˜ training error ê°€ ë” í¬ë‹¤ëŠ” ê²ƒì´ë‹¤. $\\widehat h, h^{star}$ì˜ ì •ì˜ë¥¼ ì˜ ìƒê°í•´ë³´ë©´ ì´í•´í• ìˆ˜ ìˆì„ê²ƒì´ë‹¤. 3ë²ˆì‹ì€ $h^{star}$ì˜ generalization error ë³´ë‹¤ training errorê°€ $\\gamma$ ë§Œí¼ ë” í¬ë‹¤ëŠ” ëœ»ì´ë‹¤. ì´ì œ ë‹¤ìŒê³¼ ê°™ì€ ì´ë¡  ê³µì‹ì„ ìœ ë„í• ìˆ˜ ìˆë‹¤.$\\mid \\mathcal{H} \\mid$ = $k$, $m, \\delta$ë¥¼ ê³ ì •í•œë‹¤. ìµœì†Œ $1 - \\delta$ì˜ í™•ë¥ ë¡œ \\epsilon (\\widehat h) \\leq \\left(min_{h \\in \\mathcal{H}} \\epsilon (h)\\right) + 2 \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}$\\gamma = \\sqrt{\\frac{1}{2m} \\log \\frac{2k}{\\delta}}$ $min_{h \\in \\mathcal{H}} \\epsilon (h)=\\epsilon ( h^{*})$ ìœ„ ì‹ì—ì„œ trade off ê´€ê³„ë¥¼ í™•ì¸ í• ìˆ˜ ìˆë‹¤. ë§Œì•½ hypothesisì˜ ì°¨ìˆ˜ë¥¼ ëŠ˜ë ¤ì£¼ê²Œ ë˜ë©´ generalization error ($\\epsilon ( h^{*})$) ëŠ” ì¤„ì–´ ë“¤ê²ƒì´ë‹¤. í•˜ì§€ë§Œ hypothesisì˜ ê°¯ìˆ˜(k)ëŠ” ëŠ˜ì–´ê°€ê²Œ ë˜ì–´ì„œ ê²°êµ­ ì´ ê²ƒì€ biasëŠ” ì¤„ì–´ë“¤ê²Œ ë˜ì§€ë§Œ varianceëŠ” ì»¤ì§€ê²Œ ëœë‹¤. ë°˜ëŒ€ë¡œ hypothesisì˜ ì°¨ìˆ˜ê°€ ì¤„ì–´ë“¤ë©´ generalization errorëŠ” ì»¤ì§€ê²Œ ë˜ì§€ë§Œ ê·¸ë§Œí° hypothesisì˜ ê°¯ìˆ˜ëŠ” ì¤„ì–´ë“¤ê²Œ ë˜ì–´ì„œ bias /variance ì˜ tradeoff ê´€ê³„ê°€ ë‚˜íƒ€ë‚˜ê²Œ ëœë‹¤.","categories":[],"tags":[]},{"title":"SVM","slug":"SVM-1","date":"2019-05-06T15:46:00.000Z","updated":"2019-05-06T15:46:00.796Z","comments":true,"path":"2019/05/07/SVM-1/","link":"","permalink":"http://progresivoJS.github.io/2019/05/07/SVM-1/","excerpt":"","text":"Support Vector Machine Andrew ng lecture note ë¥¼ ê³µë¶€í•˜ë©° ì •ë¦¬í•œ ìë£Œì…ë‹ˆë‹¤. SVMì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” Margin(ë§ˆì§„)ê³¼ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ì£¼ëŠ” ê²½ê³„ì„ ê³¼ ë°ì´í„° ì‚¬ì´ì˜ ê±°ë¦¬ Gapì´ ì»¤ì•¼ í•œë‹¤ëŠ” ê²ƒì— ì´ˆì ì„ ë§ì¶°ì•¼ í•œë‹¤. 1. Margin ì—¬ê¸°ì—ì„œëŠ” â€˜confidenceâ€™ë¼ëŠ” ê°œë…ì´ ë“±ì¥í•œë‹¤. confidenceëŠ” ì˜ˆì¸¡ì´ ì–¼ë§ˆë‚˜ ë§ëŠ”ì§€ì— ëŒ€í•œ í™•ì‹ ì„ ë‚˜íƒ€ë‚¸ë‹¤. ê·¸ë¦¼ì„ ë³´ë©´ ê²½ê³„ì„ (Seperating hyperplane) ê·¼ì²˜ì— ìˆìœ¼ë©´ Confidenceê°€ ë‚®ê³  ì¦‰ ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë‚®ì•„ì§€ê³ , ê²½ê³„ì„ ì—ì„œ ë©€ì–´ì§ˆìˆ˜ë¡ Confidenceê°€ ë†’ì•„ì§€ë©° ì˜ˆì¸¡ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§„ë‹¤. 2. Notation ì—¬ê¸°ì—ì„œ $x$ featureì™€ $y$ labelë¥¼ ì‚¬ìš©í•œë‹¤. label $y$ëŠ” SVMì—ì„œ $y \\in \\{-1, 1\\}$ ë¡œ ì§€ì •í•´ ì¤€ë‹¤. ì‚¬ìš©í•  íŒŒë¼ë¯¸í„°ëŠ” $w$, $b$ë¡œ í‘œê¸°í• ê²ƒì´ë‹¤. classifierëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. h_{w,b}(x)=g(w^Tx + b)ë§Œì•½ $w^Tx + b \\geq 0$ ì´ë©´ $g(w^Tx + b)=1$ ì´ ë˜ê³ , $w^Tx + b \\leq 0$ ì´ë©´ $g(w^Tx + b)=-1$ ì´ ëœë‹¤. 3. Functional / geomeric margins 3.1 Functional margins training dataset $(x^{(i)},y^{(i)})$ ê°€ ì£¼ì–´ì§€ë©´ functional margin $\\left(\\widehat\\gamma^{(i)} \\right)$ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\widehat\\gamma^{(i)}=y^{(i)} (w^Tx + b)ì‹ì—ì„œ ë³´ë©´ functional marginì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” $y^{(i)}=1$ ì´ë©´ $w^Tx + b$ ê°€ ì–‘ì˜ ë¶€í˜¸ë¡œ ì»¤ì ¸ì•¼ í•˜ê³ , $y^{(i)}=-1$ ì´ë©´ $w^Tx + b$ ê°€ ìŒì˜ ë¶€í˜¸ë¡œ ì»¤ì ¸ì•¼ í•œë‹¤. ë˜í•œ $y^{(i)} (w^Tx + b) &gt; 0$ ì´ë©´ ì˜ˆì¸¡ì´ ë§ì•˜ë‹¤ëŠ” ëœ»ë„ ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ functional marginì€ confidenceì™€ ì˜ˆì¸¡ì˜ ì •í™•ì„±ì„ ë‚˜íƒ€ë‚¸ë‹¤. ë‹¨ ì—¬ê¸°ì„œ functional margin $\\left(\\widehat\\gamma \\right)$ì€ ë°ì´í„° ë§ˆë‹¤ì˜ functional marginsì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ ê°’ì´ functional marginì´ ëœë‹¤. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}3.2 geomeric margins ì—¬ê¸°ì—ì„œ $w$ëŠ” ê²½ê³„ì„ (seperating hyperplane)ê³¼ ì§êµí•œë‹¤ AëŠ” ë°ì´í„°ì¤‘ í•˜ë‚˜ì¸ $x^{(i)}$ì´ê³  ë¼ë²¨ $y=1$ì„ ë‚˜íƒ€ë‚¸ë‹¤ ì„  ABëŠ” ê²½ê³„ì„ ê³¼ì˜ ê±°ë¦¬ $\\gamma ^{(i)}$ë¡œ ë‚˜íƒ€ë‚¸ë‹¤ ì  BëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤ x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}$\\frac{w}{\\lVert w \\rVert}$ ëŠ” unit vectorë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ê²½ê³„ì„  $w^Tx + b=0$ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ, ê²½ê³„ì„  ìœ„ì˜ ì  Bë¥¼ ì´ìš©í•˜ë©´ $w^T \\left( x^{(i)}-\\gamma^{(i)} \\cdot \\frac{w}{\\lVert w \\rVert}\\right)+b=0$ì¸ ì‹ì„ ìœ ë„í• ìˆ˜ ìˆë‹¤ $\\gamma^{(i)}$ì— ëŒ€í•œ ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ \\gamma^{(i)}=\\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert}geometric margins$\\left(\\gamma^{(i)}\\right)$ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ \\gamma^{(i)}=y^{(i)}\\left( \\left(\\frac{w}{\\lVert w \\rVert}\\right)^T x^{(i)}+\\frac{b}{\\lVert w \\rVert} \\right)ë§Œì•½ $\\lVert w \\rVert =1$ ì´ë©´ ê²°êµ­ functional marginê³¼ ê°™ë‹¤ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œìˆ˜ìˆë‹¤ geometric marginë„ ë°ì´í„° ë§ˆë‹¤ì˜ geometric marginì¤‘ì—ì„œ ê°€ì¥ ì‘ì€ ê°’ì´ geometric marginì´ ëœë‹¤. \\widehat\\gamma=min_{i=1,\\ldots,m} \\widehat\\gamma^{(i)}4. The optimal margin classifier ë§ˆì§„ì„ ìµœëŒ€í™” í•˜ëŠ” ê²½ê³„ì„ ì„ ì°¾ì•„ë‚´ëŠ” ê²ƒì´ ìµœì í™”í•œë‹¤ëŠ” ê²ƒì´ë‹¤. \\begin{matrix} max_{\\gamma ,w,b} && \\gamma \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq \\gamma , i=1,\\ldots,m \\\\ && \\lVert w \\rVert=1 \\end{matrix}marginì„ ìµœëŒ€í™” í•˜ëŠ” ìµœì í™” ë¬¸ì œì´ê³ , ëª¨ë“  ë°ì´í„° ë§ˆë‹¤ ìµœì†Œí•œì˜ ë§ˆì§„ë³´ë‹¤ëŠ” í•­ìƒ í¬ê³ , $\\lVert w \\rVert=1$ì€ ê²°êµ­ functional / geometric marginì´ ê°™ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤ ìœ„ì˜ ì‹ì€ í’€ê¸°ê°€ ê¹Œë‹¤ë¡œìš´ ì‹ì´ë¯€ë¡œ ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ë³€í˜•í• ìˆ˜ ìˆë‹¤ \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}ìœ„ì˜ ì‹ì„ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•´ ë³´ë©´, ì˜ˆë¥¼ ë“¤ì–´ ë§Œì•½ ì„ì˜ì˜ ì ì„ $x$, ê²½ê³„ì„  ìœ„ì˜ ì ì„ $x_p$ ë¼ê³  í•œë‹¤ë©´ $x_p$ì—ì„œ $w$ë°©í–¥ìœ¼ë¡œ $\\gamma$ë§Œí¼ ì´ë™í•œê²ƒì´ $x$ì´ë‹¤. x = x_p + r \\frac{w}{\\lVert w \\rVert}w \\cdot x + b = w \\left ( x_p + r \\frac{w}{\\lVert w \\rVert} \\right) + b = w \\cdot x_p + b + r \\frac{w \\cdot w}{\\lVert w \\rVert} $w \\cdot x_p + b = 0$ ì´ë¯€ë¡œ ($x_p :$ ê²½ê³„ì„  ìœ„ì˜ì ) w \\cdot x + b = r \\lVert w \\rVertr = \\frac{w \\cdot x + b}{\\lVert w \\rVert} = \\frac{c}{\\lVert w \\rVert}$c:$ constant ê²°êµ­ $\\gamma$ë¥¼ ìµœëŒ€í™” í•œë‹¤ëŠ”ê²ƒì€ $w$ë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤ 5. Lagrange duality ìµœì í™” ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ì„œëŠ” Lagrange dualityì— ëŒ€í•´ì„œ ì´í•´ë¥¼ í•˜ê³  ìˆì–´ì•¼ í•œë‹¤. ê°•ì˜ ë…¸íŠ¸ì—ì„œ ë‚˜ì˜¨ ì •ë„ë¡œë§Œ ê°„ë‹¨íˆ ì •ë¦¬í•´ë³´ê² ë‹¤ \\begin{matrix} min_w && f(w) \\end{matrix}\\begin{matrix} s.t. && g_i(w) \\leq 0 \\\\ && h_i(w)=0 \\end{matrix}ë¶€ë“±ì‹ ì œì•½ì¡°ê±´ì´ ìˆëŠ” ê²ƒì€ Primal optimization ë¬¸ì œë¼ê³  ë¶€ë¥¸ë‹¤ ì¼ë°˜í™”ëœ ë¼ê·¸ë‘ì§€ì•ˆì‹(generalized Lagrangian)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ L(w,\\alpha , \\beta)=f(w)+\\sum_{i=1}^k\\alpha_i g_i(w) + \\sum_{i=1}^l\\beta_i h_i(w)ì—¬ê¸°ì—ì„œ $\\alpha ,\\beta$ëŠ” Lagrange multipliers ì´ë‹¤ 5.1 primal optimal problem \\theta_P(w)=max_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)\\theta_P(w)= \\begin{cases} f(x) & \\text{if }g_i(w) \\leq 0 \\text{, } h_i(w)=0 \\\\ \\infty & \\text{if } g_i(w) > 0 \\text{ or }h_i(w)\\neq0 \\end{cases}ì œì•½ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•˜ë©´ $\\theta_P(w)=f(x)$ê°€ ë˜ê³  ì œì•½ ì¡°ê±´í•˜ë‚˜ë¼ë„ ë§Œì¡±ì„ëª»í•˜ë©´ ë¬´í•œëŒ€ë¡œ ë°œì‚°í•œë‹¤ ê²°êµ­ ìœ„ì‹ì—ì„œ ì•Œìˆ˜ ìˆëŠ” ê²ƒì€ ì œì•½ì‹ì„ ëª¨ë‘ ë§Œì¡± ì‹œí‚¤ë ¤ë©´ $\\theta_P(w)$ë¥¼ ê°€ì¥ ìµœì†Œí™” í•´ì•¼ ëœë‹¤ëŠ”ê²ƒì„ ì•Œìˆ˜ ìˆë‹¤. p^*=min_w \\theta_P(w)=min_wmax_{\\alpha ,\\beta : \\alpha >0} L(w,\\alpha , \\beta)5.2 dual optimal problem dual problemì€ primal problemê³¼ ë°˜ëŒ€ë¡œ $w$ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë¼ê·¸ë‘ì§€ì•ˆì„ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ \\theta_D(\\alpha, \\beta) =min_w L(w,\\alpha , \\beta)d^{*}=max_{\\alpha ,\\beta : \\alpha >0} \\theta_D(\\alpha , \\beta)=max_{\\alpha ,\\beta : \\alpha >0} min_w L(w,\\alpha , \\beta)5.3 primal / dual optimal problem primal optimal ê³¼ dual optimal ì‹ì€ min maxë¥¼ ìë¦¬ë°”ê¾¼ê²ƒ ë§ê³ ëŠ” ë‹¤ë¥¸ê²Œ ì—†ë‹¤. max min í•¨ìˆ˜ê°€ min maxë³´ë‹¤ í•­ìƒ ì‘ê±°ë‚˜ ê°™ë‹¤. í•˜ì§€ë§Œ íŠ¹ì •í•œ ì¡°ê±´í•˜ì—ì„œëŠ” $d$ = $p$ ê°€ ì„±ë¦½í•œë‹¤ $f$ì™€ $g_i$ê°€ convex, $h_i$ê°€ affineí•˜ê³  ëª¨ë“  $i$ì— ëŒ€í•´ì„œ $g_i(w) &lt; 0$ ë¼ê³  ê°€ì •í•˜ë©´ $w, \\alpha ,\\beta$ëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•˜ê²Œ ëœë‹¤. $w$ëŠ” Primal problemì˜ solutionì´ ë˜ê³ , $\\alpha, \\beta$ëŠ” Dual problemì˜ solutionì´ ëœë‹¤ 5.4 KKT conditions {\\partial\\over\\partial w_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,n{\\partial\\over\\partial \\beta_i}L(w^*,\\alpha^* , \\beta^*)=0, \\text{ } i=1,\\ldots,l\\alpha_i^* g_i(w^*)=0, \\text{ } i=1,\\ldots,kg_i(w^*)\\leq 0, \\text{ } i=1,\\ldots,k\\alpha_i^* \\geq0, \\text{ } i=1,\\ldots,k$w,\\alpha , \\beta$ê°€ KKT ì¡°ê±´ì„ ë§Œì¡±í•œë‹¤ë©´ dual problemê³¼ primal Problemì´ ê°™ì•„ì§€ë¯€ë¡œ ë‘ê°œ ëª¨ë‘ì˜ í•´ê°€ ëœë‹¤ 5. Optimal margin classifiers ë‹¤ì‹œ Margin classifiersë¬¸ì œë¡œ ëŒì•„ì™€ ë³´ì \\begin{matrix} min_{w,b} && \\frac{1}{2}\\lVert w \\rVert^2 \\end{matrix}\\begin{matrix} s.t. && y^{(i)} (w^Tx^{(i)} + b) \\geq 1 , i=1,\\ldots,m \\end{matrix}ì œì•½ì‹ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³ ì³ ì¨ì¤„ìˆ˜ìˆë‹¤ g_i(w)=-y^{(i)} (w^Tx^{(i)} + b)+1 \\leq 0Functional marginì€ 1ì´ë˜ê²Œ ëœë‹¤ ë‹¤ìŒ ê·¸ë¦¼ì—ì„œ marginì´ ê°€ì¥ ì‘ì€ ì ì€ 3ê°œê°€ ìˆë‹¤(ë‘ê°œì˜ negative í•œê°œì˜ positive) 3ê°œê°€ support vectorê°€ ëœë‹¤. ë˜í•œ support vectorì—ì„œëŠ” $\\alpha$ê°’ì´ ì ˆëŒ€ 0ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë ‡ë‹¤ëŠ”ê²ƒì€ KKTì¡°ê±´ì— ì˜í•´ $g_i(w)$ê°€ 0ì´ ë˜ì–´ì•¼ í•œë‹¤ëŠ”ê²ƒì´ë‹¤ Lagranian ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. L(w^*,\\alpha^* , \\beta^*)=\\frac{1}{2}\\lVert w \\rVert^2-\\sum_{i=1}^m \\alpha_i \\left[y^{(i)} (w^Tx^{(i)} + b)-1\\right]ë¶€ë“±ì‹ ì œì•½ì¡°ê±´ë§Œ ìˆìœ¼ë¯€ë¡œ $\\alpha_i$ë§Œ ì¡´ì¬í•œë‹¤ dual problemì„ í‘¸ê¸° ìœ„í•´ì„œ $minimize_{w,b}L(w,\\alpha , \\beta)$ $\\alpha$ëŠ” ê³ ì •í•œ ìƒíƒœì—ì„œ $w$, $b$ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•´ì•¼í•œë‹¤ wì— ëŒ€í•´ ë¯¸ë¶„í•œë‹¤ \\nabla_w L(w,\\alpha , \\beta)= w -\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}=0w=\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}bì— ëŒ€í•´ ë¯¸ë¶„í•œë‹¤ {\\partial\\over\\partial b}L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i y^{(i)}=0ë¯¸ë¶„í•´ì„œ êµ¬í•œ wë¥¼ ë¼ê·¸ë‘ì§€ì•ˆ ì‹ì— ëŒ€ì…í•´ì£¼ê³  ì •ë¦¬í•´ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì„ìˆ˜ ìˆë‹¤ L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j \\left(x^{(i)}\\right)^T x^{(j)}-b\\sum_{i=1}^m\\alpha_i y^{(i)}$\\sum_{i=1}^m\\alpha_i y^{(i)}=0$ì´ë¯€ë¡œ ìµœì¢…ì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤ L(w,\\alpha , \\beta)=\\sum_{i=1}^m\\alpha_i - \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j \\left(x^{(i)}\\right)^T x^{(j)}í˜„ì¬ê¹Œì§€ w,bì— ëŒ€í•´ì„œ ìµœì†Œí™” í•˜ëŠ” ë¬¸ì œë¥¼ í’€ì—ˆê³  ìµœì¢… dual problemì„ í’€ê¸°ìœ„í•´ì„œëŠ” ì œì•½ì‹ $\\alpha &gt;0$ ì„ í¬í•¨í•œ $max_\\alpha W(\\alpha)$ë¥¼ êµ¬í•´ì•¼ í•œë‹¤. dual problem ì‹ì€ë‹¤ìŒê³¼ ê°™ë‹¤ max_{\\alpha} W(\\alpha)=\\sum_{i=1}^m\\alpha_i- \\frac{1}{2} \\sum_{i,j=1}^my^{(i)}y^{(j)}\\alpha_i\\alpha_j ë‹¤ìŒì˜ ì‹ì„ ìµœëŒ€í™”í•˜ëŠ” $\\alpha$ë¥¼ì°¾ìœ¼ë©´ $w=\\sum_{i=1}^m\\alpha_i y^{(i)}x^{(i)}$ë¥¼ ì´ìš©í•´ optimal $w$ë¥¼ ì°¾ëŠ”ë‹¤ $b$ëŠ” êµ¬í•´ì§„ $w$ë¥¼ê°€ì§€ê³  ë‹¤ìŒì‹ì— ëŒ€ì…í•˜ì—¬ í’€ìˆ˜ ìˆë‹¤. b^{*}=-\\frac{max_{i:y^{(i)}=-1}w^{*T}x^{(i)}+min_{i:y^{(i)}=1}w^{*T}x^{(i)}}{2}ì¶”ê°€ì ìœ¼ë¡œ $w=\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)}$ë¥¼ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ë„ì¶œí•´ë‚¼ìˆ˜ ìˆë‹¤. ì´ê²ƒì€ ë‚˜ì¤‘ì— kernel trickì—ì„œ ì‚¬ìš©í•˜ê²Œ ë ê²ƒì´ë‹¤. w^Tx +b =\\left(\\sum_{i=1}^m \\alpha_i y^{(i)} x^{(i)}\\right)x + b=\\sum_{i=1}^m \\alpha_i y^{(i)}+bclassificationí• ë•Œ ì˜ˆì¸¡í•˜ë ¤ëŠ” input data ($x$)ì™€ íŠ¸ë ˆì´ë‹ í•™ìŠµ ë°ì´í„° ($x^{(i)}$)ì˜ ë‚´ì  í•© ìœ¼ë¡œ ì˜ˆì¸¡í• ìˆ˜ìˆë‹¤. ìœ„ ì‹ì€ kernel trickì—ì„œ ì‚¬ìš©ëœë‹¤.","categories":[],"tags":[]},{"title":"EM Algorithm","slug":"em-algorithm","date":"2019-05-05T14:29:55.000Z","updated":"2019-05-05T14:29:55.089Z","comments":true,"path":"2019/05/05/em-algorithm/","link":"","permalink":"http://progresivoJS.github.io/2019/05/05/em-algorithm/","excerpt":"","text":"latent variable ì¶”ë¡  clusteringê³¼ classificationì˜ ê°€ì¥ í° ì°¨ì´ì ì€ ìˆ¨ì–´ìˆëŠ” ë³€ìˆ˜ê°€ ìˆëŠëƒ ì—†ëŠëƒì´ë‹¤.clusteringì€ latent variableì´ í¬í•¨ë˜ì–´ ìˆë‹¤classificationì€ observed variableë¡œ ë¶„ë¥˜ í•œë‹¤ $\\{ X, Z\\}$ : ëª¨ë“  variable $X$ : ê´€ì¸¡ëœ(Observed) variable $Z$ : hidden(latent) Variable $\\theta$ : ë¶„í¬ parameter latent variableì„ marginal out í•´ì£¼ë©´ ëœë‹¤ P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta)ë¡œê·¸ ì†ì— summationì´ ìˆìœ¼ë©´ ê³„ì‚°ì´ ë³µì¡í•´ì ¸ì„œ ê²°êµ­ì—ëŠ” summationì˜ ìœ„ì¹˜ë¥¼ ë°”ê¿”ì¤˜ì•¼ í•œë‹¤(Jensensâ€™s inequality). ê·¸ë¦¬ê³  $q(z)$ì˜ ì„ì˜ì˜ pdfë¥¼ ë„£ì–´ì¤€ë‹¤ \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z P(X,Z \\mid \\theta)\\right\\}l(\\theta) = \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\}CF) Jensenâ€™s inequality $\\psi$ê°€ convex functionì¼ë•ŒëŠ” f \\left( \\frac{x+y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\leq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j} $\\psi$ê°€ concave functionì¼ë•ŒëŠ” f \\left( \\frac{x+y}{2}\\right) \\geq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\geq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j}ë¡œê·¸ëŠ” Concave functionì´ë¯€ë¡œ jensenâ€™s inequalityì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì´ ë¡œê·¸ ì†ì˜ summationì„ ë‹¤ìŒê³¼ ê°™ì´ ë¹¼ì„œ ì‹ì„ ì •ë¦¬í• ìˆ˜ ìˆë‹¤ l(\\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\} \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)}ì˜¤ë¥¸ìª½í•­ì˜ ì‹ì„ ì •ë¦¬í•´ì£¼ë©´ \\sum_z \\left\\{q(z) \\ln P(X,Z \\mid \\theta) - q(z)\\ln q(z) \\right\\}ì²«ë²ˆì§¸ í•­ì€ $q(z)$ì˜ ê°€ì¤‘í‰ê· ì´ê³  ë‘ë²ˆì§¸ í•­ì€ $q(z)$ê°€ í™•ë¥ ë¶„í¬ë¼ëŠ” ê°€ì •í•˜ì— ì—”íŠ¸ë¡œí”¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì‹ì„ notation í•´ì¤„ ìˆ˜ ìˆë‹¤ Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)$Q(\\theta, q)$ì€ ê²°êµ­ $l(\\theta)$ì˜ lower boundì´ë‹¤ ì´ê²ƒì„ ìµœëŒ€í™” ì‹œì¼œì£¼ë©´ $l(\\theta)$ê°’ë„ ê°™ì´ ìµœëŒ€í™” ì‹œì¼œì¤„ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤ ë‹¨ í•­ìƒ ê·¸ëŸ°ê²ƒì€ ì•„ë‹ˆë‹¤ ì™œëƒí•˜ë©´ ì„œë¡œ inequalityí•˜ê¸° ë•Œë¬¸ì´ë‹¤ Maximizing lower bound(1) l(\\theta) \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)} = \\sum_z q(z) \\ln \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{q(z)}ë§¨ ì˜¤ë¥¸ìª½ì˜ í•­ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í• ìˆ˜ ìˆë‹¤ \\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)} + q(z)\\ln P(X \\mid \\theta) \\right\\}=\\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)}\\right\\} + \\ln P(X \\mid \\theta)ë‹¤ìŒì„ ìµœì í™” ì‹œí‚¤ê¸° ìœ„í•œ ì‹ìœ¼ë¡œ ë°”ê¿”ì£¼ê¸° ìœ„í•´ summation ì†ì˜ ë¡œê·¸ë¥¼ ì—­ìˆ˜ë¡œ ì·¨í•´ì¤Œìœ¼ë¡œì¨ Kullback-Leiber divergenceë¡œ ë³€í˜• ì‹œì¼œì¤€ë‹¤ L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}ì´ ì‹ì„ ë³´ë©´ ë§ì€ ëœ»ì„ ë³¼ìˆ˜ ìˆë‹¤ ê¸°ì¡´ì˜ ìš°ë¦¬ê°€ maximizeì‹œí‚¤ê³  ì‹¶ì—ˆë˜ $\\ln P(X \\mid \\theta)$ì— KL divergence termì„ ë¹¼ì£¼ëŠ” ì‹ì´ ë˜ì—ˆë‹¤ ì¦‰ KL divergence termì„ 0ì— ê°€ê¹ê²Œ í•´ì£¼ë©´ í•´ì¤„ìˆ˜ë¡ ìš°ë¦¬ì˜ objective functionì— ê°€ê¹Œì›Œ ì§„ë‹¤KL divergence termì´ 0ì— ê°€ê¹Œì›Œ ì§„ë‹¤ëŠ” ê²ƒì€ $q(z)$ ë¶„í¬ ëª¨ì–‘ê³¼ $P(Z \\mid X, \\theta)$ì˜ ë¶„í¬ ëª¨ì–‘ì´ ë¹„ìŠ·í•´ ì§„ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤ Maximizing lower bound(2) Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}ìš°ë¦¬ê°€ $L(\\theta, q)$ë¥¼ êµ¬í•œ ì´ìœ ëŠ”ìš°ë¦¬ê°€ ì œì¼ ì²˜ìŒ êµ¬í•œ $Q(\\theta, q)$ë§Œ ê°€ì§€ê³  optimal valueë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ì‰½ì§€ ì•Šë‹¤. ì™œëƒí•˜ë©´ $q(z)$ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •í™•í•œ ì§€ì‹ì´ ì—†ê¸° ë•Œë¬¸ì´ë‹¤í•˜ì§€ë§Œ $L(\\theta, q)$ë¥¼ ê°€ì§€ê³  $q(z)$ë¥¼ ì—…ë°ì´íŠ¸ í•˜ê¸°ëŠ” ì‰½ë‹¤ ì„ì˜ì˜ $\\theta$ì— ëŒ€í•´ $q(z)$ëŠ” $P(Z \\mid X, \\theta)$ì˜ ë¶„í¬ì™€ ë¹„ìŠ·í•˜ê²Œ Updateí•´ì£¼ë©´ ëœë‹¤ KL\\left(q(Z) \\lVert P(Z \\mid X, \\theta)\\right) = 0 \\to q^t(z) = P(Z \\mid X, \\theta^t)íŠ¹ì •í•œ iterationë™ì•ˆ $\\theta^t$ë¡œ ë˜ë©´ $q^t(z)$ë¡œ ì—…ê·¸ë ˆì´ë“œ ëœë‹¤ Q(\\theta, q^t) = E_{q^t(z)} \\ln P(X,Z \\mid \\theta^t) + H(q^t)\\theta^{t+1} = argmax_{\\theta}Q(\\theta, q^t) = argmax_{\\theta}E_{q^t(z)} \\ln P(X,Z \\mid \\theta)ê·¸ëŸ¬ë©´ ì—…ê·¸ë ˆì´ë“œ ëœ $q^t$ë¥¼ ê°€ì§€ê³  ë‹¤ì‹œ $\\theta$ë¥¼ ì—…ê·¸ë ˆì´ë“œ í•´ì¤€ë‹¤ ì •ë¦¬ P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta) 1. EM ì•Œê³ ë¦¬ì¦˜ì€ latent variableì´ í¬í•¨ëœ maximum likelihoodë¥¼ ì°¾ëŠ”ê²ƒì´ë‹¤ 2. ì²˜ìŒì—ëŠ” $\\theta$ë¥¼ ëœë¤ìœ¼ë¡œ ì •í•´ì¤€ë‹¤ 3. likelihoodê°€ convergeí• ë•Œ ê¹Œì§€ iterationì„ ëŒë ¤ì¤€ë‹¤","categories":[],"tags":[]},{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend ë¥¼ ê³µë¶€í•˜ë©° ë²ˆì—­í•˜ì—¬ì„œ ì˜¬ë¦½ë‹ˆë‹¤ ê° ì˜í™”ì— ëŒ€í•´ì„œ í‰ì ì´ 5ì ê¹Œì§€ ì¤„ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•œë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : ì´ ìœ ì €ì˜ ëª…(ìˆ˜) $n_m$ : ì´ ì˜í™”ì˜ ê°¯ìˆ˜ $r(i,j)$ : $j$ë¼ëŠ” ìœ ì €ê°€ ì˜í™” $i$ì— í‰ì ì„ ì¤¬ìœ¼ë©´ 1 $r(i,j)$ : $r(i,j) = 1$ì´ë¼ëŠ” ì¡°ê±´ í•˜ì— user $j$ê°€ movie $i$ì—ê²Œ ì¤€ í‰ì  ì ìˆ˜ ìš°ë¦¬ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ê°€ì§€ê³  missing valueë¥¼ predictí•´ì•¼ í•œë‹¤ 1. Content_based algorithm content_basedëŠ” contentê°€ ì–´ë–¤ íŠ¹ì •í•œ ì ì¬ featureê°€ ìˆì„ê±°ë¼ê³  ìƒê°í•˜ê³  ê·¸ feature vectorë¥¼ ì ìš©í•˜ëŠ” ê²ƒì´ë‹¤.ì˜ˆë¥¼ ë“¤ì–´ ì˜í™”ë¥¼ ì¶”ì²œí•˜ëŠ” ê±°ë¼ë©´ ì˜í™”ì—ëŠ” ê° ì¥ë¥´ê°€ ì¡´ì¬í•œë‹¤. ì˜í™”ì˜ ë¡œë§¨ìŠ¤ ì¥ë¥´ ì •ë„, ì•¡ì…˜ ì •ë„ë¥¼ ê°€ì¤‘ì¹˜ ê°œë…ìœ¼ë¡œ ì£¼ì–´ì„œ ê° ì˜í™”ì˜ latent feature vectorë¥¼ ì§€ì •í•´ì¤€ë‹¤ ì—¬ê¸°ì—ì„œëŠ” romanceë¥¼ $x_1$, action $x_2$ì˜ feature vectorë¥¼ ë§Œë“¤ì–´ì¤€ë‹¤ extra(constant) featureë„ ë„£ì–´ì¤€ë‹¤ Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 ê°ê°ì˜ ì˜í™”ëŠ” $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ ê°ê°ì˜ ì˜í™” $x^i$ì€ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ì—ë¥¼ ë“¤ì–´ ì˜í™” Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ ì˜ feature vectorë¥¼ ê°€ì§€ê³  ìˆë‹¤ content_based ë°©ì‹ì—ì„œëŠ” ê°ê°ì˜ content feature vectorì— ë”°ë¥¸ user parameter vectorë¥¼ learningì‹œì¼œì•¼í•œë‹¤ê°ê°ì˜ userë§ˆë‹¤ ê°ê°ì˜ í‰ì ì€ linear regressionë°©ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤ ë§Œì•½ Aliceì˜ parameter vector($\\theta^1$)ì´ $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ ì´ë¼ê³  í•œë‹¤ë©´ Aliceê°€ Cute love ì˜í™”ì— í‰ì ì„ ì¤„ ì ìˆ˜ëŠ” $(\\theta^{(1)})^T x^{(3)}$ inner productë¥¼ í•´ì£¼ë©´ 4.95 í‰ì ì„ ì¤„ê±°ë¼ëŠ” ì˜ˆì¸¡ì´ ë‚˜ì˜¨ë‹¤ $\\theta$ë¥¼ learning í•´ë³´ì Notation n : featureì˜ dimension(constantë¥¼ ì œì™¸í•œê²ƒ) ì—¬ê¸°ì„œëŠ” 2(romance, action)ì´ë‹¤ $m^j$ : $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì˜ ê°¯ìˆ˜ $j$ userê°€ í‰ì ì„ ì¤€ ì˜í™”ì— í•œí•´ì„œ $j$ userì˜ ì˜í™”ì— ì¤€ ì˜ˆì¸¡ í‰ì ê³¼ ì‹¤ì œ í‰ì ì˜ ì°¨ë¥¼ ìµœì†Œí™” í•´ì•¼í•œë‹¤ min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ìµœì í™” í•˜ëŠ”ë° $m^{(j)}$ëŠ” í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì—†ì• ì£¼ì–´ë„ ëœë‹¤ min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2ì´ê±°ë¥¼ ëª¨ë“  userì—ê²Œ ì ìš©ì‹œì¼œì•¼ í•œë‹¤. ìš°ë¦¬ì˜ objective functionì´ ëœë‹¤ J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•´ì¤€ë‹¤ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ kê°€ 0ì¸ê²ƒê³¼ Kê°€ 0ì´ ì•„ë‹Œê²ƒì˜ ëœ»ì€ featureì˜ constant termì„ í•˜ëŠëƒ ì•ˆí•˜ëŠëƒ ì´ë‹¤ ìš°ë¦¬ì˜ Obejctive functionì˜ ìµœì¢…ì‹ì€ {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_basedëŠ” ê° contentì— ëŒ€í•œ featureë¥¼ ì•Œì•„ì•¼ í•œë‹¤ëŠ” ë‹¨ì ì´ ìˆë‹¤. ë‹¤ìŒì— ì•Œì•„ë³¼ ë°©ë²•ì€ ì´ê±°ë¥¼ ë³´ì™„í•´ì¤€ Colloaborative filteringë°©ë²•ì´ë‹¤ 2. Collaborative filtering content_based ë°©ì‹ì—ì„œëŠ” content featureë¥¼ ì•Œê³  ìˆëŠ” ìƒíƒœì˜€ë‹¤. í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤ ë˜í•œ Featureì˜ ê°¯ìˆ˜ë¥¼ ì¡°ê¸ˆë” ë§ì´ ì•Œê¸°ë¥¼ ì›í•œë‹¤ userì˜ ì˜í™” ì·¨í–¥ ë²¡í„° $\\theta$ì™€ ê° ì˜í™”ì˜ ì¥ë¥´ feature ë²¡í„° $x$ë¥¼ ì„œë¡œ êµì°¨ì ìœ¼ë¡œ learning Notation $n_m$ : ì˜í™”ì˜ ê°¯ìˆ˜ $n_u$ : userì˜ ìˆ˜ $\\theta$ë¥¼ ëœë¤ì ìœ¼ë¡œ Initializeí•œë‹¤ $\\theta$ë¥¼ ê°€ì§€ê³  $x$ë¥¼ updateì‹œí‚¨ë‹¤ \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 updateëœ $x$ë¥¼ ê°€ì§€ê³  $\\theta$ë¥¼ Updateì‹œí‚¨ë‹¤ \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_basedì™€ ë‹¤ë¥¸ì ì€ costant Termì„ ë„£ì–´ì£¼ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤ cost function Jë¥¼ ìµœëŒ€í•œ ì¤„ì—¬ì£¼ëŠ” ë²¡í„°ë¥¼ ì°¾ëŠ”ë‹¤ 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5ê°œì˜ ì˜í™”ì™€ 4ëª…ì˜ user matrixì´ë‹¤ predicted ratingì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ë‚¼ìˆ˜ ìˆë‹¤ X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì  $e_{i,j}$ : $i$ ìœ ì €ê°€ $j$ ì˜í™”ì—ê²Œì¤€ ì‹¤ì œ í‰ì ê³¼ ì˜ˆì¸¡ í‰ì ì˜ ì°¨ì´ $p_{i,k}$ : $i$ ìœ ì €ì˜ latent feature vector $q_{k,j}$ : $j$ ì˜í™”ì˜ latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : ìœ ì €ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ìœ ì € ëª…ìˆ˜ (X) Latent ê°¯ìˆ˜}\\right)$ $Q$ : ì˜í™”ë“¤ì˜ Latent Matrix / shape : $\\left( \\text{ Latent ê°¯ìˆ˜ (X) ì˜í™” ê°¯ìˆ˜ }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{â€˜}$, $q_{k,j}^{â€˜}$ ê°ê° ë²¡í„°ì´ë‹¤","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) ìµœëŒ€ê°€ëŠ¥ë„ Notation $D$ : Data (ê´€ì¸¡í•œ(Observed) ë°ì´í„°) $\\theta$ : parameter (í™•ë¥ )&lt;/font&gt; $H$ : ì•ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ $T$ : ë’·ë©´ì´ ë‚˜ì˜¨ íšŸìˆ˜ Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ ë¥¼ ê°€ì¥ ë†’ì—¬ì£¼ëŠ” $\\theta$ ë¥¼ êµ¬í•˜ëŠ”ê²ƒ MLE ê³„ì‚° 1. Maximum Liklihood ì‹ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ ì„ ì”Œìš´ë‹¤ ê³±ì…ˆì€ ê³„ì‚°ì´ ë³µì¡í•˜ë¯€ë¡œ $ln$ë¥¼ ì”Œì›Œì¤€ë‹¤ â†’ $log$ $function$ : ê³±ì„ í•©ìœ¼ë¡œ ë°”ê¿”ì¤€ë‹¤ ë¡œê·¸ëŠ” ë‹¨ì¡° ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ $\\underset{\\theta}{\\operatorname{argmax}}$ ì˜ ê°’ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤ \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ ì— ëŒ€í•´ì„œ ë¯¸ë¶„ì„ í•œë‹¤(derivative) êµ¬í•˜ê³ ì í•˜ëŠ” $\\theta$ì— ëŒ€í•´ ë¯¸ë¶„í•œ ê°’ì´ $0$ì´ ë˜ë„ë¡ ì‹ì„ ì„¸ìš´ë‹¤ ë¯¸ë¶„í•œ ê°’ì´ $0$ ë˜ê²Œ í•˜ëŠ” $\\theta$ê°’ì„ êµ¬í•œë‹¤ \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE ê´€ì ì—ì„œ $\\hat\\theta$ ìš°ë¦¬ê°€ ìƒì‹ì ìœ¼ë¡œ ìƒê°í•˜ê³  ìˆëŠ” í™•ë¥ ì´ MLE(Maximum Liklihood Estimation)ìœ¼ë¡œ êµ¬í•œ ê²ƒì´ë‹¤ \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(ì‚¬ì „ ì§€ì‹)ì„ ê³ ë ¤í•œë‹¤ MLE(Maximum Liklihood Estimation)ê³¼ ë‹¤ë¥´ê²Œ ì¼ì–´ë‚œ ì‚¬ê±´ë§Œì„ ê³ ë ¤í•˜ëŠ”ê²ƒì´ ì•„ë‹ˆë‹¤ MLEëŠ” $P(D\\mid\\theta)$ë¥¼ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAPëŠ” $P(\\theta\\mid D)$ ì¦‰ ë°ì´í„°ê°€ ì£¼ì–´ì¡Œì„ë•Œ $\\theta$ì˜ í™•ë¥  ì‚¬í›„í™•ë¥ (Posterior)ì„ ìµœëŒ€í™” í•˜ëŠ” $\\theta$ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤ MAP ê³„ì‚° 1. ë² ì´ì¦ˆ ì •ë¦¬(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}ğ‘·ğ’ğ’”ğ’•ğ’†ğ’“ğ’Šğ’ğ’“=\\frac{ğ‘³ğ’Šğ’Œğ’†ğ’ğ’Šğ’‰ğ’ğ’ğ’… \\cdot ğ‘·ğ’“ğ’Šğ’ğ’“ ğ‘²ğ’ğ’ğ’˜ğ’ğ’†ğ’…ğ’ˆğ’†}{ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•}2. ê´€ê³„ì‹ ì •ë¦¬ P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $ğ‘µğ’ğ’“ğ’ğ’‚ğ’ğ’Šğ’›ğ’Šğ’ğ’ˆ ğ‘ªğ’ğ’ğ’”ğ’•ğ’‚ğ’ğ’•$ì€ í¬ê²Œ ì¤‘ìš”í•˜ì§€ ì•ŠëŠ”ë‹¤. ì£¼ì–´ì§„ ë°ì´í„°ëŠ” ì´ë¯¸ ì¼ì–´ë‚œ ì‚¬ê±´ì´ê³  ì •í•´ì ¸ ìˆê¸° ë•Œë¬¸ì´ë‹¤. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ ì‚¬ì „í™•ë¥  : ì‚¬ì „í™•ë¥  ë¶„í¬ê°€ ë² íƒ€ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤ P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. ì‚¬í›„í™•ë¥  P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP ê´€ì ì—ì„œ $\\hat\\theta$ ë§Œì•½ ë˜ì§„ íšŸìˆ˜ê°€ ë§ì•„ì§€ê²Œ ë˜ë©´ $\\alpha, \\beta$ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì€ ë¯¸ë¹„í•´ì§€ë¯€ë¡œ ê²°êµ­ MLEë¡œ êµ¬í•œ ê²ƒê³¼ ê°™ì•„ì§€ê²Œ ëœë‹¤ \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]}]}
{"meta":{"title":"James Blog","subtitle":"James Data Scientist Blog","description":null,"author":"James Park","url":"http://progresivoJS.github.io","root":"/"},"pages":[],"posts":[{"title":"EM Algorithm","slug":"em-algorithm","date":"2019-05-05T14:29:55.000Z","updated":"2019-05-05T14:29:55.089Z","comments":true,"path":"2019/05/05/em-algorithm/","link":"","permalink":"http://progresivoJS.github.io/2019/05/05/em-algorithm/","excerpt":"","text":"latent variable 추론 clustering과 classification의 가장 큰 차이점은 숨어있는 변수가 있느냐 없느냐이다.clustering은 latent variable이 포함되어 있다classification은 observed variable로 분류 한다 $\\{ X, Z\\}$ : 모든 variable $X$ : 관측된(Observed) variable $Z$ : hidden(latent) Variable $\\theta$ : 분포 parameter latent variable을 marginal out 해주면 된다 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta)로그 속에 summation이 있으면 계산이 복잡해져서 결국에는 summation의 위치를 바꿔줘야 한다(Jensens’s inequality). 그리고 $q(z)$의 임의의 pdf를 넣어준다 \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z P(X,Z \\mid \\theta)\\right\\}l(\\theta) = \\ln P(X \\mid \\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\}CF) Jensen’s inequality $\\psi$가 convex function일때는 f \\left( \\frac{x+y}{2}\\right) \\leq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\leq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j} $\\psi$가 concave function일때는 f \\left( \\frac{x+y}{2}\\right) \\geq \\frac{f(x) + f(y)}{2}\\psi\\left( \\frac{\\sum a_i x_i}{\\sum a_j} \\right) \\geq \\frac{\\sum a_i \\psi (x_i)}{\\sum a_j}로그는 Concave function이므로 jensen’s inequality에 의해 다음과 같이 로그 속의 summation을 다음과 같이 빼서 식을 정리할수 있다 l(\\theta) = \\ln \\left\\{ \\sum_z q(z)\\frac{P(X,Z \\mid \\theta)}{q(z)} \\right\\} \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)}오른쪽항의 식을 정리해주면 \\sum_z \\left\\{q(z) \\ln P(X,Z \\mid \\theta) - q(z)\\ln q(z) \\right\\}첫번째 항은 $q(z)$의 가중평균이고 두번째 항은 $q(z)$가 확률분포라는 가정하에 엔트로피를 나타낸다. 따라서 다음과 같이 식을 notation 해줄 수 있다 Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)$Q(\\theta, q)$은 결국 $l(\\theta)$의 lower bound이다 이것을 최대화 시켜주면 $l(\\theta)$값도 같이 최대화 시켜줄수 있다는 것이다 단 항상 그런것은 아니다 왜냐하면 서로 inequality하기 때문이다 Maximizing lower bound(1) l(\\theta) \\geq \\sum_z q(z) \\ln \\frac{P(X,Z \\mid \\theta)}{q(z)} = \\sum_z q(z) \\ln \\frac{P(Z \\mid X, \\theta)P(X \\mid \\theta)}{q(z)}맨 오른쪽의 항을 다음과 같이 정리할수 있다 \\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)} + q(z)\\ln P(X \\mid \\theta) \\right\\}=\\sum_z \\left\\{q(z) \\ln \\frac{P(Z \\mid X, \\theta)}{q(z)}\\right\\} + \\ln P(X \\mid \\theta)다음을 최적화 시키기 위한 식으로 바꿔주기 위해 summation 속의 로그를 역수로 취해줌으로써 Kullback-Leiber divergence로 변형 시켜준다 L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}이 식을 보면 많은 뜻을 볼수 있다 기존의 우리가 maximize시키고 싶었던 $\\ln P(X \\mid \\theta)$에 KL divergence term을 빼주는 식이 되었다 즉 KL divergence term을 0에 가깝게 해주면 해줄수록 우리의 objective function에 가까워 진다KL divergence term이 0에 가까워 진다는 것은 $q(z)$ 분포 모양과 $P(Z \\mid X, \\theta)$의 분포 모양이 비슷해 진다는 것을 의미한다 Maximizing lower bound(2) Q(\\theta, q) = E_{q(z)} \\ln P(X,Z \\mid \\theta) + H(q)L(\\theta, q) = \\ln P(X \\mid \\theta) - \\sum_z \\left\\{ q(z) \\ln \\frac{q(z)}{P(Z \\mid X, \\theta)} \\right\\}우리가 $L(\\theta, q)$를 구한 이유는우리가 제일 처음 구한 $Q(\\theta, q)$만 가지고 optimal value를 구하는 것은 쉽지 않다. 왜냐하면 $q(z)$를 업데이트하는 방법에 대한 정확한 지식이 없기 때문이다하지만 $L(\\theta, q)$를 가지고 $q(z)$를 업데이트 하기는 쉽다 임의의 $\\theta$에 대해 $q(z)$는 $P(Z \\mid X, \\theta)$의 분포와 비슷하게 Update해주면 된다 KL\\left(q(Z) \\lVert P(Z \\mid X, \\theta)\\right) = 0 \\to q^t(z) = P(Z \\mid X, \\theta^t)특정한 iteration동안 $\\theta^t$로 되면 $q^t(z)$로 업그레이드 된다 Q(\\theta, q^t) = E_{q^t(z)} \\ln P(X,Z \\mid \\theta^t) + H(q^t)\\theta^{t+1} = argmax_{\\theta}Q(\\theta, q^t) = argmax_{\\theta}E_{q^t(z)} \\ln P(X,Z \\mid \\theta)그러면 업그레이드 된 $q^t$를 가지고 다시 $\\theta$를 업그레이드 해준다 정리 P(X \\mid \\theta) = \\sum_z P(X,Z \\mid \\theta) 1. EM 알고리즘은 latent variable이 포함된 maximum likelihood를 찾는것이다 2. 처음에는 $\\theta$를 랜덤으로 정해준다 3. likelihood가 converge할때 까지 iteration을 돌려준다","categories":[],"tags":[]},{"title":"Recommend system","slug":"Recommend-system","date":"2019-05-04T06:29:55.000Z","updated":"2019-05-04T06:29:55.136Z","comments":true,"path":"2019/05/04/Recommend-system/","link":"","permalink":"http://progresivoJS.github.io/2019/05/04/Recommend-system/","excerpt":"","text":"andrew ng lecture note recommend 를 공부하며 번역하여서 올립니다 각 영화에 대해서 평점이 5점까지 줄수 있다고 가정한다 Movie Alice(1) Bob(2) Carol(3) Dave(4) Love at star 5 5 0 0 Romance Forevere 5 ? ? 0 Cute love ? 4 0 ? Nonstop Car 0 0 5 4 Sword 0 0 5 ? Notation $n_u$ : 총 유저의 명(수) $n_m$ : 총 영화의 갯수 $r(i,j)$ : $j$라는 유저가 영화 $i$에 평점을 줬으면 1 $r(i,j)$ : $r(i,j) = 1$이라는 조건 하에 user $j$가 movie $i$에게 준 평점 점수 우리는 주어진 데이터를 가지고 missing value를 predict해야 한다 1. Content_based algorithm content_based는 content가 어떤 특정한 잠재 feature가 있을거라고 생각하고 그 feature vector를 적용하는 것이다.예를 들어 영화를 추천하는 거라면 영화에는 각 장르가 존재한다. 영화의 로맨스 장르 정도, 액션 정도를 가중치 개념으로 주어서 각 영화의 latent feature vector를 지정해준다 여기에서는 romance를 $x_1$, action $x_2$의 feature vector를 만들어준다 extra(constant) feature도 넣어준다 Movie Alice(1) Bob(2) Carol(3) Dave(4) $x_0$(constant) $x_1$(romance) $x_2$(action) Love at star 5 5 0 0 1 0.9 0 Romance Forevere 5 ? ? 0 1 1.0 0.1 Cute love ? 4 0 ? 1 0.99 0 Nonstop Car 0 0 5 4 1 0.1 1.0 Sword 0 0 5 ? 1 0 0.9 각각의 영화는 $\\begin{bmatrix}Love.. &amp; Romance.. &amp; Cute.. &amp; Nonstop.. &amp; Sword.. \\\\\\end{bmatrix} =\\begin{bmatrix}x^1 &amp; x^2 &amp; x^3 &amp; x^4 &amp; x^5 \\\\\\end{bmatrix}$ 각각의 영화 $x^i$은 feature vector를 가지고 있다에를 들어 영화 Love at star $x^1 = \\begin{bmatrix}1 \\\\0.9\\\\0\\end{bmatrix}$ 의 feature vector를 가지고 있다 content_based 방식에서는 각각의 content feature vector에 따른 user parameter vector를 learning시켜야한다각각의 user마다 각각의 평점은 linear regression방식으로 나타난다 만약 Alice의 parameter vector($\\theta^1$)이 $x^1 = \\begin{bmatrix}0 \\\\5\\\\0\\end{bmatrix}$ 이라고 한다면 Alice가 Cute love 영화에 평점을 줄 점수는 $(\\theta^{(1)})^T x^{(3)}$ inner product를 해주면 4.95 평점을 줄거라는 예측이 나온다 $\\theta$를 learning 해보자 Notation n : feature의 dimension(constant를 제외한것) 여기서는 2(romance, action)이다 $m^j$ : $j$ user가 평점을 준 영화의 갯수 $j$ user가 평점을 준 영화에 한해서 $j$ user의 영화에 준 예측 평점과 실제 평점의 차를 최소화 해야한다 min_{\\theta^{(j)}} \\frac{1}{2 m^{(j)}} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2 m^{(j)}}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2최적화 하는데 $m^{(j)}$는 필요 없으므로 없애주어도 된다 min_{\\theta^{(j)}} \\frac{1}{2} \\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2이거를 모든 user에게 적용시켜야 한다. 우리의 objective function이 된다 J(\\theta^1, \\ldots, \\theta^{n_u}) =\\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}\\frac{1}{2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2$\\theta$에 대해서 미분을 해준다 $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)}$ $(for \\text{ } k= 0)$ $\\theta_k^{(j)} : \\theta_k^{(j)} - \\alpha \\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)$ $(for \\text{ } k\\neq 0)$ k가 0인것과 K가 0이 아닌것의 뜻은 feature의 constant term을 하느냐 안하느냐 이다 우리의 Obejctive function의 최종식은 {\\partial^2\\over\\partial\\theta_k^{(j)}}J(\\theta^1, \\ldots, \\theta^{n_u})=\\left(\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)x_k^{(i)} + \\lambda \\theta_k^{(j)} \\right)content_based는 각 content에 대한 feature를 알아야 한다는 단점이 있다. 다음에 알아볼 방법은 이거를 보완해준 Colloaborative filtering방법이다 2. Collaborative filtering content_based 방식에서는 content feature를 알고 있는 상태였다. 하지만 현실에서는 불가능하다 또한 Feature의 갯수를 조금더 많이 알기를 원한다 user의 영화 취향 벡터 $\\theta$와 각 영화의 장르 feature 벡터 $x$를 서로 교차적으로 learning Notation $n_m$ : 영화의 갯수 $n_u$ : user의 수 $\\theta$를 랜덤적으로 Initialize한다 $\\theta$를 가지고 $x$를 update시킨다 \\underset{x^{(1)}, \\cdots, x^{(n_m)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_m}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(j)}\\right)^2 update된 $x$를 가지고 $\\theta$를 Update시킨다 \\underset{\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}} {1\\over2} \\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 Minimizing $\\theta^{(1)}, \\cdots, \\theta^{(n_u)}$ and $x^{(1)}, \\cdots, x^{(n_m)}$ simultaneously J(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}) ={1\\over2} \\sum_{(i,j):r(i,j)=1} \\left(\\left(\\theta^{(j)}\\right) ^T x^{(i)} - y^{(i,j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n \\left( \\theta_k^{(j)}\\right)^2 + \\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n \\left( x_k^{(i)}\\right)^2\\underset{x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}}{\\operatorname{min}}J\\left(x^{(1)}, \\cdots, x^{(n_m)},\\theta^{(1)}, \\cdots, \\theta^{(n_u)}\\right)content_based와 다른점은 costant Term을 넣어주지 않는다는 것이다 cost function J를 최대한 줄여주는 벡터를 찾는다 3. Low rank matrix Factorization Y = \\begin{bmatrix} 5 & 5 & 0 &0\\\\ 5 & ? & ? &0\\\\ ? & 4 & 0 &?\\\\ 0 & 0 & 5 &4\\\\ 0 & 0 & 5 &0\\\\ \\end{bmatrix}5개의 영화와 4명의 user matrix이다 predicted rating은 다음과 같이 나타낼수 있다 X = \\begin{bmatrix} --- \\left(x^{(1)}\\right)^T ---\\\\ --- \\left(x^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(x^{(n_m)}\\right)^T ---\\\\ \\end{bmatrix}\\Theta = \\begin{bmatrix} --- \\left(\\theta^{(1)}\\right)^T ---\\\\ --- \\left(\\theta^{(2)}\\right)^T ---\\\\ \\vdots\\\\ --- \\left(\\theta^{(n_u)}\\right)^T ---\\\\ \\end{bmatrix}predicted rating matrix = $\\Theta^T \\cdot X$ Notation $r_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점 $e_{i,j}$ : $i$ 유저가 $j$ 영화에게준 실제 평점과 예측 평점의 차이 $p_{i,k}$ : $i$ 유저의 latent feature vector $q_{k,j}$ : $j$ 영화의 latent feature vector $\\beta$ : Regularization Term $\\alpha$ : Learning rate $P$ : 유저들의 Latent Matrix / shape : $\\left( \\text{유저 명수 (X) Latent 갯수}\\right)$ $Q$ : 영화들의 Latent Matrix / shape : $\\left( \\text{ Latent 갯수 (X) 영화 갯수 }\\right)$ e_{i,j}^2 = \\left( r_{i,j} -\\sum_{k=1}^K p_{i,k} q_{k,j} \\right)^2 + {\\beta \\over 2} \\sum_{k=1}^K \\left(\\lVert P \\rVert^2 + \\lVert Q \\rVert^2\\right)p_{i,k}^{'} = p_{i,k} + \\alpha {\\partial \\over \\partial p_{i,k}}e_{i,j}^2 = p_{i,k} + \\alpha \\left( 2 e_{i,j}q_{k,j} - \\beta p_{i,k}\\right)q_{k,j}^{'} = q_{k,j} + \\alpha {\\partial \\over \\partial q_{k,j}}e_{i,j}^2 = q_{k,j} + \\alpha \\left( 2 e_{i,j}p_{i,k} - \\beta q_{k,j}\\right)$p_{i,k}^{‘}$, $q_{k,j}^{‘}$ 각각 벡터이다","categories":[],"tags":[]},{"title":"MLE MAP","slug":"MLE-MAP-1","date":"2019-03-29T06:28:54.000Z","updated":"2019-03-29T06:28:54.847Z","comments":true,"path":"2019/03/29/MLE-MAP-1/","link":"","permalink":"http://progresivoJS.github.io/2019/03/29/MLE-MAP-1/","excerpt":"","text":"1. MLE(Maximum Liklihood Estimation) 최대가능도 Notation $D$ : Data (관측한(Observed) 데이터) $\\theta$ : parameter (확률)&lt;/font&gt; $H$ : 앞면이 나온 횟수 $T$ : 뒷면이 나온 횟수 Maximum Likelihood Estimation (MLE) of $\\theta$ $\\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta)$ $P(D\\mid \\theta)$ 를 가장 높여주는 $\\theta$ 를 구하는것 MLE 계산 1. Maximum Liklihood 식 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} P(D\\mid \\theta) = \\underset{\\theta}{\\operatorname{argmax}} \\theta^{T} (1-\\theta)^{H}2. $log$ $function$ 을 씌운다 곱셈은 계산이 복잡하므로 $ln$를 씌워준다 → $log$ $function$ : 곱을 합으로 바꿔준다 로그는 단조 증가 함수이므로 $\\underset{\\theta}{\\operatorname{argmax}}$ 의 값은 변하지 않는다 \\hat\\theta = \\underset{\\theta}{\\operatorname{argmax}} ln (P(D\\mid \\theta)) = \\underset{\\theta}{\\operatorname{argmax}}\\{Tln(\\theta) + Hln(1-\\theta)\\}3. $\\theta$ 에 대해서 미분을 한다(derivative) 구하고자 하는 $\\theta$에 대해 미분한 값이 $0$이 되도록 식을 세운다 미분한 값이 $0$ 되게 하는 $\\theta$값을 구한다 \\frac{d}{d\\theta}(Tln(\\theta) + Hln(1-\\theta)) = 0\\frac{T}{\\theta} - \\frac{H}{1-\\theta} = 0\\theta = \\frac{T}{T+H}4. MLE 관점에서 $\\hat\\theta$ 우리가 상식적으로 생각하고 있는 확률이 MLE(Maximum Liklihood Estimation)으로 구한 것이다 \\hat\\theta = \\frac{T}{T+H}2. MAP(Maximum a Posteriori Estimation) Notation Prior Knowledge(사전 지식)을 고려한다 MLE(Maximum Liklihood Estimation)과 다르게 일어난 사건만을 고려하는것이 아니다 MLE는 $P(D\\mid\\theta)$를 최대화 하는 $\\theta$를 구하는 것이다 MAP는 $P(\\theta\\mid D)$ 즉 데이터가 주어졌을때 $\\theta$의 확률 사후확률(Posterior)을 최대화 하는 $\\theta$를 구하는 것이다 MAP 계산 1. 베이즈 정리(Bayes' theorem) P(\\theta\\mid D)=\\frac{P(D\\mid\\theta)P(\\theta)}{P(D)}𝑷𝒐𝒔𝒕𝒆𝒓𝒊𝒐𝒓=\\frac{𝑳𝒊𝒌𝒆𝒍𝒊𝒉𝒐𝒐𝒅 \\cdot 𝑷𝒓𝒊𝒐𝒓 𝑲𝒏𝒐𝒘𝒍𝒆𝒅𝒈𝒆}{𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕}2. 관계식 정리 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta) $𝑵𝒐𝒓𝒎𝒂𝒍𝒊𝒛𝒊𝒏𝒈 𝑪𝒐𝒏𝒔𝒕𝒂𝒏𝒕$은 크게 중요하지 않는다. 주어진 데이터는 이미 일어난 사건이고 정해져 있기 때문이다. $P(D\\mid \\theta)$ Liklihood : $\\theta^{T} (1-\\theta)^{H}$ $P(\\theta)$ 사전확률 : 사전확률 분포가 베타 분포를 따른다고 가정한다 P(\\theta)=\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1}}{B(\\alpha,\\beta)}, B(\\alpha,\\beta)=\\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha,\\beta)}, \\Gamma(\\alpha)=(\\alpha-1)!3. 사후확률 P(\\theta\\mid D)\\propto P(D\\mid \\theta)P(\\theta)P(D\\mid \\theta)P(\\theta) \\propto \\theta^{T} (1-\\theta)^{H}\\theta^{\\alpha-1}(1-\\theta)^{\\beta -1} \\propto \\theta^{T+\\alpha-1} (1-\\theta)^{H+\\beta-1}4. MAP 관점에서 $\\hat\\theta$ 만약 던진 횟수가 많아지게 되면 $\\alpha, \\beta$가 미치는 영향은 미비해지므로 결국 MLE로 구한 것과 같아지게 된다 \\hat\\theta=\\frac{T+\\alpha-1} {T+H+\\alpha+\\beta-2}","categories":[],"tags":[]}]}